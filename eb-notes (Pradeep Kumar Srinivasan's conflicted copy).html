<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<link rel="icon" type="image/png" href="./images/favicon-32x32.png" sizes="32x32" />
	<link rel="icon" type="image/png" href="./images/favicon-16x16.png" sizes="16x16" />
        <title>EB Notes - SPK's Rationality Essays</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />
        <link rel="stylesheet" type="text/css" href="./css/highlight.css" />

	<!-- <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script> -->
	<!-- <script type="text/javascript" src="/js/header-links.js"></script> -->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

	<link href="atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed" />

	<!-- Google Analytics stuff -->
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-51321929-1', 'pradeep90.github.io');
	  ga('send', 'pageview');

	</script>

    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">Rationality Essays</a>
            </div>
            <div id="navigation">
                <a href="./">Home</a>
                <a href="./notes.html">Notes</a>
                <!-- <a href="/about.html">About</a> -->
                <a href="./archive.html">Archive</a>
		<a href="./atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed">RSS</a>
            </div>
        </div>

        <div id="content">
          <h1 id="post-title">EB Notes</h1>

            <!-- <center><img src="https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/t31.0-8/p600x600/10257116_10202295769100492_2438594605053717342_o.jpg" height="400" width="300" class="sujeet-pic" alt="Sujeet pic" /></center> -->

<h1 id="automate-scientific-discovery">Automate Scientific Discovery</h1>
<p>“From Causal Inference and Data Fusion to an Automated Scientist”, Elias Bareinboim, 1541-1672/16/$33.00 (c) 2016 IEEE. IEEE INTELLIGENT SYSTEMS. Published by the IEEE Computer Society.</p>
<p>Graphical models are the bee’s knees:</p>
<blockquote>
<p>The advent of graphical methods of causal and counterfactual inference has made it possible to tackle some of the most challenging problems in scientific methodology.</p>
</blockquote>
<p>To take advantage of big data, we need to use two more things on top of graphical models:</p>
<blockquote>
<p>the ability to distinguish causal from associational relationships, and the ability to integrate data from multiple, heterogeneous sources.</p>
</blockquote>
<p>Apparently, this is called the “data fusion” problem. Once we use his theoretical framework, we can let AIs generalize their causal models like human scientists.</p>
<p>Note how the overall theme echoes that of the 1987 book “Scientific Discovery” by the AI researchers Langley, Simon, et al. We all want to automate scientific discovery.</p>
<h1 id="causal-inference-and-the-data-fusion-problem">Causal Inference and the Data-Fusion Problem</h1>
<p>Hypothesis: Confounding bias -&gt; P(y|x) != P(y|do(x)), because something could cause both x and y (including the cases where one is the ancestor of the other).</p>
<p>Admissible sets: Z is not a descendant of X means that you can’t accidentally condition on a collider and get a spurious correlation. Z blocks all backdoor paths from X to Y means that it will prevent Y from having an effect as an ancestor of X or from something confounder causing both X and Y. That way, you block the other two causes of correlations. The only correlation left between X and Y is causative correlation. If I’m right, you’ve converted a question of intervention into a question of correlation simply by averting the other causes of correlation.</p>
<p>Corollary: Admissible set -&gt; remove confounding bias relative to the effect of X on Y.</p>
<p>Front-door criterion:</p>
<pre><code>   u
  / \
 /   \
x-&gt;z-&gt;y</code></pre>
<p>Note that x d-separates u and z because y is a collider, and u and z d-separate x and y. So, P(u|z,x) = P(u|x) and P(y|x,z,u) = P(y|z,u). Substitute them into the formula for P(y|do(x))</p>
<p>Ah! Front-door criterion = back-door criterion applied twice!</p>
<p>For the causal effect of x on z, the collider y blocks all back-door paths from x to z and makes the admissible set {}. So, P(z|do(x)) = P(z|x).</p>
<p>For the causal effect of z on y, x blocks all back-door paths from z to y. So, P(y|do(z)) = Sum_x’ P(y|x’,z)P(x’)</p>
<p>So, overall effect of x on y is <code>Sum_z (P(z|x) (Sum_x' P(y|x',z)P(x')))</code>.</p>
<h1 id="back-door-paths">Back-Door Paths</h1>
<p>Hypothesis: Back-door path =&gt; confounder. No back-door path =&gt; no confounder.</p>
<h1 id="papers-i-need-to-understand">Papers I need to Understand</h1>
<p>Pearl conference presentation on Data Fusion problem</p>
<p>Causal inference and the data-fusion problem</p>
<p>Causality the book</p>
<p>Recovering from Selection Bias in Causal and Statistical Inference</p>
<p>Recovering Causal Effects From Selection Bias (?)</p>
<h1 id="my-core-ideas">My Core Ideas</h1>
<p>Locality of causality</p>
<p>Expected value of information</p>
<p>Entropy</p>
<h1 id="past-project-ideas-and-papers">Past Project Ideas and Papers</h1>
<h2 id="my-own-research-interests">My Own Research Interests</h2>
<p>Scientific discovery.</p>
<p>Programming.</p>
<p>How humans learn. The causal models that are inherent in the textbooks we study and in the categories we have of the world around us.</p>
<p><strong>Hypothesis</strong>: I’m interested in how to scale models from the micro-level to the macro-level. How do we form higher-level concepts?</p>
<p>For example, climate science (as in the thesis). Or going from our understanding of individual functions in a program to the overall properties of the program. Ditto for a car.</p>
<p>Hypothesis: I believe that we shouldn’t limit ourselves to the variables given in the data (as you keep mentioning, there could be millions of such low-level variables). How do we get to the high-level variables we care about? How would an automated program identify high-level “variables”?</p>
<p>Instead of just feature selection, I want category creation. Build high level abstractions out of low level variables. I believe that will reduce our uncertainty about the joint distribution and make it cheaper to calculate the queries we are interested in. We are interested in high level variables.</p>
<p>Hypothesis: I think that is related to the decisions that we face - what interventions are we uncertain about? Also depends on the granularity of our tools. (example: macro-economics works with things like interest rates and tax rates, not individual firms.)</p>
<h2 id="inference-data-fusion">Inference / Data-fusion</h2>
<p>Causal inference and the data-fusion problem.</p>
<h2 id="bounding">Bounding</h2>
<blockquote>
<p>[T]he assessment of treatment efficacy in the face of imperfect compliance.</p>
<p>…</p>
<p>Methodologically, the message of this chapter has been to demonstrate that, even in cases where causal quantities are not identifiable, reasonable assumptions about the structure of causal relationships in the domain can be harnessed to yield useful quantitative information about the strengths of those relationships. Once such assumptions are articulated in graphical form and re-encoded in terms of canonical partitions, they can be submitted to algebraic methods that yield informative bounds on the quantities of interest. The canonical partition further allows us to supplement structural assumptions with prior beliefs about the population under study and invite Gibbs sampling techniques to facilitate Bayesian estimation of the target quantities.</p>
<p>– Chapter 8, Causality</p>
</blockquote>
<h2 id="instrumental-variables-linear-models">Instrumental Variables / Linear Models</h2>
<blockquote>
<p>In this paper, we extend graph-based identification methods by allowing background knowledge in the form of non-zero parameter values. Such informa- tion could be obtained, for example, from a previ- ously conducted randomized experiment, from sub- stantive understanding of the domain, or even an identification technique. To incorporate such in- formation systematically, we propose the addition of auxiliary variables to the model, which are con- structed so that certain paths will be conveniently cancelled. This cancellation allows the auxiliary variables to help conventional methods of iden- tification (e.g., single-door criterion, instrumental variables, half-trek criterion), as well as model test- ing (e.g., d-separation, over-identification). More- over, by iteratively alternating steps of identifica- tion and adding auxiliary variables, we can improve the power of existing identification methods via a bootstrapping approach that does not require ex- ternal knowledge. We operationalize this method for simple instrumental sets (a generalization of in- strumental variables) and show that the resulting method is able to identify at least as many models as the most general identification method for linear systems known to date. We further discuss the ap- plication of auxiliary variables to the tasks of model testing and z-identification.</p>
<p>– Incorporating Knowledge into Structural Equation Models using Auxiliary Variables</p>
</blockquote>
<h2 id="adjustment-algorithms">Adjustment / Algorithms</h2>
<blockquote>
<p>We address the problem of nding a minimal separator in a di- rected acyclic graph (DAG), namely, nding a set Z of nodes that d-separates a given pair of nodes, such that no proper subset of Z d-separates that pair. We analyze several versions of this problem and o er polynomial algorithms for each. These include: nding a minimal separator from a restricted set of nodes, nding a minimum- cost separator, and testing whether a given separator is minimal. We con rm the intuition that any separator which cannot be reduced by a single node must be minimal.</p>
<p>– Finding Minimal D-separators</p>
</blockquote>
<blockquote>
<p>Covariate adjustment is a widely used approach to estimate total causal effects from observational data. Several graphical criteria have been de- veloped in recent years to identify valid covari- ates for adjustment from graphical causal mod- els. These criteria can handle multiple causes, latent confounding, or partial knowledge of the causal structure; however, their diversity is con- fusing and some of them are only sufficient, but not necessary. In this paper, we present a cri- terion that is necessary and sufficient for four different classes of graphical causal models: di- rected acyclic graphs (DAGs), maximum ances- tral graphs (MAGs), completed partially directed acyclic graphs (CPDAGs), and partial ancestral graphs (PAGs). Our criterion subsumes the ex- isting ones and in this way unifies adjustment set construction for a large set of graph classes.</p>
<p>– A Complete Generalized Adjustment Criterion</p>
</blockquote>
<h2 id="structural-learning">Structural Learning</h2>
<blockquote>
<p>Randomized controlled experiments are often described as the most reliable tool available to scien- tists for discovering causal relationships among quantities of interest. However, it is often unclear how many and which different experiments are needed to identify the full (possibly cyclic) causal structure among some given (possibly causally insufficient) set of variables. Recent results in the causal discovery literature have explored various identifiability criteria that depend on the assump- tions one is able to make about the underlying causal process, but these criteria are not directly constructive for selecting the optimal set of experiments. Fortunately, many of the needed construc- tions already exist in the combinatorics literature, albeit under terminology which is unfamiliar to most of the causal discovery community. In this paper we translate the theoretical results and apply them to the concrete problem of experiment selection. For a variety of settings we give explicit constructions of the optimal set of experiments and adapt some of the general combinatorics results to answer questions relating to the problem of experiment selection.</p>
<p>– Experiment Selection for Causal Discovery</p>
</blockquote>
<blockquote>
<p>We show that if any number of variables are allowed to be simultaneously and indepen- dently randomized in any one experiment, log 2 (N ) + 1 experiments are sufficient and in the worst case necessary to determine the causal relations among N &gt;= 2 variables when no latent variables, no sample selection bias and no feedback cycles are present. For all K, 0 &lt; K &lt; 2 1 N we provide an upper bound on the number experiments required to determine causal structure when each ex- periment simultaneously randomizes K vari- ables. For large N , these bounds are signifi- cantly lower than the N - 1 bound required when each experiment randomizes at most one variable. For k max &lt; N 2 , we show that N -1)+ 2k N log 2 (k max ) experiments are ( k max max sufficient and in the worst case necessary. We offer a conjecture as to the minimal number of experiments that are in the worst case suf- ficient to identify all causal relations among N observed variables that are a subset of the vertices of a DAG.</p>
<p>– On the Number of Experiments Sufficient and in the Worst Case Necessary to Identify All Causal Relations Among N Variables</p>
</blockquote>
<h2 id="reinforcement-learning-machine-learning">Reinforcement Learning / Machine Learning</h2>
<blockquote>
<p>Reinforcement learning (RL) agents have been de- ployed in complex environments where interactions are costly, and learning is usually slow. One promi- nent task in these settings is to reuse interactions performed by other agents to accelerate the learn- ing process. Causal inference provides a family of methods to infer the effects of actions from a combination of data and qualitative assumptions about the underlying environment. Despite its suc- cess of transferring invariant knowledge across do- mains in the empirical sciences, causal inference has not been fully realized in the context of transfer learning in interactive domains. In this paper, we use causal inference as a basis to support a prin- cipled and more robust transfer of knowledge in RL settings. In particular, we tackle the problem of transferring knowledge across bandit agents in settings where causal effects cannot be identified by do-calculus [Pearl, 2000] and standard learning techniques. Our new identification strategy com- bines two steps - first, deriving bounds over the arms distribution based on structural knowledge; second, incorporating these bounds in a dynamic allocation procedure so as to guide the search to- wards more promising actions. We formally prove that our strategy dominates previously known algo- rithms and achieves orders of magnitude faster con- vergence rates than these algorithms. Finally, we perform simulations and empirically demonstrate that our strategy is consistently more efficient than the current (non-causal) state-of-the-art methods.</p>
<p>– Transfer Learning in Multi-Armed Bandits</p>
</blockquote>
<h2 id="transfer-learning-machine-learning">Transfer Learning / Machine Learning</h2>
<blockquote>
<p>We provide a formal definition of the notion of “transportability,” or “external validity,” which we view as a license to transfer causal information learned in exper- imental studies to a different environment, in which only observational studies can be conducted. We introduce a formal representation called “selection diagrams” for expressing knowledge about differences and commonalities between populations of in- terest and, using this representation, we derive procedures for deciding whether causal effects in the target environment can be inferred from experimental findings in a dif- ferent environment. When the answer is affirmative, the procedures identify the set of experimental and observational studies that need be conducted to license the transport. We further demonstrate how transportability analysis can guide the transfer of knowl- edge among non-experimental studies to minimize re-measurement cost and improve prediction power. We further provide a causally principled definition of “surrogate endpoint” and show that the theory of transportability can assist the identification of valid surrogates in a complex network of cause-effect relationships.</p>
<p>– Transportability Across Studies</p>
</blockquote>
<h2 id="fairness-discrimination-machine-learning">Fairness-Discrimination / Machine Learning</h2>
<blockquote>
<p>Abstract Anti-discrimination is an increasingly important task in data science. In this paper, we investigate the problem of discover- ing both direct and indirect discrimination from the historical data, and removing the discriminatory effects before the data is used for predictive analysis (e.g., building classifiers). We make use of the causal network to capture the causal structure of the data. Then we model direct and indirect discrimination as the path-specific effects, which explicitly distinguish the two types of discrimination as the causal effects transmitted along different paths in the network. Based on that, we pro- pose an effective algorithm for discovering direct and indirect discrimination, as well as an algorithm for precisely removing both types of discrimination while retaining good data utility. Different from previous works, our approaches can ensure that the predictive models built from the modified data will not incur discrimination in decision making. Experiments us- ing real datasets show the effectiveness of our approaches.</p>
<p>Discrimination refers to unjustified distinctions in decisions against individuals based on their membership in a certain group. Federal Laws and regulations (e.g., the Equal Credit Opportunity Act of 1974) have been established to prohibit discrimination on several grounds, such as gender, age, sex- ual orientation, race, religion or belief, and disability or ill- ness, which are referred to as the protected attributes. Nowa- days various predictive models have been built around the collection and use of historical data to make important de- cisions like employment, credit and insurance. If the histor- ical data contains discrimination, the predictive models are likely to learn the discriminatory relationship present in the data and apply it when making new decisions. Therefore, it is imperative to ensure that the data goes into the predictive models and the decisions made with its assistance are not subject to discrimination.</p>
<p>– A Causal Framework for Discovering and Removing Direct and Indirect Discrimination</p>
</blockquote>
<h2 id="interference">Interference</h2>
<blockquote>
<p>Abstract. The term “interference” has been used to describe any set- ting in which one subject’s exposure may affect another subject’s out- come. We use causal diagrams to distinguish among three causal mech- anisms that give rise to interference. The first causal mechanism by which interference can operate is a direct causal effect of one individ- ual’s treatment on another individual’s outcome; we call this direct interference. Interference by contagion is present when one individ- ual’s outcome may affect the outcomes of other individuals with whom he comes into contact. Then giving treatment to the first individual could have an indirect effect on others through the treated individ- ual’s outcome. The third pathway by which interference may operate is allocational interference. Treatment in this case allocates individ- uals to groups; through interactions within a group, individuals may affect one another’s outcomes in any number of ways. In many settings, more than one type of interference will be present simultaneously. The causal effects of interest differ according to which types of interference are present, as do the conditions under which causal effects are iden- tifiable. Using causal diagrams for interference, we describe these dif- ferences, give criteria for the identification of important causal effects, and discuss applications to infectious diseases.</p>
<p>– Causal Diagrams for Interference</p>
</blockquote>
<h2 id="effect-restoration">Effect Restoration</h2>
<blockquote>
<p>This paper highlights several areas where graphical techniques can be harnessed to address the problem of measurement errors in causal inference. In particular, it discusses the control of unmeasured confounders in parametric and nonparametric models and the computational prob- lem of obtaining bias-free effect estimates in such models. We derive new conditions under which causal effects can be restored by observing proxy variables of unmeasured confounders with/without external studies.</p>
<p>– Measurement bias and effect restoration in causal inference</p>
</blockquote>
<h2 id="mediation-analysis">Mediation Analysis</h2>
<blockquote>
<p>Recent advances in causal inference have given rise to a general and easy-to-use for- mula for assessing the extent to which the effect of one variable on another is mediated by a third. This Mediation Formula is applicable to nonlinear models with both dis- crete and continuous variables, and permits the evaluation of path-specific effects with minimal assumptions regarding the data-generating process. We demonstrate the use of the Mediation Formula in simple examples and illustrate why parametric methods of analysis yield distorted results, even when parameters are known precisely. We stress the importance of distinguishing between the necessary and sufficient interpretations of “mediated-effect” and show how to estimate the two components in nonlinear systems with continuous and categorical variables.</p>
<p>– The Causal Mediation Formula</p>
</blockquote>
<h2 id="explanation-actual-causation">Explanation / Actual Causation</h2>
<blockquote>
<p>This chapter offers a formal explication of the notion of “actual cause,” an event recognized as responsible for the production of a given outcome in a specific scenario, as in: “Socrates drinking hemlock was the actual cause of Socrates death.” Human intuition is extremely keen in detecting and ascertaining this type of causation and hence is considered the key to constructing explanations (Section 7.2.3) and the ultimate criterion (known as “cause in fact”) for determining legal responsibility.</p>
<p>…</p>
<p>Statements of the type “a car accident was the cause of Joe’s death,” made relative to a specific scenario, are classified as “singular,” “single-event,” or “token-level” causal statements. Statements of the type “car accidents cause deaths,” when made relative to a type of events or a class of individuals, are classified as “generic” or “type-level” causal claims (see Section 7.5.4). We will call the cause in a single-event statement an actual cause and the one in a type-level statement a general cause.</p>
<p>– Chapter 10, Causality</p>
</blockquote>
<h2 id="constraints-theory">Constraints / Theory</h2>
<p>(Paraphrasing) Help you identify functional constraints and thus help you test causal models as well as infer them from data.</p>
<h2 id="cyclic-models">Cyclic Models</h2>
<blockquote>
<p>We show that the d-separation criterion constitutes a valid test for conditional independence that are induced by feedback systems involving discrete variables</p>
<p>– Identifying Independencies in Causal Graphs with Feedback</p>
</blockquote>
<h2 id="databases">Databases</h2>
<blockquote>
<p>Provenance is often used to validate data, by verifying its origin and explaining its derivation. When searching for “causes” of tuples in the query results or in general observations, the analysis of lineage becomes an essential tool for providing such justifications. However, lineage can quickly grow very large, limiting its immediate use for providing intuitive explanations to the user. The formal notion of causality is a more refined concept that identifies causes for observations based on user-defined criteria, and that assigns to them gradual degrees of responsibility based on their respective contributions. In this paper, we initiate a discussion on causality in databases, give some simple definitions, and motivate this formalism through a number of example applications.</p>
<p>– Causality in Databases</p>
</blockquote>
<h2 id="theses">Theses</h2>
<p>Automated Macro-scale Causal Hypothesis Formation Based on Micro-scale Observation (2016), K. Chalupka [link]</p>
<h2 id="not-interested">Not Interested</h2>

<div class="info">Created: June 22, 2017</div>
<div class="info">Last modified: September 30, 2017</div>
<div class="info">Status: in-progress notes</div>
<div class="info"><b>Tags</b>: notes, eb</div>

<br />
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'spkrationalitytrainingground'; // required: replace example with your forum shortname
    var disqus_identifier = '/eb-notes%20%28Pradeep%20Kumar%20Srinivasan%27s%20conflicted%20copy%29.html';
    var disqus_title = 'EB Notes';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
