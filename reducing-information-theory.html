<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<link rel="icon" type="image/png" href="./images/favicon-32x32.png" sizes="32x32" />
	<link rel="icon" type="image/png" href="./images/favicon-16x16.png" sizes="16x16" />
        <title>Reducing Information Theory to Probability Theory - SPK's Rationality Essays</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />
        <link rel="stylesheet" type="text/css" href="./css/highlight.css" />

	<!-- <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script> -->
	<!-- <script type="text/javascript" src="/js/header-links.js"></script> -->

	<!-- Google Analytics stuff -->
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-51321929-1', 'pradeep90.github.io');
	  ga('send', 'pageview');

	</script>

    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">Rationality Essays</a>
            </div>
            <div id="navigation">
                <a href="./">Home</a>
                <!-- <a href="/sequences.html">Sequences</a> -->
                <a href="./notes.html">Notes</a>
                <!-- <a href="/about.html">About</a> -->
                <a href="./archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
          <h1 id="post-title">Reducing Information Theory to Probability Theory</h1>

            <!-- <center><img src="https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/t31.0-8/p600x600/10257116_10202295769100492_2438594605053717342_o.jpg" height="400" width="300" class="sujeet-pic" alt="Sujeet pic" /></center> -->

<p><strong>Note</strong>: I’m not an expert probability theorist, so this could be wrong. But I feel quite confident in it.</p>
<h1 id="taboo-information">Taboo information</h1>
<p>I never got a handle on the concept of information. I could see no good reason why it had to be defined the way it was:</p>
<pre><code>information content of a message = log2(1/p)</code></pre>
<p>I got the feeling you shouldn’t need anything more than probability theory to solve any problems, even those that apparently need “information theory”.</p>
<p>Why did I feel this way? Well, in real life, we know that we want more information. We think more information is better than less information. That seems reasonable. You can take better actions with more information. But, then we come across this term in math called “information” too. And, somehow, without realizing it, we act like it’s like our old information and that more of it is better too.</p>
<p>That was the source of my confusion. We have these two different (for now) concepts but we treat them the same way without justifying it.</p>
<p>To make it more explicit, rename the term “information” with some other name that doesn’t clash with our existing concepts.</p>
<p>So, let’s try to reduce “information theory” to basic probability theory, so that we do away with the concepts of information and entropy. We can still use them afterwards, but only as a convenience, just like we talk about sentences and paragraphs, when in reality there are only the individual English letters underneath.</p>
<p>In short, let’s <a href="./Rationalists-Taboo.html">taboo</a> the word “information”.</p>
<h1 id="multiplying-our-confidence">Multiplying our confidence</h1>
<p>What is your aim? You want to find the correct hypothesis and put all your confidence in it. That way you will know exactly which outcome will happen in the future and can exploit that knowledge. And you want to do this quickly.</p>
<p>What does an outcome (or message) really do? What happens when you see an outcome for which you assigned a probability P(E)? Well, you use it to update the posterior probability of all your hypotheses, using Bayes Theorem:</p>
<pre><code>P(H|E) = P(H) x P(E|H) / P(E) -- (1)

P(H|E): posterior probability
P(H): prior probability
P(E|H): likelihood of E judged by this hypothesis
P(E): probability of outcome E</code></pre>
<p>True. But what really does that have to do with your aim? You just want to find the correct hypothesis (call it H0). You couldn’t give less of a damn about the rest. So, the question we really want to answer is, what does this outcome do to your confidence in the correct hypothesis H0? As per Bayes Theorem, P(H0|E) = P(H0) x P(E|H0) / P(E). But this is H0 we’re talking about. It makes perfectly accurate predictions every time. That means P(E|H0) is always 1.</p>
<p>Therefore, P(H0|E) = P(H0) x 1/P(E).</p>
<p>So, what an outcome E really does is multiply our confidence in the correct hypothesis by 1/P(E). (Note that increasing our confidence in H0 automatically reduces our confidence in the rest since the sum of all P(H) must be 1.)</p>
<p>And this, I theorize, is why we define information as log2(1/P(E)). It’s an indirect measure of how much an outcome multiplies our confidence in the correct hypothesis. There’s nothing new or special about “information”. In the past, I kept getting confused by the fact that information seemed like a completely different beast from probability. But, all you’re doing is taking the logarithm. That shouldn’t do anything. That doesn’t add any information. It’s just a convenient representation. That’s all.</p>
<h1 id="information-in-multiple-messages">Information in Multiple Messages</h1>
<p>What if there are multiple messages? How do we calculate their information content?</p>
<p>Let’s say you get n messages. The problem here is that your probability distribution will change as your confidence in your hypotheses changes. After the first message, you will update the posterior probabilities of all the hypotheses using Bayes Theorem. Then, you will <em>those</em> probabilities to predict the probability of the next message, and then you’ll update again, and so on. And since we calculate the information content of a message using the probability we assigned it, then it seems that we have to update on the first n-1 messages before we can assign a probability to the nth message and find its information content. Is there a quicker way?</p>
<p>Well, all we’re interested in is the amount by which our confidence in H0 is multiplied. That will look like this:</p>
<pre><code>P(H0|E1,E2,...,En) = P(H0) x 1/P(E1) x 1/P(E2|E1) x ... x 1/P(En|E1,E2,...,En-1)

where P(E2|E1) means the probability of E2 after we have updated on E1, and so on.</code></pre>
<p>That looks pretty complicated. However, we can collapse the product on the right side:</p>
<pre><code>P(H0|E1,E2,...,En) = P(H0) x 1/P(E1,E2,...,En)</code></pre>
<p>Basically, we just consider the sequence of messages E1, E2, etc. as one giant message E.</p>
<pre><code>P(H0|E) = P(H0) x 1/P(E)</code></pre>
<p>More importantly, we can judge the probability of the total message E using our initial probability distribution! We don’t have to update on every message piecemeal. Instead of doing the tedious and complicated calculation for each message, we can just figure out the final probability in one shot and update our confidence in H0.</p>
<h1 id="why-can-you-add-bits">Why can you add bits?</h1>
<p>This used to be the most mystifying part of information theory. Not only did bits seem very different from probabilities, you could <em>add</em> bits and you could add them in any order. They had no identity saying that this bit came from this probability distribution, and those three bits came that other one. They were just bits. They were like dollars, you could add or subtract them without specifying where they came from - this dollar came from my salary, that one came from selling my bike. It was independent of the source.</p>
<p>How can we explain bits in terms of probability theory?</p>
<p>Well, remember our giant message E made out of smaller messages E1, E2, etc.? Well, that needn’t be the only way to break it up. You could have any sequence of messages as long as their product was the same P(E) and the effect would still be the same. It’s the same multiplication of confidence whether it came from one giant message, or ten smaller messages, or even a thousand.</p>
<pre><code>P(E) = P(E1,E2,...,En) = P(E1) x P(E2|E1) x ... x P(En|E1,E2,...,En-1)</code></pre>
<p>Take the logarithm everywhere.</p>
<pre><code>log2(P(E)) = log2(P(E1,E2,...,En)) = log2(P(E1)) + log2(P(E2|E1)) + ... + log2(P(En|E1,E2,...,En-1))</code></pre>
<p>So,</p>
<pre><code>inf(E) = inf(E1,E2,...,En) = inf(E1) + inf(E2|E1) + ... + inf(En|E1,E2,...,En-1)

where inf(E) is the information content of E in bits.</code></pre>
<p>This means that you can either calculate the information content <code>inf(E)</code> of the giant message as a whole, using your initial confidence levels in your hypotheses. Or you can update on each message one at a time, and then calculate the information content of the next message <code>inf(E2|E1)</code> using the current confidence levels. It works out the same either way.</p>
<p>You can add bits because you can multiply probabilities. There’s nothing special about bits. In fact, this is probably why we denoted information as the logarithm of 1/P(E) - so that we can add it easily and don’t have to deal with very small fractions.</p>
<h1 id="rising-to-the-top">Rising to the Top</h1>
<p>What happens if P(E) is small? Then, 1/P(E) will be large and thus the smug, ever-correct hypothesis H0 will get its confidence multiplied a lot. So, when low-probability outcomes occur, H0 has a bonanza. And since we want to discover H0 as soon as possible (we don’t know which one it is), we want P(E) to be as small as possible each time so that H0 rises to probability 1 very quickly.</p>
<p>However, there’s a tiny problem. You can’t ask for low-probability outcomes to happen. You have assigned low probability to them! You don’t expect them to happen.</p>
<p>So, we want our confidence in the correct hypothesis to rise very quickly, for which we need low-probability outcomes to happen, but you can’t ask for those outcomes to happen. What do we do then? Are we doomed to taking a long time for the correct hypothesis to beat the others and get to probability 1?</p>
<h1 id="entropy-to-the-rescue">Entropy to the rescue</h1>
<p>Essentially, you have two different experiments X1 and X2, and you can choose only one. Which one will you pick? Remember, our aim is to get to H0 as quickly as possible.</p>
<p>Well, you want the one that will multiply P(H0) the most. But you don’t know which outcome will happen. If you knew somehow that X1 would give an outcome of probability 0.1 and X2 would give an outcome of probability 0.2, then you could tell that P(H0) would get multiplied by 1/0.1 = 10 and 1/0.2 = 5 respectively. And you would choose X1 to get the bigger boost.</p>
<p>However, the same problem plagues you that you don’t <em>know</em> for sure which outcomes will occur. But you do have probabilistic knowledge.</p>
<p>Well, take the first experiment X1. You assign probabilities p_i to its various outcomes m_i.</p>
<p>Now assume you run experiments with similar probability distributions to X1 several times. You expect there to be a distribution of outcomes. If there are n experiments, then you expect n x p_i messages of type m_i. H0 will predict the correct outcome every time. So, they will all multiply your confidence in H0 by the inverse of their probability. So,</p>
<pre><code>P(H0|o_1,o_2,...) = P(H0) x 1/p_o1 x 1/p_o2 x ...

where o_i is the outcome of the ith experiment and p_oi is its probability.</code></pre>
<p>Since there are n x p_i messages of type m_i, you get</p>
<pre><code>P(H0|o_1,o_2,...) = P(H0) x Product( (1/p_i)^(n x p_i) )</code></pre>
<p>Taking the power n in common:</p>
<pre><code>P(H0|m_1,m_2,...) = P(H0) x Product( (1/p_i)^p_i )^n</code></pre>
<p>Let’s call Product( (1/p_i)^p_i ) as A. This “A” is a function of just your probability distribution over the possible messages.</p>
<pre><code>P(H0|m_1,m_2,...) = P(H0) x A^n</code></pre>
<p>So, if you get n messages from that probability distribution, for a large enough n, your confidence in the correct hypothesis will be multiplied by A^n.</p>
<p>This is exactly what we wanted to know! Now, to compare two experiments X1 and X2, just look at their values for A. If A for X1 is greater than X2, then we can expect to get our confidence multiplied more with X1. So, to get the maximum confidence-multiplier, look for the experiment for which you have the highest A.</p>
<p>What is this mysterious A? Well, it’s nothing but 2^(entropy)!</p>
<p>Entropy is defined in information theory as</p>
<pre><code>entropy = Sum(-p_i x log2(p_i))</code></pre>
<p>Multiplying by a logarithm is the same as raising the power of the value inside. So:</p>
<pre><code>entropy = Sum(log2(p_i^(-p_i))) = Sum(log2((1/p_i)^p_i))</code></pre>
<p>Also, sum of logarithms is the same as products of the values inside.</p>
<pre><code>entropy = log2(Product((1/p_i)^p_i))</code></pre>
<p>That inner part is nothing but our A. So,</p>
<pre><code>entropy = log2(A)</code></pre>
<p>Or,</p>
<pre><code>A = 2^(entropy)</code></pre>
<p>So, the way to quickly gain confidence in the correct hypothesis is to pick the experiment with maximum entropy.</p>
<h1 id="notes">Notes</h1>
<p>Even the notion that the source can only give you n kinds of messages is information. You can’t just start with that assumption because that means you’re sneaking in information without declaring it. You have to get that indirectly by asking your hypotheses what they predict and ignoring messages that get zero (or extremely low) probability.</p>
<p>TODO: Now, there exists one correct hypothesis that predicts all the past evidence with perfect accuracy and will predict all future evidence too. Of course, you may have several such candidates and would need more evidence to eliminate the rest. However, if you don’t have <em>any</em> hypothesis at all that has a perfect score so far - assigning a likelihood of 1 to the correct outcome each time - then that means your hypothesis set is lacking.</p>
<p>As you keep getting new messages (from whichever sources), the correct hypothesis H0 will sit back and assign likelihood 1 every time and thus will keep getting rewarded with higher posterior probability. There will be at least one possible message that will rule out the other hypotheses (otherwise, they would be the correct hypothesis). So, in the fullness of time, everybody else will end up with posterior 0, and H0 will have posterior 1. Note that this situation will never change. None of the others can get back into the game and H0 cannot lose because P(E) will become equal to P(E|H0) and thus it will remain at full confidence P(H0) = 1.</p>
<h2 id="other-thoughts---todo">Other thoughts - TODO</h2>
<p>My aim is to show that information and entropy are not essential concepts. Anything you can do with them, you can do with pure probability as well.</p>
<p>P(H) is your belief level that H is the correct hypothesis.</p>
<p>Corollaries that information is subjective, that you have to seek high-entropy situations (with practical examples), and the notion of information and uncertainty for hypotheses.</p>
<p>Why am I writing this? Because information seemed so mystifying. TODO: Give examples of the mystifying definitions of information.</p>
<p>Fractional bits: when does the information-as-length-of-message analogy get really stretched? When 1/P(E) is not a power of two.</p>
<p>(My reduction of entropy to probability is not as clean as the one of information. You need to posit a large number of messages. I think that’s true even with the message-length definition of entropy - it only works out in the ideal case, with a large number of messages.</p>
<p>No, actually, it is pretty simple there. It’s just the expected information content of messages. However, that leaves the question: why do you want to maximize this “entropy” thing? Because you want to maximize “information”? Is that satisfactory? TODO)</p>
<h1 id="summary">Summary</h1>
<h2 id="questions-and-surprises">Questions and Surprises</h2>
<p>Why is information defined the way it is?</p>
<p>Shouldn’t probability theory be capable of answering all questions? Given the framework of hypotheses and prior probabilities over them (as per my understanding of Bayesian probability).</p>
<p>What is the general work of thinking? Making correct predictions - put more probability mass on the actual outcome and thus less on the rest. That’s all that matters. How do we do that? By discovering the correct hypothesis. Which means putting more confidence in the right hypothesis. So, our aim is to put as much probability mass on the correct hypothesis (and thus less on the others). Note: we’ll assume that the hypotheses make at least one different prediction or are different in their prior probabilities.</p>
<p>What happens in Bayesian terms when you see an outcome (or a “message”)? You update the posterior probabilities of your hypotheses using it.</p>
<p>Ok. Now, switch to Information Theory goggles. Why is information special? Where do we use it? Why don’t we use probability theory there? The general claim seems to be that you need information theory. You can’t just use normal Bayesian updating like above to get your answers.</p>
<p>How do we calculate the information content in multiple messages?</p>
<p>Why can we add bits? What’s the deal with bits? If they represent message lengths, why can they be fractional?</p>
<p>The central question: Why is more information better than less information? This is not as stupid a question as it seems. We defined information in an arbitrary way. Now, why do we want to maximize that apparently arbitrary function?</p>
<p>How do we maximize information reliably?</p>
<h2 id="questions-refined">Questions refined</h2>
<p>You can ideally solve problems by using Bayesian updating.</p>
<p>In short, taboo the word “information”. It’s just a coincidence (for now) that we call this weird <code>log2(1/p)</code> thing the same thing as we call knowledge or data. Let’s call it something else - say, “vasbezngvba”. Now, why do you want to maximize this vasbezngvba thing? Why create a new field of study called Vasbezngvba Theory? Why on earth should this vasbezngvba thing have so many nice properties?</p>
<h2 id="surprises">Surprises</h2>
<p>What’s new now that you’ve reduced information theory to probability theory? What can you do now that you couldn’t do earlier?</p>
<p>The main answer I provide is what information does and why we need to maximize it.</p>

<div class="info">Created: January  8, 2016</div>
<div class="info">Last modified: January 18, 2016</div>
<div class="info">Status: in-progress</div>
<div class="info"><b>Tags</b>: reduction, information theory</div>

<!-- <div id="sequence-navigation" style="text-align: right"> -->
<!--   <p>Part of <a href="./sequences.html"><i>No Sequence</i></a> -->

<!--   <p>Previous post: "<a href="">Start of Sequence</a>" -->

<!--   <p>Next post: "<a href="">Head of Sequence</a>" -->
<!-- </div> -->

<br />
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'spkrationalitytrainingground'; // required: replace example with your forum shortname
    var disqus_identifier = '/reducing-information-theory.html';
    var disqus_title = 'Reducing Information Theory to Probability Theory';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
