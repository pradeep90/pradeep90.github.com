<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<link rel="icon" type="image/png" href="./images/favicon-32x32.png" sizes="32x32" />
	<link rel="icon" type="image/png" href="./images/favicon-16x16.png" sizes="16x16" />
        <title>Variables - SPK's Rationality Essays</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />
        <link rel="stylesheet" type="text/css" href="./css/highlight.css" />

	<!-- <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script> -->
	<!-- <script type="text/javascript" src="/js/header-links.js"></script> -->

	<!-- Google Analytics stuff -->
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-51321929-1', 'pradeep90.github.io');
	  ga('send', 'pageview');

	</script>

    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">Rationality Essays</a>
            </div>
            <div id="navigation">
                <a href="./">Home</a>
                <!-- <a href="/sequences.html">Sequences</a> -->
                <a href="./notes.html">Notes</a>
                <!-- <a href="/about.html">About</a> -->
                <a href="./archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
          <h1 id="post-title">Variables</h1>

            <!-- <center><img src="https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/t31.0-8/p600x600/10257116_10202295769100492_2438594605053717342_o.jpg" height="400" width="300" class="sujeet-pic" alt="Sujeet pic" /></center> -->

<h1 id="what-are-variables">What are variables?</h1>
<p>How do you create a variable?</p>
<p>Concrete examples.</p>
<p>Solve this and everything else will fall into place, I think.</p>
<p>I’m not saying it will be a cakewalk from here, but one way or another you will learn the true limits of the scientific method.</p>
<h1 id="how-do-programmers-deal-with-change">How do programmers deal with change?</h1>
<p>They look for hot spots - parts that can change - and then encapsulate the change to minimize the work needed to change it. For example, you’re printing five variables into a report. You know that you may change whether the fourth and fifth variable occur. So, you will either have the first three or you will have all five, rarely anything else. Now, given that we want one-button changes, the best way to go forward is to encapsulate the fourth and fifth variables into one variable and allow yourself to toggle them with just one change.</p>
<p>In short, you want to handle the most changes with the least effort. The only trouble is that you don’t <em>know</em> which changes you’re going to make. If you knew that some circumstance of fate would make you change the sixth, thirteenth, and fourty-fourth variables, you would set up a single button that would let you do exactly that change and nothing else.</p>
<p>So, the length of your program should reflect, I think, the number of bits of uncertainty you have about what you will want from your program in the future. It’s about minimizing the number of bits the future-you would have to transmit to the program you write. If there are four types of changes, all of which you expect to be equally likely, then you need two on-off buttons. Off-off will refer to the first change, off-on will refer to the second, on-off to the third, and on-on to the fourth.</p>
<p>The actual changes you need to make don’t matter. They’re baked in: you’re not uncertain about them and you can’t do anything about them. If in the future you need to make change X, you need to do all that X entails, no getting around it. You’re just uncertain about which change you would need to make.</p>
<h1 id="unlikely-changes">Unlikely changes</h1>
<p>Now we’ll take the case where some changes are more likely than others. Say you have just two types of changes, but you expect the first one to occur three times as often as the second one. That is, you expect the first one with 0.75 probability and the second one with 0.25 probability. The future-you knows this about you. So, how long a message does he need to send to inform the program about the change that he needs to make?</p>
<p>From information theory, we know the number of bits needed to represent an outcome of probability p is -log2(p). So, he needs -log2(0.75) = ~0.42 bits to represent the first case and -log2(0.25) = 2 bits to represent the second case.</p>
<p>(Note, there is also the case where no change is needed and we need to represent that default message too.)</p>
<p>A corollary is that if you knew exactly what you would need a program to do in the future, then you wouldn’t need any buttons for change at all. You could just hardcode the program and use it forever. Similarly, if you knew very little about what would change, you would have to have lots and lots of buttons for change. You couldn’t hardcode anything. You would have to make everything abstract, in case it changed value; like extracting variables instead of using magic numbers, extracting functions in case you need to either change the logic or re-use the same logic elsewhere, etc.</p>
<p>Why do we encapsulate change (by extracting functions and such)? To minimize the work needed to change the logic in the future. If you’ve repeated the same logic in N places, you need to change N locations or risk having a wrong program. By extracting a function with that logic, you save work on the order of N, which is usually just two or three.</p>
<h1 id="the-upshot">The Upshot</h1>
<p>What can we conclude? I think the size of a program represents how uncertain you are about its future changes. Programmers want to minimize the amount of work to make any change needed in the future, so they design their programs to make likely changes easy and unlikely changes hard. (You can’t make both types of change easy.) And since the ease of a change is generally about the number of lines of code changed, this means that you would need to modify only a few lines for an expected change, and a lot of lines for an unexpected change.</p>
<p>So, experienced programmers will probably write code that is easy to change over time, because they know exactly what changes to expect in the future.</p>
<h1 id="abstractions">Abstractions</h1>
<p>I suspect that we decouple parts of a system when we’re confident that they won’t affect each other in the future. When designing a car, you think about the colour scheme independently of the design of the engine, because you expect that they have little or nothing to do with each other.</p>
<p>Abstraction means that we don’t care about what’s inside a box as long as it behaves a certain way on the outside. And for our abstraction not to leak, the inside of the box must be independent of the other parts of the system. Otherwise, our inferences will go wrong. We can justifiably believe that pressing the brakes will make our car slow down because there’s nothing interfering in the connection between the pedal and the engine. However, if someone comes and cuts the brake wires, our above assumption will be wrong and the car won’t behave the way we want it to.</p>
<p>So, we can decide on a level of abstraction for a problem by thinking about what parts of the system don’t interact with each other. That is the first higher-level assumption we have to make in our hypothesis. Everything else follows from our choice of abstractions.</p>
<p>We get powerful abstractions when we choose as big an aspect of the system that is independent of the other aspects most of the time. Should it be completely independent? How do we decide on abstractions for a particular system?</p>
<p>In short, an abstraction says that the inside of a box is independent of the rest of the system, given that it satisfies the interface. That is, P(Inside | Any Other Part, Satisfies Interface) = P(Inside | Satisfies Interface). Knowing about any other part of the system gives you no more information about the inside of this part, once you know that it satisfies some interface. How can we discover such parts of the system?</p>
<h1 id="the-need-to-know-principle">The Need-to-Know Principle</h1>
<p>How do we decide on our abstractions?</p>
<p>Here’s one idea: look at exactly what the other parts of the system need to know about this part. In programming, the GUI only needs to know about the data it needs to present, not the intricacies of the database behind the scenes. All the GUI needs to know about the rest of the program is that it can send data in a particular format, nothing else. This way, you can change the entire backend tomorrow and still be able to use the same GUI so long as you maintain the agreed data format. Your program is more modular and versatile when parts only know so much about each other as they need to know.</p>
<p>I suppose they call this the Minimum Information principle. The idea is that you can change everything else about this part except for its interface, and things should work just fine.</p>
<p>Key question between two parts of the system: What do you need to know about me? If nothing, then we are completely orthogonal.</p>
<p>So, the causal model isn’t about concrete “variables”. It’s about abstract interfaces. For example, anything that satisfies the interface of “providing a force F” on anything that satisfies the interface of “mass m” will make it satisfy the interface of “acceleration F/m”.</p>
<h1 id="black-boxes">Black Boxes</h1>
<p>Now, we have figured out that a causal model is about relationships between interfaces. What sort of relationships are these?</p>
<p>How can we ensure that these interfaces are valid? How can we make sure our abstractions are airtight? (I get reminded of the functor law that says <code>fmap id_x = id_y</code>, ensuring that no unnecessary effects take place. Basically, it keeps your abstraction from leaking.) Importantly, how can we leverage the idea of locality of causality?</p>
<p>But, firstly, what exactly is an “interface”? How do you start creating interfaces? Give me concrete examples.</p>
<p>One thing it does is tell you whether some collection of matter satisfies the interface or not.</p>
<p>Let’s take Newton’s Second Law of Motion: a = F/m. How do we decide whether something satisfies the interfaces of force, mass, or acceleration? The only thing that matters for “acceleration” is that your velocity should change over time (which in turn talks about your displacement).</p>
<h1 id="limiting-access-to-information">Limiting access to information</h1>
<p>The inside should be independent of the rest. That is, the rest of the system must depend only on the interface. This is physically impossible because as a chunk of matter you are in contact with the rest of the system and thus can be acted upon in several ways. I think it means that no matter what the rest of your characteristics are, the effects we care about in this causal model will depend only on your interface. If you added further tests, then your abstractions may not hold.</p>
<p>Programming abstractions are unique, I think, in the way they can limit the amount of information shared between modules. You can truly ensure that the only thing another module knows about you is what is passed through your interface.</p>
<p>Actually, is that true? At the level of a programming language, yes, we can’t distinguish between two concrete modules that have the same interface. However, they will certainly differ in their low-level implementations. If you are expert enough to inspect and interpret the binary code, you can differentiate between them.</p>
<p>So, the safety of programming abstractions comes about because it is hard to observe and manipulate raw binary code to do what you want (though crackers feel it’s worth it). High-level programming languages restrict what you can see or do, thereby protecting your abstractions from your own meddling and thus increasing your power. Otherwise, a module A can use hacks to access the innards of another module B (for efficiency, maybe) and break when you change B’s implementation.</p>
<p>Therefore, abstraction is about limiting access to information. You’re in trouble when you assume that modules can access less information than they actually can.</p>
<p>And this is probably where locality of causality helps us. It says that one part of the world can only access information from (i.e., cause or be affected by) the parts that are close to it in time and space. You have no way of accessing information from far away or from the distant past.</p>
<p>Still, if you could observe the part of the system precisely, you could find out more than the interface tells you.</p>
<p>Basically, our job becomes easier if certain effects depend only on particular characteristics, not on others. The speed of a car is not affected too much by the colour of the paint on the outside.</p>
<p>How can we find out what an effect doesn’t depend on? The same way as we know everything else: using the scientific method. Hypothesize things it might depend on, and vary them until you get to a level of abstraction that does change it. So, you first assert that the engine speed depends on the colour of the paint. Then, you change the paint colour several ways, and find that it doesn’t change the engine speed. So, you realize that they are probably independent.</p>
<h1 id="notes">Notes</h1>
<h2 id="what-dont-you-mark-as-variables">What don’t you mark as variables?</h2>
<p>You don’t encapsulate things that you don’t expect to change at all. These are things you’re nearly certain about.</p>
<h2 id="variables-and-instruments">Variables and Instruments</h2>
<p>How does precision of the instruments we have affect the level of abstraction of variables?</p>
<h2 id="random-thoughts">Random thoughts</h2>
<p>Rewriting essays (or programs) is so effective at clarifying thoughts because it makes you look for similarities between parts of your essays and abstract what is common.</p>
<h1 id="notes-on-words">Notes on Words</h1>
<p>(from Eliezer’s sequence on a Human’s Guide to Words)</p>
<p>Note: Keep in mind that a multi-level model exists only in your map, not in the territory. There is no Boeing 747 by itself, there is a configuration of quarks that meets the observational tests you apply to the 747.</p>
<p>Definitions exist to help us answer queries about <em>observables</em>. Whether Barney “is a man” depends on whether you want to feed him hemlock or want to marry him or something else (and thus want to know if he will die, make you happy, etc.).</p>
<p>You shouldn’t take out more information than you put in. That is, if you say that noticing the features “big”, “yellow”, and “striped” lets you identify something as a tiger, and <em>further</em> infer other features like “dangerous” and “fast”, then you must have previously observed that correlation for a sufficient number of animals. Else, you are making unjustified inferences, and might be wrong.</p>
<p>So, a category is a hypothesis and you need sufficient evidence to raise its posterior probability. In this case, you should see lots of instances where “big”, “yellow”, and “striped” animals had the other properties and very few instances where they didn’t (assuming that big-yellow-striped uniquely identifies the tiger category; otherwise, you need to add more diagnostic features, like whiskers and tail).</p>
<p>There’s nothing more to a category than the observables it can constrain. Once you answer that a blegg is blue, egg-shaped, etc. there’s nothing more to be said. There is no more information about a blegg from that category.</p>
<h2 id="compression">Compression</h2>
<p>How faithfully can we represent the real world in our mind? The universe is very big and our mental capacity is very small. So, it seems we have to accept a loss of detail when we compress the territory into our map.</p>
<p>What are the limits of our mental representation? One limit is the amount of evidence you can get. To have predictive power about a lot of things, you need a corresponding amount of evidence. I’m not sure about how many bits of information we can get (or even how to calculate the number of bits of information from some source). More prosaic limits are those of brain frequency, memory input rates, etc.</p>
<p>Basically, are there states of the world among which you cannot distinguish? Well, we’re limited by the precision of our senses. Our eyes cannot distinguish between two specks that are within nanometres in size of each other. We can’t see X rays, I think, or hear dog whistles. The events where someone blows a dog whistle or doesn’t will appear the same way to me if I can’t see them whistling and there’s no dog or other auditory instrument. The statement “the room is quiet” doesn’t differentiate between them.</p>
<h2 id="defining-categories">Defining Categories</h2>
<p>How should you define categories? Look at Thingspace and see which things are close to each other and draw a boundary that covers only those things and none others.</p>
<p>Wait. What is “Thingspace”? Take all the observation tools you have and you observe different configurations of quarks. Get their height, weight, colour, etc. by using your tools. Now, create an infinite dimension space with those features as dimensions and locate each “object” as a point in space. This is Thingspace.</p>
<p>I still feel this is not quite canonical. What do we mean by an “object”? Why take a whole object? Why not just one half of it or 2/15th?</p>
<h1 id="reductionism-and-variables">Reductionism and Variables</h1>
<p>In other words, I feel we’re still doing some unspecified, implicit mental work here. The key question is: can you teach a computer to do this, even in principle? We may not have the processor speed or memory to host a sufficiently powerful AI, but can you write a program that works even on a small part of the universe?</p>
<p>Better still, as a proof of concept, can you show that your program works for some made-up universe, something much simpler than our own universe? What are the questions you would like answered? What are the sources of evidence available to the program?</p>
<p>Here’s an unrelated question: why does Bayes Theorem work? If the universe is an automaton running the laws of physics, why does Bayes Theorem and the rest of math work? In fact, what does Bayes Theorem help us do?</p>
<p>In short, I’m not able to reconcile the ideas of reductionism and probability theory. I’m struggling to understand how we can use variables and probability theory (which talks in terms of those variables) in a universe that has only one level of reality and where things proceed as per some fixed rules (the laws of physics).</p>
<p>Personally, though, what is the precise query I care about? My question is: what variables should I use when trying to make a causal model that represents the knowledge in some textbook or research paper? Isn’t that the sensible thing to do - if causal thinking is the bee’s knees and very efficient and all, then shouldn’t you try to represent as much of your knowledge as possible in a causal model? Anyway, that’s what I plan to do. Why? Because I believe causal models will help me answer queries about the world more accurately than other forms of knowledge representation (including just the default mental maps we have). I believe they will also help me solve problems faster, by using Bayes Theorem to decide which experiments to run or what to observe so as to get the most information.</p>
<h1 id="words-as-messages">Words as Messages</h1>
<p>You want to convey information to another person (or yourself) and you want to use the shortest message you can to do so. The way to go is to use a binary string whose length is proportional to the expected frequency of that event. So, if you expect it to rain with 25% probability, then assign a string of 2 bits (since -log2(0.25) = 2 bits); since you expect that event to happen 25% of the time, the contribution of this event to the expected message length is 0.25 x 2 bits = 0.5 bits.</p>
<p>The diagnostic properties of a word (“mortal featherless biped” for human) should have mutual information about each other or about other properties (one heart, two lungs, etc.).</p>
<p><strong>Exercise</strong>: With all this knowledge about mutual information and Thingspace, how would you create a language from scratch?</p>
<p>Wait a minute! F*ck! If words describe clusters in Thingspace, then you had best ensure that your words are as succinct as possible! You want to capture exactly the things you care about and no others. PG was right:</p>
<blockquote>
<p>Maybe I’m excessively attached to conciseness. I write code the same way I write essays, making pass after pass looking for anything I can cut. But I have a legitimate reason for doing this. You don’t know what the ideas are until you get them down to the fewest words.</p>
<p>– Paul Graham, <a href="http://www.paulgraham.com/discover.html">Persuade xor Discover</a></p>
</blockquote>
<p>Hmmm… I think it’s not just words that hint at categories. Phrases and entire sentences probably do the same thing too. If the word “conciseness” helps you answer queries about the length of an essay or a speech, then PG’s statement “maybe I’m excessively attached to conciseness” helps you answer questions about the length of his average descriptions and about his unwillingness to publish something that isn’t concise and his favourite works and such, especially compared to other authors.</p>
<p>If you claim to talk about a bunch of important properties that are related to each other, i.e., they tell you something about each other, then you must have words or a short combination of words that captures it exactly. If they are important, they are likely probable and so must have short words or phrases assigned to them, even be it technical jargon. Also, if these properties have mutual information, then you probably have a short key set of diagnostic properties that lets you infer everything else, the way yellowed skin lets you infer jaundice and the appropriate treatment. You should be able to do better than explaining each concept in isolation. Instead of belabouring the ideas of “featherless”, “biped”, and “mortal”, just say “human”; your reader will catch these properties and a lot more.</p>
<p>(I suspect that giving one concrete example, like that of “human” above, can obviate a whole bunch of words simply by conveying richer information, especially if it is highly representative of that category.)</p>
<p>Basically, your writing should work by specifying only the diagnostic key to the category. That’s how you compress messages. The reader should be able to infer all the other properties the few words you specify. Those ideas should follow naturally. If you’re repeating ideas ad nauseam, you haven’t hit upon the few characteristics that uniquely identify this concept.</p>
<hr />
<p>So, first of all, usage of a word is falsifiable. You really can’t just use words any way you want. A word refers to a category, which in turn gives you membership tests you can use on a part of reality. Each category constrains observable variables in some way; it forbids certain outcomes.</p>
<p>Next, a category is only as fine-grained as its membership tests. You decide whether a thing belongs to a category by a diagnostic test or by its similarity with the other things in the category. You can only infer things about a new member that all or most of the existing members have in common, like mortality in humans or flying ability in birds. As you get better tools, you can find out more things they have in common, like the 23 chromosome humans have. Or even things they don’t, like blood groups, in which case you split the category into subcategories.</p>
<p>In other words, you get different partitions of Thingspace when you choose different property-sets to form categories. These need not even be hierarchically organized like animal, vertebrate, feline, etc. When you choose “lives on land”, humans and snakes seem close together and whales seem far away; but when you choose “give birth to live babies”, humans and whales come close and snakes are far away. Both let you infer different properties (in the first case, can move on land and can deal with land predators; in the second case, is warm-blooded and has hair).</p>
<h1 id="bootstrapping-your-thingspace">Bootstrapping your Thingspace</h1>
<p>More precise observation tools like microscopes let you distinguish among more states than before, while inventions like X-ray machines let you observe completely different variables thus adding a new dimension to Thingspace. So, more precision means you form smaller, tighter clusters and find more similarities in each one, like, what looked like the same microbes to the naked eye might have some crucial differences in size and shape that let you distinguish between them and find specific cures. And new dimensions let you form totally different categories using the new variables, like diseases that cause bone damage.</p>
<p>So, more precision means splitting existing categories and more variables means forming totally different categories or just splitting old ones.</p>
<p>This is important because you either get information from your existing tools, or you get new or more precise tools. When you get information about an object from your existing tools, you just add it to your Thingspace and use similarity based on existing variables to decide where the category boundaries are. When you get a more precise tool, you update your similarity measure and thus redraw your category boundaries. Finally, when you get a new tool and thus a new variable, you measure where existing things stand with respect to the new dimension and maybe create new categories or just extend old ones.</p>
<p>So, each similarity measure defines one category. And the similarity measure depends on inputs from a set of observation tools. Take Eliezer’s example of choosing five properties like colour, shape, luminance, texture, and interior; you get one category - the Blegg-vs-Rube category. You may not need all five properties to test for membership; the colour and shape may be enough to let you infer the others. However, if you get new tools or ones with more precision, you can create new similarity measures that include them, and thus get new categories.</p>
<p>Earlier, you had plain-old categories, using imprecise tools or only a few features; now, you can have finer-grained, feature-rich categories. Say you get a new tool measuring elasticity. Earlier, even if you could predict the luminance from the colour and shape, you were still clueless about the object’s elasticity and thus could not decide whether to play tennis with it. Now, even if the new tool is primitive and imprecise, telling you only whether you can bounce the object but not how much, you can make your decision more confidently than before. You are now less uncertain than you were, which is the point.</p>
<p>Shouldn’t you use all the variables you have when creating categories? No, only when they have mutual information. What does that mean? If knowing the output of one observation tool tells you something about the output of another, then you should probably put them together in one category aka use them in the similarity measure of that category.</p>
<p>Your work is not done after you define similarity measures, though. You still have to decide on each category’s diagnostic features: the smaller set of properties that is mostly sufficient to let you infer the rest. How do you do this? (TODO)</p>
<h1 id="current-conclusion">Current Conclusion</h1>
<p>So, how does all this help answer my question about variables and reductionism? Well, you use a category as a variable. That’s the answer. You are free, of course, to choose a random set of observational tools to form your own category, mutual information be damned. But, because those properties won’t be correlated with each other, you will not be able to get small keys with which to infer the rest. The results you get could be more succinctly expressed by splitting that category apart.</p>
<p>Corollary: To become less uncertain about the world, figure out categories where the properties have high mutual information. Now, knowing just a few properties about a member, you can learn the rest for free. You get a lot of information for a little.</p>
<p>Note that you never have a floating “variable” that is free of any connection with an observational tool. This variable would not depend on the state of the world in any way, and thus knowing its “value” wouldn’t help us decrease our uncertainty.</p>
<p>I was concerned earlier because people seemed to be using high-level concepts that seemed to not be fully defined in terms of obvious features. Like depression, for example. It wasn’t a straight-forward observable (no depress-o-meter existed) and it connected different ideas like explanatory style, mood, physiological behaviour, etc.</p>
<p>The most worrying part about such high-level concepts, however, was that there always seemed to be something more that the researchers had in mind. The diagnostic symptoms were just the tip of the iceberg. If they said you were depressed, they could also predict how likely you were to quit your job or commit suicide or be a female or be older than 35. My mind boggled at such facts. I kept failing the acid test of understanding: can I make a computer do this? How could I? They never made all of these other facts explicit. It was all just understood. My (hypothetical) computer program would just stare blankly if asked to predict the rate of suicide. That wasn’t in the experimental data at all.</p>
<p>Now, I think I understand. “Depression” formed a category in their mind. I suppose the scientists in a field gain this knowledge by reading a lot of other papers, some of which will tell you the suicide rate among depressives. The “depression” category itself isn’t explicitly stored anywhere. It’s all in their heads, and implicitly in the mass of research papers and books that makes up a field.</p>
<p>However, remember that a category is explained <em>completely</em> by the observational tools you use to decide its membership. There is no hidden <em>essence</em> of a category that remains even after you read off all its member properties. Once you know the colour, shape, luminance, texture, and interior of the Blegg-vs-Rube members, there is nothing more to know. So, yes, researchers talking about “depression” may have a lot of other properties in mind, but we can track those down, and they all come down to things you can observe. There’s no magical “understanding” they have that you can’t have.</p>
<p>But it’s not all good news. Just because you can technically track down all the properties doesn’t mean it’s easy or painless. There’s no single database (that I know of) where you can read off the properties for some category like “depression”. It’s scattered over several books and important research papers that you are assumed to have read.</p>
<p>Ah! This tip-of-the-iceberg nature of categories might be why you need to have a <em>lot</em> of background knowledge in any field. To infer all the consequences of a new finding, you need to know what other properties are implied by the category. Similarly, to flag down discrepancies, you must know the properties implied, like when a depressive matches every other property but is more productive at his job - something is wrong. So, there’s a lot hidden behind the high-level variables in a field (like “depression” or “tiger” or “functional programming”).</p>
<p>I’m still not convinced, though. Do you really need to know all of those properties? Also, isn’t a small diagnostic key enough to let you infer the rest of the properties? Do you need to store the entire category in mind? Can’t you refer to it when needed?</p>
<p>Summary about words: Words are more than just their dictionary definition. They imply a whole lot more.</p>
<h1 id="concept-space-and-the-limits-of-knowledge">Concept-space and the Limits of Knowledge</h1>
<p>Say you have 40 binary variables. By choosing different values for each variable, you have 2^40 possible objects you can see. Now, a concept is a rule that includes or excludes examples. A concept may include or exclude any particular object - so, there are 2<sup>(2</sup>40) concepts!</p>
<p>To predict what will happen in the future, you assert that all the data you’ve seen so far is explained by a particular concept and that all future data will be explained by it as well. However, for this you need to pick out exactly the right concept out of two-power-trillion concepts (in the above toy example with just 40 binary variables)! That means you need to observe log2(2<sup>(2</sup>40)) = 2^40 = one trillion examples to come to the right answer. Each example gives you one bit of information - whether it obeys the rule or not. And remember, this was a toy scenario with just 40 binary variables aka just 40 bits or 5 bytes.</p>
<blockquote>
<p>So, here in the real world, where objects take more than 5 bytes to describe and a trillion examples are not available and there is noise in the training data, we only even think about highly regular concepts. A human mind - or the whole observable universe - is not nearly large enough to consider all the other hypotheses.</p>
<p>From this perspective, learning doesn’t just rely on inductive bias, it is nearly all inductive bias - when you compare the number of concepts ruled out a priori, to those ruled out by mere evidence.</p>
</blockquote>
<blockquote>
<p>The way to carve reality at its joints, is to draw <em>simple</em> boundaries around concentrations of unusually high probability density in Thingspace.</p>
</blockquote>
<blockquote>
<p>The probability distributions come from drawing the boundaries.</p>
</blockquote>
<p>What does that mean? I think your probability distribution is over the concepts <em>within</em> the concept space you’ve chosen. So, if you restrict your scope to just concepts that obey a certain format, like a decision tree, then your probability mass is divided over concepts that look like decision trees. This set of all decision trees is a subset of the much vaster general concept space, and I suppose this is what Eliezer means by drawing boundaries to shrink your concept space.</p>
<h1 id="still-unanswered-questions">Still unanswered questions</h1>
<p>How do we figure out the diagnostic key?</p>
<p>Crucially, how do categories overlap with causal models? If categories are based on mutual information (which is conditional dependence), won’t causal models cover that? If you know A causes B and C, for example, then B and C will be correlated. Isn’t it more parsimonious to let causal models contain that information instead of representing it in your categories too (like having A, B, and C in a category with A as the key)?</p>
<p>Wait. Now, what does “A causes B” mean, given that A and B are categories?</p>
<p>Why talk about categories at all? Why not just deal in terms of causal models? Aren’t they more richer and more compact? Can’t you do with a causal model everything that a category does? Why do we even have “words”? If it’s about succinctness, why not transmit causal information? Is it that we don’t always have causal information? When would that be? Maybe categories are for places where you still haven’t broken down the causal structure.</p>
<p>Is the trillion bits analogy valid at all? Don’t we know that the universe runs on the laws of physics? (How uncertain are we about that?) Isn’t it just a matter of logical deduction from there on?! But you have bounded resources, right?</p>
<p>What is inductive bias? Does Bayes Theorem incorporate inductive bias?</p>
<p>Doesn’t causal thinking and locality of causality factorize our uncertainty? Won’t that bring down the trillion-bits uncertainty down to something manageable? Put another way, how many bits of information do we need to get a causal model that can predict everything we want? What are the assumptions that help us compress the concept space so much? Is it the causal assumptions like Causal Markov Condition and Faithfulness and such?</p>
<p>In short, what is the best path ahead for making the most productive decisions and knowing the most about the world? Should I focus fully on applying the causal model idea everywhere? What are its weak points?</p>
<p>Best of all, just give me one fully-specified structural causal equation. Just one. It can be as simple as you like. All it should do is represent X = f(A, B, C) and show how if you “manipulate” A, then X changes as per f. To make it even easier, maybe do this first in a simpler made-up universe or in a programming context. Then, move on to the real world.</p>
<hr />
<p>Answer the reductionism questions. How does Bayes Theorem work, etc. I suspect they’re about updating your beliefs, about coming to accurate maps of the territory. And yes, if you have categories backed by impoverished tools, you will have an uncertain map of the territory, like humans with our models over “the wings of a plane” and not the quarks comprising it.</p>
<p>Give me explicit examples of categories, especially variables as categories. It’s fine when you have a physical object whose properties you can measure. What about hidden variables like the neurological changes that comprise depression? I think they are completely described by the observables you can measure right now. But, what if they predict certain things about observables you don’t have access to, like the probability of depressives committing suicide (if you haven’t measured that)? I think that you consider each such hypothesis as a separate point in Thingspace. Like the normal theory about depression plus a 10% rate of suicide vs another one with a 15% rate of suicide, and so on.</p>
<p>Test: Show that this whole idea of categories made of properties with mutual information actually lets you compress your knowledge. Show me concrete examples. (Biological taxonomy is one, I think.)</p>

<div class="info">Created: November 12, 2015</div>
<div class="info">Last modified: November 21, 2015</div>
<div class="info">Status: in-progress notes</div>
<div class="info"><b>Tags</b>: notes, variables</div>

<!-- <div id="sequence-navigation" style="text-align: right"> -->
<!--   <p>Part of <a href="./sequences.html"><i>No Sequence</i></a> -->

<!--   <p>Previous post: "<a href="">Start of Sequence</a>" -->

<!--   <p>Next post: "<a href="">Head of Sequence</a>" -->
<!-- </div> -->

<br />
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'spkrationalitytrainingground'; // required: replace example with your forum shortname
    var disqus_identifier = '/Variables.html';
    var disqus_title = 'Variables';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
