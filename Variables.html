<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<link rel="icon" type="image/png" href="./images/favicon-32x32.png" sizes="32x32" />
	<link rel="icon" type="image/png" href="./images/favicon-16x16.png" sizes="16x16" />
        <title>Variables - SPK's Rationality Essays</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />
        <link rel="stylesheet" type="text/css" href="./css/highlight.css" />

	<!-- <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script> -->
	<!-- <script type="text/javascript" src="/js/header-links.js"></script> -->

	<!-- Google Analytics stuff -->
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-51321929-1', 'pradeep90.github.io');
	  ga('send', 'pageview');

	</script>

    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">Rationality Essays</a>
            </div>
            <div id="navigation">
                <a href="./">Home</a>
                <!-- <a href="/sequences.html">Sequences</a> -->
                <a href="./notes.html">Notes</a>
                <!-- <a href="/about.html">About</a> -->
                <a href="./archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
          <h1 id="post-title">Variables</h1>

            <!-- <center><img src="https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/t31.0-8/p600x600/10257116_10202295769100492_2438594605053717342_o.jpg" height="400" width="300" class="sujeet-pic" alt="Sujeet pic" /></center> -->

<h1 id="what-are-variables">What are variables?</h1>
<p>How do you create a variable?</p>
<p>Concrete examples.</p>
<p>Solve this and everything else will fall into place, I think.</p>
<p>I’m not saying it will be a cakewalk from here, but one way or another you will learn the true limits of the scientific method.</p>
<h1 id="how-do-programmers-deal-with-change">How do programmers deal with change?</h1>
<p>They look for hot spots - parts that can change - and then encapsulate the change to minimize the work needed to change it. For example, you’re printing five variables into a report. You know that you may change whether the fourth and fifth variable occur. So, you will either have the first three or you will have all five, rarely anything else. Now, given that we want one-button changes, the best way to go forward is to encapsulate the fourth and fifth variables into one variable and allow yourself to toggle them with just one change.</p>
<p>In short, you want to handle the most changes with the least effort. The only trouble is that you don’t <em>know</em> which changes you’re going to make. If you knew that some circumstance of fate would make you change the sixth, thirteenth, and fourty-fourth variables, you would set up a single button that would let you do exactly that change and nothing else.</p>
<p>So, the length of your program should reflect, I think, the number of bits of uncertainty you have about what you will want from your program in the future. It’s about minimizing the number of bits the future-you would have to transmit to the program you write. If there are four types of changes, all of which you expect to be equally likely, then you need two on-off buttons. Off-off will refer to the first change, off-on will refer to the second, on-off to the third, and on-on to the fourth.</p>
<p>The actual changes you need to make don’t matter. They’re baked in: you’re not uncertain about them and you can’t do anything about them. If in the future you need to make change X, you need to do all that X entails, no getting around it. You’re just uncertain about which change you would need to make.</p>
<h1 id="unlikely-changes">Unlikely changes</h1>
<p>Now we’ll take the case where some changes are more likely than others. Say you have just two types of changes, but you expect the first one to occur three times as often as the second one. That is, you expect the first one with 0.75 probability and the second one with 0.25 probability. The future-you knows this about you. So, how long a message does he need to send to inform the program about the change that he needs to make?</p>
<p>From information theory, we know the number of bits needed to represent an outcome of probability p is -log2(p). So, he needs -log2(0.75) = ~0.42 bits to represent the first case and -log2(0.25) = 2 bits to represent the second case.</p>
<p>(Note, there is also the case where no change is needed and we need to represent that default message too.)</p>
<p>A corollary is that if you knew exactly what you would need a program to do in the future, then you wouldn’t need any buttons for change at all. You could just hardcode the program and use it forever. Similarly, if you knew very little about what would change, you would have to have lots and lots of buttons for change. You couldn’t hardcode anything. You would have to make everything abstract, in case it changed value; like extracting variables instead of using magic numbers, extracting functions in case you need to either change the logic or re-use the same logic elsewhere, etc.</p>
<p>Why do we encapsulate change (by extracting functions and such)? To minimize the work needed to change the logic in the future. If you’ve repeated the same logic in N places, you need to change N locations or risk having a wrong program. By extracting a function with that logic, you save work on the order of N, which is usually just two or three.</p>
<h1 id="the-upshot">The Upshot</h1>
<p>What can we conclude? I think the size of a program represents how uncertain you are about its future changes. Programmers want to minimize the amount of work to make any change needed in the future, so they design their programs to make likely changes easy and unlikely changes hard. (You can’t make both types of change easy.) And since the ease of a change is generally about the number of lines of code changed, this means that you would need to modify only a few lines for an expected change, and a lot of lines for an unexpected change.</p>
<p>So, experienced programmers will probably write code that is easy to change over time, because they know exactly what changes to expect in the future.</p>
<h1 id="abstractions">Abstractions</h1>
<p>I suspect that we decouple parts of a system when we’re confident that they won’t affect each other in the future. When designing a car, you think about the colour scheme independently of the design of the engine, because you expect that they have little or nothing to do with each other.</p>
<p>Abstraction means that we don’t care about what’s inside a box as long as it behaves a certain way on the outside. And for our abstraction not to leak, the inside of the box must be independent of the other parts of the system. Otherwise, our inferences will go wrong. We can justifiably believe that pressing the brakes will make our car slow down because there’s nothing interfering in the connection between the pedal and the engine. However, if someone comes and cuts the brake wires, our above assumption will be wrong and the car won’t behave the way we want it to.</p>
<p>So, we can decide on a level of abstraction for a problem by thinking about what parts of the system don’t interact with each other. That is the first higher-level assumption we have to make in our hypothesis. Everything else follows from our choice of abstractions.</p>
<p>We get powerful abstractions when we choose as big an aspect of the system that is independent of the other aspects most of the time. Should it be completely independent? How do we decide on abstractions for a particular system?</p>
<p>In short, an abstraction says that the inside of a box is independent of the rest of the system, given that it satisfies the interface. That is, P(Inside | Any Other Part, Satisfies Interface) = P(Inside | Satisfies Interface). Knowing about any other part of the system gives you no more information about the inside of this part, once you know that it satisfies some interface. How can we discover such parts of the system?</p>
<h1 id="the-need-to-know-principle">The Need-to-Know Principle</h1>
<p>How do we decide on our abstractions?</p>
<p>Here’s one idea: look at exactly what the other parts of the system need to know about this part. In programming, the GUI only needs to know about the data it needs to present, not the intricacies of the database behind the scenes. All the GUI needs to know about the rest of the program is that it can send data in a particular format, nothing else. This way, you can change the entire backend tomorrow and still be able to use the same GUI so long as you maintain the agreed data format. Your program is more modular and versatile when parts only know so much about each other as they need to know.</p>
<p>I suppose they call this the Minimum Information principle. The idea is that you can change everything else about this part except for its interface, and things should work just fine.</p>
<p>Key question between two parts of the system: What do you need to know about me? If nothing, then we are completely orthogonal.</p>
<p>So, the causal model isn’t about concrete “variables”. It’s about abstract interfaces. For example, anything that satisfies the interface of “providing a force F” on anything that satisfies the interface of “mass m” will make it satisfy the interface of “acceleration F/m”.</p>
<h1 id="black-boxes">Black Boxes</h1>
<p>Now, we have figured out that a causal model is about relationships between interfaces. What sort of relationships are these?</p>
<p>How can we ensure that these interfaces are valid? How can we make sure our abstractions are airtight? (I get reminded of the functor law that says <code>fmap id_x = id_y</code>, ensuring that no unnecessary effects take place. Basically, it keeps your abstraction from leaking.) Importantly, how can we leverage the idea of locality of causality?</p>
<p>But, firstly, what exactly is an “interface”? How do you start creating interfaces? Give me concrete examples.</p>
<p>One thing it does is tell you whether some collection of matter satisfies the interface or not.</p>
<p>Let’s take Newton’s Second Law of Motion: a = F/m. How do we decide whether something satisfies the interfaces of force, mass, or acceleration? The only thing that matters for “acceleration” is that your velocity should change over time (which in turn talks about your displacement).</p>
<h1 id="limiting-access-to-information">Limiting access to information</h1>
<p>The inside should be independent of the rest. That is, the rest of the system must depend only on the interface. This is physically impossible because as a chunk of matter you are in contact with the rest of the system and thus can be acted upon in several ways. I think it means that no matter what the rest of your characteristics are, the effects we care about in this causal model will depend only on your interface. If you added further tests, then your abstractions may not hold.</p>
<p>Programming abstractions are unique, I think, in the way they can limit the amount of information shared between modules. You can truly ensure that the only thing another module knows about you is what is passed through your interface.</p>
<p>Actually, is that true? At the level of a programming language, yes, we can’t distinguish between two concrete modules that have the same interface. However, they will certainly differ in their low-level implementations. If you are expert enough to inspect and interpret the binary code, you can differentiate between them.</p>
<p>So, the safety of programming abstractions comes about because it is hard to observe and manipulate raw binary code to do what you want (though crackers feel it’s worth it). High-level programming languages restrict what you can see or do, thereby protecting your abstractions from your own meddling and thus increasing your power. Otherwise, a module A can use hacks to access the innards of another module B (for efficiency, maybe) and break when you change B’s implementation.</p>
<p>Therefore, abstraction is about limiting access to information. You’re in trouble when you assume that modules can access less information than they actually can.</p>
<p>And this is probably where locality of causality helps us. It says that one part of the world can only access information from (i.e., cause or be affected by) the parts that are close to it in time and space. You have no way of accessing information from far away or from the distant past.</p>
<p>Still, if you could observe the part of the system precisely, you could find out more than the interface tells you.</p>
<p>Basically, our job becomes easier if certain effects depend only on particular characteristics, not on others. The speed of a car is not affected too much by the colour of the paint on the outside.</p>
<p>How can we find out what an effect doesn’t depend on? The same way as we know everything else: using the scientific method. Hypothesize things it might depend on, and vary them until you get to a level of abstraction that does change it. So, you first assert that the engine speed depends on the colour of the paint. Then, you change the paint colour several ways, and find that it doesn’t change the engine speed. So, you realize that they are probably independent.</p>
<h1 id="notes">Notes</h1>
<h2 id="what-dont-you-mark-as-variables">What don’t you mark as variables?</h2>
<p>You don’t encapsulate things that you don’t expect to change at all. These are things you’re nearly certain about.</p>
<h2 id="variables-and-instruments">Variables and Instruments</h2>
<p>How does precision of the instruments we have affect the level of abstraction of variables?</p>
<h2 id="random-thoughts">Random thoughts</h2>
<p>Rewriting essays (or programs) is so effective at clarifying thoughts because it makes you look for similarities between parts of your essays and abstract what is common.</p>
<h1 id="notes-on-words">Notes on Words</h1>
<p>(from Eliezer’s sequence on a Human’s Guide to Words)</p>
<p>Note: Keep in mind that a multi-level model exists only in your map, not in the territory. There is no Boeing 747 by itself, there is a configuration of quarks that meets the observational tests you apply to the 747.</p>
<p>Definitions exist to help us answer queries about <em>observables</em>. Whether Barney “is a man” depends on whether you want to feed him hemlock or want to marry him or something else (and thus want to know if he will die, make you happy, etc.).</p>
<p>You shouldn’t take out more information than you put in. That is, if you say that noticing the features “big”, “yellow”, and “striped” lets you identify something as a tiger, and <em>further</em> infer other features like “dangerous” and “fast”, then you must have previously observed that correlation for a sufficient number of animals. Else, you are making unjustified inferences, and might be wrong.</p>
<p>So, a category is a hypothesis and you need sufficient evidence to raise its posterior probability. In this case, you should see lots of instances where “big”, “yellow”, and “striped” animals had the other properties and very few instances where they didn’t (assuming that big-yellow-striped uniquely identifies the tiger category; otherwise, you need to add more diagnostic features, like whiskers and tail).</p>
<p>There’s nothing more to a category than the observables it can constrain. Once you answer that a blegg is blue, egg-shaped, etc. there’s nothing more to be said. There is no more information about a blegg from that category.</p>
<h2 id="compression">Compression</h2>
<p>How faithfully can we represent the real world in our mind? The universe is very big and our mental capacity is very small. So, it seems we have to accept a loss of detail when we compress the territory into our map.</p>
<p>What are the limits of our mental representation? One limit is the amount of evidence you can get. To have predictive power about a lot of things, you need a corresponding amount of evidence. I’m not sure about how many bits of information we can get (or even how to calculate the number of bits of information from some source). More prosaic limits are those of brain frequency, memory input rates, etc.</p>
<p>Basically, are there states of the world among which you cannot distinguish? Well, we’re limited by the precision of our senses. Our eyes cannot distinguish between two specks that are within nanometres in size of each other. We can’t see X rays, I think, or hear dog whistles. The events where someone blows a dog whistle or doesn’t will appear the same way to me if I can’t see them whistling and there’s no dog or other auditory instrument. The statement “the room is quiet” doesn’t differentiate between them.</p>
<h2 id="defining-categories">Defining Categories</h2>
<p>How should you define categories? Look at Thingspace and see which things are close to each other and draw a boundary that covers only those things and none others.</p>
<p>Wait. What is “Thingspace”? Take all the observation tools you have and you observe different configurations of quarks. Get their height, weight, colour, etc. by using your tools. Now, create an infinite dimension space with those features as dimensions and locate each “object” as a point in space. This is Thingspace.</p>
<p>I still feel this is not quite canonical. What do we mean by an “object”? Why take a whole object? Why not just one half of it or 2/15th?</p>
<h1 id="reductionism-and-variables">Reductionism and Variables</h1>
<p>In other words, I feel we’re still doing some unspecified, implicit mental work here. The key question is: can you teach a computer to do this, even in principle? We may not have the processor speed or memory to host a sufficiently powerful AI, but can you write a program that works even on a small part of the universe?</p>
<p>Better still, as a proof of concept, can you show that your program works for some made-up universe, something much simpler than our own universe? What are the questions you would like answered? What are the sources of evidence available to the program?</p>
<p>Here’s an unrelated question: why does Bayes Theorem work? If the universe is an automaton running the laws of physics, why does Bayes Theorem and the rest of math work? In fact, what does Bayes Theorem help us do?</p>
<p>In short, I’m not able to reconcile the ideas of reductionism and probability theory. I’m struggling to understand how we can use variables and probability theory (which talks in terms of those variables) in a universe that has only one level of reality and where things proceed as per some fixed rules (the laws of physics).</p>
<p>Personally, though, what is the precise query I care about? My question is: what variables should I use when trying to make a causal model that represents the knowledge in some textbook or research paper? Isn’t that the sensible thing to do - if causal thinking is the bee’s knees and very efficient and all, then shouldn’t you try to represent as much of your knowledge as possible in a causal model? Anyway, that’s what I plan to do. Why? Because I believe causal models will help me answer queries about the world more accurately than other forms of knowledge representation (including just the default mental maps we have). I believe they will also help me solve problems faster, by using Bayes Theorem to decide which experiments to run or what to observe so as to get the most information.</p>
<h1 id="words-as-messages">Words as Messages</h1>
<p>You want to convey information to another person (or yourself) and you want to use the shortest message you can to do so. The way to go is to use a binary string whose length is proportional to the expected frequency of that event. So, if you expect it to rain with 25% probability, then assign a string of 2 bits (since -log2(0.25) = 2 bits); since you expect that event to happen 25% of the time, the contribution of this event to the expected message length is 0.25 x 2 bits = 0.5 bits.</p>
<p>The diagnostic properties of a word (“mortal featherless biped” for human) should have mutual information about each other or about other properties (one heart, two lungs, etc.).</p>
<p><strong>Exercise</strong>: With all this knowledge about mutual information and Thingspace, how would you create a language from scratch?</p>
<p>Wait a minute! F*ck! If words describe clusters in Thingspace, then you had best ensure that your words are as succinct as possible! You want to capture exactly the things you care about and no others. PG was right:</p>
<blockquote>
<p>Maybe I’m excessively attached to conciseness. I write code the same way I write essays, making pass after pass looking for anything I can cut. But I have a legitimate reason for doing this. You don’t know what the ideas are until you get them down to the fewest words.</p>
<p>– Paul Graham, <a href="http://www.paulgraham.com/discover.html">Persuade xor Discover</a></p>
</blockquote>
<p>Hmmm… I think it’s not just words that hint at categories. Phrases and entire sentences probably do the same thing too. If the word “conciseness” helps you answer queries about the length of an essay or a speech, then PG’s statement “maybe I’m excessively attached to conciseness” helps you answer questions about the length of his average descriptions and about his unwillingness to publish something that isn’t concise and his favourite works and such, especially compared to other authors.</p>
<p>If you claim to talk about a bunch of important properties that are related to each other, i.e., they tell you something about each other, then you must have words or a short combination of words that captures it exactly. If they are important, they are likely probable and so must have short words or phrases assigned to them, even be it technical jargon. Also, if these properties have mutual information, then you probably have a short key set of diagnostic properties that lets you infer everything else, the way yellowed skin lets you infer jaundice and the appropriate treatment. You should be able to do better than explaining each concept in isolation. Instead of belabouring the ideas of “featherless”, “biped”, and “mortal”, just say “human”; your reader will catch these properties and a lot more.</p>
<p>(I suspect that giving one concrete example, like that of “human” above, can obviate a whole bunch of words simply by conveying richer information, especially if it is highly representative of that category.)</p>
<p>Basically, your writing should work by specifying only the diagnostic key to the category. That’s how you compress messages. The reader should be able to infer all the other properties the few words you specify. Those ideas should follow naturally. If you’re repeating ideas ad nauseam, you haven’t hit upon the few characteristics that uniquely identify this concept.</p>
<hr />
<p>So, first of all, usage of a word is falsifiable. You really can’t just use words any way you want. A word refers to a category, which in turn gives you membership tests you can use on a part of reality. Each category constrains observable variables in some way; it forbids certain outcomes.</p>
<p>Next, a category is only as fine-grained as its membership tests. You decide whether a thing belongs to a category by a diagnostic test or by its similarity with the other things in the category. You can only infer things about a new member that all or most of the existing members have in common, like mortality in humans or flying ability in birds. As you get better tools, you can find out more things they have in common, like the 23 chromosome humans have. Or even things they don’t, like blood groups, in which case you split the category into subcategories.</p>
<p>In other words, you get different partitions of Thingspace when you choose different property-sets to form categories. These need not even be hierarchically organized like animal, vertebrate, feline, etc. When you choose “lives on land”, humans and snakes seem close together and whales seem far away; but when you choose “give birth to live babies”, humans and whales come close and snakes are far away. Both let you infer different properties (in the first case, can move on land and can deal with land predators; in the second case, is warm-blooded and has hair).</p>
<h1 id="bootstrapping-your-thingspace">Bootstrapping your Thingspace</h1>
<p>More precise observation tools like microscopes let you distinguish among more states than before, while inventions like X-ray machines let you observe completely different variables thus adding a new dimension to Thingspace. So, more precision means you form smaller, tighter clusters and find more similarities in each one, like, what looked like the same microbes to the naked eye might have some crucial differences in size and shape that let you distinguish between them and find specific cures. And new dimensions let you form totally different categories using the new variables, like diseases that cause bone damage.</p>
<p>So, more precision means splitting existing categories and more variables means forming totally different categories or just splitting old ones.</p>
<p>This is important because you either get information from your existing tools, or you get new or more precise tools. When you get information about an object from your existing tools, you just add it to your Thingspace and use similarity based on existing variables to decide where the category boundaries are. When you get a more precise tool, you update your similarity measure and thus redraw your category boundaries. Finally, when you get a new tool and thus a new variable, you measure where existing things stand with respect to the new dimension and maybe create new categories or just extend old ones.</p>
<p>So, each similarity measure defines one category. And the similarity measure depends on inputs from a set of observation tools. Take Eliezer’s example of choosing five properties like colour, shape, luminance, texture, and interior; you get one category - the Blegg-vs-Rube category. You may not need all five properties to test for membership; the colour and shape may be enough to let you infer the others. However, if you get new tools or ones with more precision, you can create new similarity measures that include them, and thus get new categories.</p>
<p>Earlier, you had plain-old categories, using imprecise tools or only a few features; now, you can have finer-grained, feature-rich categories. Say you get a new tool measuring elasticity. Earlier, even if you could predict the luminance from the colour and shape, you were still clueless about the object’s elasticity and thus could not decide whether to play tennis with it. Now, even if the new tool is primitive and imprecise, telling you only whether you can bounce the object but not how much, you can make your decision more confidently than before. You are now less uncertain than you were, which is the point.</p>
<p>Shouldn’t you use all the variables you have when creating categories? No, only when they have mutual information. What does that mean? If knowing the output of one observation tool tells you something about the output of another, then you should probably put them together in one category aka use them in the similarity measure of that category.</p>
<p>Your work is not done after you define similarity measures, though. You still have to decide on each category’s diagnostic features: the smaller set of properties that is mostly sufficient to let you infer the rest. How do you do this? (TODO)</p>
<h1 id="current-conclusion">Current Conclusion</h1>
<p>So, how does all this help answer my question about variables and reductionism? Well, you use a category as a variable. That’s the answer. You are free, of course, to choose a random set of observational tools to form your own category, mutual information be damned. But, because those properties won’t be correlated with each other, you will not be able to get small keys with which to infer the rest. The results you get could be more succinctly expressed by splitting that category apart.</p>
<p>Corollary: To become less uncertain about the world, figure out categories where the properties have high mutual information. Now, knowing just a few properties about a member, you can learn the rest for free. You get a lot of information for a little.</p>
<p>Note that you never have a floating “variable” that is free of any connection with an observational tool. This variable would not depend on the state of the world in any way, and thus knowing its “value” wouldn’t help us decrease our uncertainty.</p>
<p>I was concerned earlier because people seemed to be using high-level concepts that seemed to not be fully defined in terms of obvious features. Like depression, for example. It wasn’t a straight-forward observable (no depress-o-meter existed) and it connected different ideas like explanatory style, mood, physiological behaviour, etc.</p>
<p>The most worrying part about such high-level concepts, however, was that there always seemed to be something more that the researchers had in mind. The diagnostic symptoms were just the tip of the iceberg. If they said you were depressed, they could also predict how likely you were to quit your job or commit suicide or be a female or be older than 35. My mind boggled at such facts. I kept failing the acid test of understanding: can I make a computer do this? How could I? They never made all of these other facts explicit. It was all just understood. My (hypothetical) computer program would just stare blankly if asked to predict the rate of suicide. That wasn’t in the experimental data at all.</p>
<p>Now, I think I understand. “Depression” formed a category in their mind. I suppose the scientists in a field gain this knowledge by reading a lot of other papers, some of which will tell you the suicide rate among depressives. The “depression” category itself isn’t explicitly stored anywhere. It’s all in their heads, and implicitly in the mass of research papers and books that makes up a field.</p>
<p>However, remember that a category is explained <em>completely</em> by the observational tools you use to decide its membership. There is no hidden <em>essence</em> of a category that remains even after you read off all its member properties. Once you know the colour, shape, luminance, texture, and interior of the Blegg-vs-Rube members, there is nothing more to know. So, yes, researchers talking about “depression” may have a lot of other properties in mind, but we can track those down, and they all come down to things you can observe. There’s no magical “understanding” they have that you can’t have.</p>
<p>But it’s not all good news. Just because you can technically track down all the properties doesn’t mean it’s easy or painless. There’s no single database (that I know of) where you can read off the properties for some category like “depression”. It’s scattered over several books and important research papers that you are assumed to have read.</p>
<p>Ah! This tip-of-the-iceberg nature of categories might be why you need to have a <em>lot</em> of background knowledge in any field. To infer all the consequences of a new finding, you need to know what other properties are implied by the category. Similarly, to flag down discrepancies, you must know the properties implied, like when a depressive matches every other property but is more productive at his job - something is wrong. So, there’s a lot hidden behind the high-level variables in a field (like “depression” or “tiger” or “functional programming”).</p>
<p>I’m still not convinced, though. Do you really need to know all of those properties? Also, isn’t a small diagnostic key enough to let you infer the rest of the properties? Do you need to store the entire category in mind? Can’t you refer to it when needed?</p>
<p>Summary about words: Words are more than just their dictionary definition. They imply a whole lot more.</p>
<h1 id="concept-space-and-the-limits-of-knowledge">Concept-space and the Limits of Knowledge</h1>
<p>Say you have 40 binary variables. By choosing different values for each variable, you have 2^40 possible objects you can see. Now, a concept is a rule that includes or excludes examples. A concept may include or exclude any particular object - so, there are 2<sup>(2</sup>40) concepts!</p>
<p>To predict what will happen in the future, you assert that all the data you’ve seen so far is explained by a particular concept and that all future data will be explained by it as well. However, for this you need to pick out exactly the right concept out of two-power-trillion concepts (in the above toy example with just 40 binary variables)! That means you need to observe log2(2<sup>(2</sup>40)) = 2^40 = one trillion examples to come to the right answer. Each example gives you one bit of information - whether it obeys the rule or not. And remember, this was a toy scenario with just 40 binary variables aka just 40 bits or 5 bytes.</p>
<blockquote>
<p>So, here in the real world, where objects take more than 5 bytes to describe and a trillion examples are not available and there is noise in the training data, we only even think about highly regular concepts. A human mind - or the whole observable universe - is not nearly large enough to consider all the other hypotheses.</p>
<p>From this perspective, learning doesn’t just rely on inductive bias, it is nearly all inductive bias - when you compare the number of concepts ruled out a priori, to those ruled out by mere evidence.</p>
</blockquote>
<blockquote>
<p>The way to carve reality at its joints, is to draw <em>simple</em> boundaries around concentrations of unusually high probability density in Thingspace.</p>
</blockquote>
<blockquote>
<p>The probability distributions come from drawing the boundaries.</p>
</blockquote>
<p>What does that mean? I think your probability distribution is over the concepts <em>within</em> the concept space you’ve chosen. So, if you restrict your scope to just concepts that obey a certain format, like a decision tree, then your probability mass is divided over concepts that look like decision trees. This set of all decision trees is a subset of the much vaster general concept space, and I suppose this is what Eliezer means by drawing boundaries to shrink your concept space.</p>
<h1 id="still-unanswered-questions">Still unanswered questions</h1>
<p>How do we figure out the diagnostic key?</p>
<p>Crucially, how do categories overlap with causal models? If categories are based on mutual information (which is conditional dependence), won’t causal models cover that? If you know A causes B and C, for example, then B and C will be correlated. Isn’t it more parsimonious to let causal models contain that information instead of representing it in your categories too (like having A, B, and C in a category with A as the key)?</p>
<p>Wait. Now, what does “A causes B” mean, given that A and B are categories?</p>
<p>Why talk about categories at all? Why not just deal in terms of causal models? Aren’t they richer and more compact? Can’t you do with a causal model everything that a category does? Why do we even have “words”? If it’s about succinctness, why not transmit causal information? Is it that we don’t always have causal information? When would that be? Maybe categories are for places where you still haven’t broken down the causal structure.</p>
<p>Is the trillion bits analogy valid at all? Don’t we know that the universe runs on the laws of physics? (How uncertain are we about that?) Isn’t it just a matter of logical deduction from there on?! But you have bounded resources, right?</p>
<p>What is inductive bias? Does Bayes Theorem incorporate inductive bias?</p>
<p>Doesn’t causal thinking and locality of causality factorize our uncertainty? Won’t that bring down the trillion-bits uncertainty down to something manageable? Put another way, how many bits of information do we need to get a causal model that can predict everything we want? What are the assumptions that help us compress the concept space so much? Is it the causal assumptions like Causal Markov Condition and Faithfulness and such?</p>
<p>I think the reply to this is that we artificially limit the number of variables when creating our causal models. A perfectly predictive model of a system would have as many variables as the quarks in it. This is the hidden piece I was missing so far. I was underestimating the number of variables. I kept thinking the models we have, like “smoking causes cancer” or even just “a = F / m” were sufficient to answer any questions we would have. Far from it. There is a critical assumption we make in causal modelling known as <em>causal sufficiency</em>: we assert that we have captured all the variables that might be causally linked to the ones we care about. And this goes out of the window once you realize that any of the quarks in your system might be manipulated into states that overturn the conclusions you draw, but you won’t predict that because they’re not part of your model.</p>
<p>In short, what is the best path ahead for making the most productive decisions and knowing the most about the world? Should I focus fully on applying the causal model idea everywhere? What are its weak points?</p>
<hr />
<p>Note: You don’t always need a <em>causal</em> model. If all you want to know is where the treasure is buried, just construct a Bayesian Network, do some PGM magic, and go get the treasure. You’re not intervening along the way. Similarly for inferring someone’s characteristics after hearing about their backstory: you don’t need a causal model per se.</p>
<hr />
<p>Answer the reductionism questions. How does Bayes Theorem work, etc. I suspect they’re about updating your beliefs, about coming to accurate maps of the territory. And yes, if you have categories backed by impoverished tools, you will have an uncertain map of the territory, like humans with our models over “the wings of a plane” and not the quarks comprising it.</p>
<p>Give me explicit examples of categories, especially variables as categories. It’s fine when you have a physical object whose properties you can measure. What about hidden variables like the neurological changes that comprise depression? I think they are completely described by the observables you can measure right now. But, what if they predict certain things about observables you don’t have access to, like the probability of depressives committing suicide (if you haven’t measured that)? I think that you consider each such hypothesis as a separate point in Thingspace. Like the normal theory about depression plus a 10% rate of suicide vs another one with a 15% rate of suicide, and so on.</p>
<p>Test: Show that this whole idea of categories made of properties with mutual information actually lets you compress your knowledge. Show me concrete examples. (Biological taxonomy is one, I think.)</p>
<h1 id="one-simple-example">One Simple Example</h1>
<p>Best of all, just give me one fully-specified structural causal equation. Just one. It can be as simple as you like. All it should do is represent X = f(A, B, C) and show how if you “manipulate” A, then X changes as per f. To make it even easier, maybe do this first in a simpler made-up universe or in a programming context. Then, move on to the real world.</p>
<p>Pragmatically, why do I object to abstract variables, like “depression”? Because I don’t feel it’s sufficient to cause its effects. But, why does it need to match my physical intuitions? Why can’t it just be mathematical? Because it has to work under intervention. How can you change the value of “depression”? There’s no single physical handle. I feel it violates locality of causality - how can it affect the other thing if it’s not even close? Maybe the category “depression” contains the other necessary conditions, like “is human”, “is alive”, etc.</p>
<p>In other words, I accept that you can observe an abstract variable (by consulting the observation tools for its category). What I don’t understand is how you can <em>change</em> its value. And I don’t see how that change propagates to produce the effects (which are also abstract variables).</p>
<p>So, here’s the challenge: find one simple example of a causal model, with abstract variables no less, and show how you can observe and manipulate the parent variables to change the child variable.</p>
<hr />
<p>Let’s take an example from the field of programming. Say, a Haskell function <code>f x y = x + y</code>. We can legitimately say that the value of the output of f depends on the values of x and y. We can “change” aka provide x and y as arguments. Here, f is an indivisible mechanism (a pure function in Haskell). You can observe x and y by using them in other functions or inspecting their values in the Haskell interpreter. Of course, you have to do the whole thing in a Haskell interpreter or executable program, otherwise it won’t work. That’s the base assumption. Wait, for that you need to assume that you have a computer and it has sufficient RAM and processing power to run the interpreter and that the computer is switched on and running.</p>
<p>Hmmm… so we work with several layers of abstractions. We assume that the Haskell interpreter is working fine, which in turn depends on the operating system working fine, which depends on the physical computer working fine, which depends on the power source and other factors.</p>
<p>In fact, we don’t really care too much about the layer below the Haskell interpreter. It wouldn’t matter if the computer were made of a shiny-new vegetable-based hardware, powered by cookies. As long as the interpreter let me do operations like <code>2 + 2</code> to get <code>4</code>, I would be fine.</p>
<p>Still, how am I confident that things will work? I haven’t tested out each Haskell operation to see if the interpreter hasn’t somehow been corrupted today. I haven’t peered into my laptop’s inside to see if there are any fuses. But, I predict that everything I try will work as usual… and it does! That sequence of successful narrow predictions gains my hypothesis a lot of confidence, as per Bayes Theorem.</p>
<p>So, the hypothesis that my Haskell interpreter plus operating system plus laptop plus power supply all “work correctly” is just one out of all possible hypotheses. I predict that when I open my Haskell interpreter and enter <code>2 + 2</code>, it will spit back <code>4</code>. Of all the configurations of matter that my laptop could be in, a very small percentage would lead to that particular outcome. The hypothesis saying that my laptop battery is broken, for example, predicts I won’t even be able to boot up my OS. Similarly, the hypotheses saying my hard drive is erased or that my keyboard is stuck predict that I won’t be able to do any work. So, my specific “all is well” hypothesis makes a highly confident, narrow prediction that pretty much no other hypothesis makes. And when I actually press those keys and see the result <code>2</code>, Bayes Theorem boosts my confidence in this hypothesis sky-high and downgrades nearly all others to oblivion.</p>
<p>Notice however that my “all is well” hypothesis isn’t even a very detailed one. I have no clue what’s going on inside my laptop battery. I don’t know the intimate details of the motherboard chips. To fully specify what I believe is the state of my laptop, I would need a huge amount of memory - proportional to the number of atoms inside it. I don’t know exactly what my laptop looks like inside, but I’m still able to make accurate predictions, which is all that matters.</p>
<p>Actually, that’s not fully true. I’m able to make accurate predictions for a certain class of questions. I can talk about whether my Haskell program above will work or not, which is still a non-trivial unique prediction, mind you; most other hypotheses say it won’t work. But I can’t really talk about exactly how many milliseconds it will take or how much memory it used. Wait. No, I can use <code>:set +s</code> to ask the interpreter to print exactly that information:</p>
<pre><code>Prelude&gt; 2 + 2
4
(0.02 secs, 3811272 bytes)</code></pre>
<p>Still, I can’t talk precisely about other things like the exact temperature in the battery or total weight of dust inside the casing. They are part of my laptop and could potentially interfere with its functioning (like if the battery temperature were a thousand degrees celsius). So, all I predict is that the battery temperature isn’t at catastrophic levels, or even further, that it is probably around the normal functioning temperature for a laptop battery, though I’m damned if I know what that is.</p>
<p>Coming back to the point, I can make narrow predictions only for a small class of queries, specifically those related to my laptop doing laptop-stuff. I want to browse the web on my laptop, so I care about whether it will be able to do so. I want to run Haskell programs, so I care about whether it will be able to do so. I don’t particularly want to use it to reheat pizza, so I don’t care whether it will be able to do so. Similarly, I don’t care if the motherboard will be safe enough to play frisbee with or whether the touch-sensitive mousepad will be suitable for use in a smartphone. These just form the tip of the iceberg. I haven’t even begun on what you can do if you get really creative and start rearranging the raw material of the laptop till it’s unrecognizable. It doesn’t matter.</p>
<p>I care about using my laptop for a specific range of activities and that’s all I will bother forming hypotheses about. I can’t help it. There’s just too much stuff in this world and I have only so much time and energy. I can’t look at every aspect of it. All reasoners with bounded resources will do this in one way or another, I suspect.</p>
<p>So, I will learn just enough about my laptop to help me predict whether it will suit my purposes. For a brand new laptop, I need to know very little about its innards, I can assume it will just work the way I want. With an aging laptop like mine, however, I need to be careful about how I use it. Over time, I have come to learn what applications slow it down, what specific actions trip bugs in the operating system, and how often I need to clean the fan. I have had to, otherwise my laptop becomes unusable.</p>
<p>Remember though that I am quite clear about what factors can possibly affect my laptop’s functioning or, more precisely, what factors can’t. The position of the moon, I’m fairly certain, doesn’t change my laptop speed; neither does the name of the current President of France. The direct causes of my laptop’s operation are the things that make it up and the inputs to it, like the internet cable (and thus the data that pass through it) or the power cord.</p>
<h1 id="the-smoke-vanishes-value-of-information">The Smoke Vanishes: Value of Information</h1>
<p>So, I have a rough idea of the important players in my laptop’s functioning, but I start with a relatively simple hypothesis about how they combine. I begin assuming that they will work just right (i.e., let me use my OS and applications). Over time, when my laptop defies my expectations, maybe by taking a long time to load an application or by making a loud noise, I add more detail to my hypothesis - like “a clean fan matters”, or “12 GB Bluray movies are just not happening”. Eventually, when enough things break, I won’t try to figure out how to reshape the atoms to get it to work again; I’ll just buy a new laptop. That way I don’t have to waste too much time trying to understand the physics, computer architecture, and other myriad arts needed for the job. I simply compare the cost of buying a new laptop with the cost of spending time doing something so hard.</p>
<p>That’s another insight: when you’re a bounded reasoner, you have to choose where to spend your resources. Studying something is never a purely intellectual decision. Principle of Economics #2: People face tradeoffs. Every action you take has an opportunity cost; you could be doing something else with your limited time on this planet. So, if you want to get something done, you have to trade off the time needed to figure out a devilishly complicated system vs the cost of a replacement that will do it.</p>
<p>In short, you need to consider the value of the information that you’re aiming to get. Decision theorists have worked out ways to calculate the precise Value of Information. Essentially, you look at how much better off you expect to be after you receive the information, and if it exceeds the cost of getting that information, you’re golden. The key is that information is useful only insofar as it can potentially make you change your decision. Only your decisions decide how much value you get and so information can add value only by potentially guiding you to better decisions. For example, if you haven’t any money left in your budget, there’s no point looking at the price tag of a shirt you like. Whether it is $20 or $2000, your decision is made: you’re not going to buy it, so you might as well save yourself the effort.</p>
<h1 id="goals-dictate-abstractions">Goals dictate Abstractions</h1>
<p>That’s great, but it still doesn’t answer the question of how you go about improving your hypothesis. No, the point is that you <em>only</em> care about your terminal goals and things that help or hinder achieving them. So, you don’t simply create a hypothesis involving all the possible variables you can see in a system. You just look at those variables that matter to your terminal or instrumental goals.</p>
<p>If you just want a car that will get you from point A to point B cheaply, you ignore the paint colour, air-conditioning, upholstery, or cup holders. You build a hypothesis involving only the price and mileage and other such sensible properties. You figure out that if you drive at a steady speed of 60mph (or whatever), you save the most fuel. You work out the optimal number of tune-ups the engine needs to run efficiently while not wasting too much money on the car service. You note that inflating the tires well saves you money and so on. All the while, you ignore that if you wash your car every week, it will look shinier and prettier, or that an air-freshener will make it smell better; it simply doesn’t matter to you.</p>
<p>Your goals dictate your abstractions. They tell you what to ignore and what to emphasize. Your way of thinking is tightly coupled with your economic conditions. It has to be - you’re a bounded reasoner, you can’t just act without caring about the cost.</p>
<h1 id="no-fully-general-models">No Fully-General Models</h1>
<p>I went astray because I assumed there was some one-size-fits-all way of thinking that would work the same way on any subject matter. I thought that once you understood something called “the scientific method”, you will be able to have more predictive power about any domain in the world. Well, that is still true, but here’s the catch: you won’t want to! At any point, there’s some information that will provide you the most value for your time, and learning anything else is probably a waste of time by comparison. You likely won’t want to understand dozens of complicated topics in depth, because you can always focus on one field and gain from your scarcity power and simply purchase whatever else you would have done in those other fields.</p>
<p>My folly was that I hoped to come up with a fully-general model of a system, one that could answer as near any question you could have. That is impossible, given the resource constraints not just of our puny human minds but the entire universe! You have to create abstractions, and I suspect we do that fruitfully by aligning them with our goals.</p>
<h1 id="science-vs-rationality">Science vs Rationality</h1>
<p>The aim is to get utilons, not knowledge. If there were some trick to achieve your goals without having to spend time acquiring the knowledge, you would probably take it. You should. Keep your eye on the ball.</p>
<p>But yes, once you decide on your goals, you can definitely use the scientific method to create hypotheses using the right abstractions and be off on your way. And yes, I do think it will work on any domain. The methods for reducing uncertainty work as well on figuring out engineering problems as they do for solving romantic ones.</p>
<p>And sometimes, it may not involve using the scientific method at all. You may just decide that it’s cheaper to just buy a new laptop instead of doing research by yourself to figure out how to fix your old one.</p>
<p>Hmmm… then it may not be so wise to have a self-image as a scientist. As a “scientist”, you would focus on getting to the truth about things, on getting predictive power about a system. But that goal should be subordinate to actually achieving your terminal goals. In particular, you must refrain from attacking a deliciously complex intellectual problem simply because your scientist-sense is tingling, when you can easily meet the same end by buying a cheap commodity and moving on to more valuable problems.</p>
<p>On the other hand, we humans do seem to have the constraint that “to do something well you have to like it” (<a href="http://www.paulgraham.com/love.html">PG</a>). And to fan the flames of love for science, you would probably need to seek intellectual challenges for the thrill of it. I don’t know how to balance these two pulls.</p>
<p>Anyway, science is about getting predictive power about the world. Rationality is about achieving your goals. What you want is to achieve your goals. Everything else is incidental.</p>
<h1 id="values-are-complex">Values are Complex</h1>
<p>A warning: remember that your terminal goals are complex. There is no one variable alone that you want to maximize (like happiness or virtue or whatever). You need a lot of things to go well in life. This suggests that your abstractions will be quite complex too. (You can’t ignore the fumes coming out of your car just because it doesn’t cost you; other people’s quality of life matters to you too.) Unless… you can partition your decisions such that you achieve different values independently of each other, to the extent possible. I don’t know about this. Maybe this means specializing in one area and having powerful, predictive, complicated hypotheses about that narrow domain and trading with other specialists for mutual gain.</p>
<h1 id="your-thoughts-need-constraints">Your Thoughts need Constraints</h1>
<p>Maybe resolve the pulls of the aesthetics of science and the value of economics by specializing in solving intellectual problems and put your services for hire. This may be what academia is meant to do. However, to create good abstractions and simplify problems, you <em>need</em> to have goals. You can’t just build useful models from the safety of your ivory towers.</p>
<p>Your thoughts need the constraints of your goals. Else, you will be overwhelmed by the complexity of even a single leaf.</p>
<p>A corollary is that there’s no point in “studying” something without a goal. There’s nothing to inform your abstractions there. You will probably waste time barking up the wrong tree.</p>
<p>You want to become skilled enough at scientific thinking to be able to solve pretty much any problem placed in front of you, but you will still need a goal to orient your thinking. No fully-general models, remember. You have to carve reality at the joints somewhere, and your goals will help you do so (that’s my hypothesis).</p>
<p>If your aim is to know everything about everything or even something about everything, you’ve failed before you’ve even begun. You have bounded resources.</p>
<p>Bounded resources mean that your thinking is bundled up tightly with your economic condition. In other words, information theory becomes bedfellows with economics.</p>
<h1 id="world-in-a-bottle">World in a Bottle</h1>
<p>You say you will only care about variables that matter to your goals. But, what about variables that you can measure? Do you ignore them? I can see you ignoring the colour of your car, but that’s because it’s not correlated with the mileage or speed. What if some variable is correlated? Well, if it causes something I care about, then of course I will model it, like the aerodynamics of the car’s shape. But what if it is only a side-effect? It may help me measure the value of hidden variables; like the composition of my car’s exhaust fumes - I don’t care about it directly, but it can help me detect if my engine is burning fuel inefficiently.</p>
<p>The real question is whether you will use them to update your model’s posterior probability. Technically, whenever you observe a variable, you’re supposed to make predictions and then update your posterior probability based on the actual result. But, then your hypothesis talks only about a small aspect of a small part of the world. It makes no useful predictions about anything else. If you were to apply Bayes Theorem strictly, it would be judged to make random predictions and thus go down in posterior probability. So, I guess we restrict our use of Bayes Theorem to a compressed, simplified model of the world. Maybe if we discover that some other variable is correlated with the modelled variables, we can extend our model to cover it. In general, we only have so much time, so we neglect the world outside our model.</p>
<p>Hmmm… so this is what gives rise to the Platonic fold that Taleb warns about in The Black Swan. He claims that it is precisely those things that lie outside our model that come to bite us later. Still, there’s nothing to it. You have bounded resources and you’re making the best guesses you can, so you suck it up when they fail.</p>
<p>Use your goals to abstract the system into a tiny universe and apply Bayes Theorem to come to your best guesses of what will happen within that universe. Yes, you will probably not achieve complete accuracy, but it’s the best you can do.</p>
<h1 id="to-do">To do</h1>
<p>Test my hypothesis that goals help you create abstractions. Test whether all the causal models you’ve seen so far match this idea - do they match some specific purpose? What about the different subjects in the world, like biology or architecture? Are their abstractions in line with their goals?</p>
<p>Also, actually show (with at least one concrete example) how goals help you create usable abstractions. What will you ignore?</p>
<p>Further, take a real-world simple causal model and show how manipulation works with abstract variables.</p>
<h1 id="to-do-later">To do later</h1>
<p>My revised aim: Now that I realize that studying something without goals is just futile, I need to figure out what kinds of goals I will specialize in and what sorts of abstractions they need to use. This will help me study those topics better.</p>
<h1 id="lesson">Lesson</h1>
<p>Dammit, work <em>only</em> using concrete examples. It’s not just recommended, it’s mandatory! See how I was flailing around above before I took one narrow, specific, concrete, not-even-very-imaginative example and started digging in earnest. Just like Robert Pirsig’s advice of zooming in on one <em>specific</em> brick and writing down what you see.</p>
<p>And, my word, it really is a different ball game once you write for more than an hour or two at a time. Ideas start popping out of nowhere past that magical mark. This supports PG’s <a href="http://www.paulgraham.com/head.html">hypothesis</a> that it takes you so long to simply load the details of your program or essay into your head and explore its edges at ease.</p>

<div class="info">Created: November 12, 2015</div>
<div class="info">Last modified: November 22, 2015</div>
<div class="info">Status: in-progress notes</div>
<div class="info"><b>Tags</b>: notes, variables</div>

<!-- <div id="sequence-navigation" style="text-align: right"> -->
<!--   <p>Part of <a href="./sequences.html"><i>No Sequence</i></a> -->

<!--   <p>Previous post: "<a href="">Start of Sequence</a>" -->

<!--   <p>Next post: "<a href="">Head of Sequence</a>" -->
<!-- </div> -->

<br />
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'spkrationalitytrainingground'; // required: replace example with your forum shortname
    var disqus_identifier = '/Variables.html';
    var disqus_title = 'Variables';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
