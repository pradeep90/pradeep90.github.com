<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<link rel="icon" type="image/png" href="./images/favicon-32x32.png" sizes="32x32" />
	<link rel="icon" type="image/png" href="./images/favicon-16x16.png" sizes="16x16" />
        <title>Seek High-Entropy Situations - SPK's Rationality Essays</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />
        <link rel="stylesheet" type="text/css" href="./css/highlight.css" />

	<!-- <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script> -->
	<!-- <script type="text/javascript" src="/js/header-links.js"></script> -->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

	<link href="atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed" />

	<!-- Google Analytics stuff -->
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-51321929-1', 'pradeep90.github.io');
	  ga('send', 'pageview');

	</script>

    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">Rationality Essays</a>
            </div>
            <div id="navigation">
                <a href="./">Home</a>
                <a href="./notes.html">Notes</a>
                <!-- <a href="/about.html">About</a> -->
                <a href="./archive.html">Archive</a>
		<a href="./atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed">RSS</a>
            </div>
        </div>

        <div id="content">
          <h1 id="post-title">Seek High-Entropy Situations</h1>

            <!-- <center><img src="https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/t31.0-8/p600x600/10257116_10202295769100492_2438594605053717342_o.jpg" height="400" width="300" class="sujeet-pic" alt="Sujeet pic" /></center> -->

<p>(This post is still in progress.)</p>
<h1 id="ball-weighing">Ball-Weighing</h1>
<p>Here’s a puzzle (paraphrased from David JC MacKay’s wonderful textbook on Information Theory):</p>
<blockquote>
<p>You are given 12 balls all weighing the same, except for one which is lighter or heavier (you don’t know which). You are also given a weighing scale that can tell you whether the left pan is heavier or the right pan is heavier or whether they are equal.</p>
<p>What is the least number of weighings to identify the odd ball and whether it is lighter or heavier?</p>
</blockquote>
<p>How would you do this? Take your best shot.</p>
<p>(spoiler space)</p>
<p><br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /></p>
<p>Did you get it? The answer is 3 weighings. Can you prove how? I was stuck on this for a while.</p>
<p>What are our options? On the first weighing, we can choose to weigh 6 balls on each pan or 5 or 4 or 3 or 2 or 1.</p>
<p>We want to find the correct ball (along with its relative weight) as quickly as possible. Now, we can do this in an ad hoc manner, just going with our hunches. But let’s instead try to use a more systematic method. How can we frame this in terms of information theory?</p>
<h1 id="information-is-wealth">Information is Wealth</h1>
<p>Well, we have 24 hypotheses: H1 says that ball 1 is different from the rest and is lighter; H1’ says that ball 1 is different and is heavier; H2 says that ball 2 is different and lighter; H2’ says ball 2 is different and lighter; and so on till H12 and H12’. Which of them is the correct one? We don’t know. So we assign the same probability to all of them: 1/24.</p>
<p>Our job is now to get the correct hypothesis to probability 1. How? By getting evidence and thus <a href="./reducing-information-theory.html">multiplying our confidence</a> in the correct hypothesis. And the fastest way to do that is by seeking high-entropy experiments. We have six possible types of experiments: 6 balls on each side, 5 balls on each side, etc. Right now, it doesn’t matter which balls we choose for the experiment because they are all the same to us.</p>
<p>What is the entropy of these 6 types of experiments? The entropy of an experiment (or ensemble or whatever it is you call something you’re unsure about) depends only on the probabilities you assign to the different outcomes. Nothing else matters. All your knowledge about your hypotheses and your distribution of confidence in them is captured in your final probability distribution over the outcomes.</p>
<p>So, we have to calculate our probability distribution over the 3 possible outcomes of a weighing: left-heavier, equal, and right-heavier.</p>
<p>(I could be off with the calculations here; been a while since I’ve done probability theory counting problems - they’re always tricky (for me).)</p>
<p>If we took 6 balls on each side, the probability of left-heavier is the probability of the odd ball being heavier and being on the left pan or the odd ball being lighter and being on the right side, which is 6/12 = 1/2. Probability of right-heavier is the same 1/2. Probability of equal is 0 (since one of the balls is different).</p>
<p>For 5 balls a side, the probability of left-heavier is 5/12, right-heavier is 5/12, and equal is 2/12 = 1/6. The quick way to calculate this is to just see that there are 5 spots for the odd ball on the left, 5 spots on the right, and 2 spots outside. Also, the ball can be heavier or lighter than the rest. So, there are 24 possibilities. You get left-heavier when the odd ball is on the left and is heavier, or when it is on the right and is lighter. So, there are 10 situations when you get left-heavier. Similarly, for right-heavier. The rest will give you a message of “equal”. Hence, we get odds of 10:10:4 and thus probabilities of 10/24, 10/24, and 4/24.</p>
<p>For 4 balls a side, the odds are 8:8:8 and so the probabilities are 1/3, 1/3, and 1/3.</p>
<p>For 3 balls a side, the odds are 6:6:12 and so the probabilities are 3/12, 3/12, and 6/12.</p>
<p>For 2 balls a side, the odds are 4:4:16 and so the probabilities are 2/12, 2/12, and 8/12.</p>
<p>For 1 balls a side, the odds are 2:2:20 and so the probabilities are 1/12, 1/12, and 10/12.</p>
<p>For quick reference, the various probability distributions are:</p>
<pre><code>6 balls: (1/2, 1/2, 0)
5 balls: (5/12, 5/12, 1/6)
4 balls: (1/3, 1/3, 1/3)
3 balls: (1/4, 1/4, 1/2)
2 balls: (1/6, 1/6, 2/3)
1 balls: (1/12, 1/12, 5/6)</code></pre>
<p>Which experiment will you pick? Well, the one that has highest entropy. Now, you could sit and calculate entropy using the formula <span class="math display">\[ \sum\nolimits -p\log_2(p) \]</span></p>
<p>Or you could just note that entropy is maximum when all the probabilities are equal. If there are n outcomes, then they will all have probability 1/n. Then, entropy becomes Sum( log2(n^(1/n))) which is just log2(n). So, the maximum entropy for an experiment of n outcomes is log2(n).</p>
<p>From the above experiments, the one with 4 balls on each pan has equal probability for all 3 outcomes, and thus the maximum entropy. We will pick that one.</p>
<h1 id="second-weighing-the-equal-case">Second Weighing: The equal case</h1>
<p>Now, you can get the messages “left-heavier”, “right-heavier”, or “equal” with equal probability.</p>
<p>Let’s take the “equal” case. This means that the odd ball is in the 4 balls not kept on the pan. This eliminates the hypotheses saying that the odd ball was on one of the pans. Now, we have to find which one it is and whether it is lighter or heavier. Again, what are our choices? We can put all these 4 balls from the odd group on one pan and the previous 4 balls from the normal group on the other pan. Or, we could put 3 from the odd group and 3 from the normal group. Or 2 odd and 2 normal. Or 2 odd and 2 odd. Or 1 odd and 1 odd.</p>
<p>The chief question is, of course, what is the entropy of these experiments? We want the one with highest entropy, remember. And that is satisfied when the probabilities of the three messages are as uniform as possible.</p>
<p>There’s no point putting all normal balls on the pans - we know for sure that the message will be “equal”, so there’s zero entropy there.</p>
<p>For the 4 odd and 4 normal trial, with the 4 odd on the left and the 4 normal on the right, we will get left-heavier if the odd ball is heavy, right-heavier if the odd ball is light, but we will never get “equal” because one ball is odd. This should be our first red flag. We want high entropy, where the probabilities are as nearly equal as we can manage (meaning, they are each close to 1/n). And here, one of the probabilities is zero! Anyway, the entropy is <code>-1/2 x log(1/2) + -1/2 x log(1/2)</code> = 1 bit.</p>
<p>Let’s see if the other experiments do any better. For 3 odd and 3 normal, again, the probability of equal is zero. This is not good. Again, since left-heavier and right-heavier have equal probability (1/2), the entropy is 1 bit as earlier. Ditto for 2 odd and 2 normal or 1 odd and 1 normal - 1 bit of entropy.</p>
<p>For the 2 odd and 2 odd trial, there are 4 possible spots for the odd ball, and 2 possible weights (lighter or heavier). So, 8 possibilities. We will get left-heavier if the odd ball is heavier and is on the left, or if it is lighter and is on the right. So, that is 4 out of 8 possibilities. Right-heavier is the same. There is no chance of equal again, since there is an odd ball. So, this experiment too has two outcomes with probabilities 1/2 and 1/2 and thus entropy of 1 bit.</p>
<p>For the 1 odd and 1 odd trial, there are 4 possible spots for the odd ball (on the left, on the right, and outside) and 2 possible weights. So, 8 possibilities. We get left-heavier when the odd ball is on the left and is heavier, or when it is on the right and is lighter - so, 2 possibilities. Similarly for right-heavier - 2 possibilities. The rest give “equal”. So, we get odds of 2:2:4 and thus probabilities of 1/4, 1/4, and 1/2 and thus entropy of <code>-1/4log2(1/4) + -1/4log2(1/4) + -1/2log2(1/2)</code> = 1.5 bits. Nice.</p>
<p>Can we do better? What about 1 odd and 1 normal on the left pan and 2 odd on the right pan? Then, there are 4 possible spots for the odd ball (on the left, on the right, and outside) and therefore 8 possibilities. We get left-heavier when the odd ball is on the left and is heavier (1 possibility), or when it is on the right and is lighter (2 possibilities) - so, 3 total possibilities. Similarly for right-heavier - 3 possibilities. The rest give “equal”. So, we get odds of 3:3:2 and thus probabilities of 3/8, 3/8, and 1/4 and thus entropy of <code>-3/8log2(3/8) + -3/8log2(3/8) + -1/4log2(1/4)</code> = ~1.56 bits. Nice. We pick this one.</p>
<p>We can see how the 1 odd and 1 odd trial and the 1 odd plus 2 odd trial were the only trials that even allowed the “equal” probability and thus got more uniform distributions, leading to higher entropy.</p>
<p>We can now get three possible messages again. If you get left-heavier, then either the left odd ball is heavier or one of the right odd balls is lighter. Now, we can compare the two right balls - if you get “equal”, then the left ball above was odd and heavier; if you get “left-heavier”, then the right ball is odd and lighter; if you get right-heavier, then the left ball is odd and lighter (since we saw above that one of them is lighter). This way, we get to full certainty with just three weighings.</p>
<p>We can do the same thing if we got right-heavier. And if we get equal, then it just means that the one ball left outside is odd. We can just test it with a normal ball to find out it if is lighter or heavier - again making for a total of 3 weighings.</p>
<p>Long story short, you can do the same for the other messages in the original weighing. TODO: If you got left-heavier for the 4-4 weighing, then you can choose …</p>
<h1 id="notes">Notes</h1>
<p>You want to maximize information gained. Expected information gain is nothing but entropy. And that is maximum when all outcomes are equally likely.</p>
<p>Intuitive way to calculate entropy: the flatter the probability distribution, the more the entropy; the spikier the distribution, the less the entropy. You want a spiky distribution. Insert the argument that if you get to a spiky distinction lawfully, you can’t expect to flatten again. You’re (on expectation) golden.</p>
<p>TODO: information = reduction in entropy of the ensemble; also, information in the case of maximum entropy; how to count number of weighings needed (and how you can always conjure an ideal code that gives you maximum entropy on each turn); application to real life.</p>
<p>Note: This is, in my opinion, the main missing piece in reasoning. Just knowing that you have to use Bayesian inference is not enough. It can be hard to know what to do - there are so many options. Entropy guides your search. Moreover, you don’t have to calculate it perfectly - more entropy is better than less. So, I suspect, you can use it as a compass to make your information-seeking choices. (How does it compare to the idea of strong inference, where you always aim to get differing predictions between hypotheses? Which one is better, more practical?)</p>

<div class="info">Created: January 10, 2016</div>
<div class="info">Last modified: January 27, 2016</div>
<div class="info">Status: in-progress</div>
<div class="info"><b>Tags</b>: reduction, information theory</div>

<br />
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'spkrationalitytrainingground'; // required: replace example with your forum shortname
    var disqus_identifier = '/seek-high-entropy.html';
    var disqus_title = 'Seek High-Entropy Situations';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
