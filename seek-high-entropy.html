<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<link rel="icon" type="image/png" href="./images/favicon-32x32.png" sizes="32x32" />
	<link rel="icon" type="image/png" href="./images/favicon-16x16.png" sizes="16x16" />
        <title>Seek High-Entropy Situations - SPK's Rationality Essays</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />
        <link rel="stylesheet" type="text/css" href="./css/highlight.css" />

	<!-- <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script> -->
	<!-- <script type="text/javascript" src="/js/header-links.js"></script> -->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

	<link href="atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed" />

	<!-- Google Analytics stuff -->
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-51321929-1', 'pradeep90.github.io');
	  ga('send', 'pageview');

	</script>

    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">Rationality Essays</a>
            </div>
            <div id="navigation">
                <a href="./">Home</a>
                <a href="./notes.html">Notes</a>
                <!-- <a href="/about.html">About</a> -->
                <a href="./archive.html">Archive</a>
		<a href="./atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed">RSS</a>
            </div>
        </div>

        <div id="content">
          <h1 id="post-title">Seek High-Entropy Situations</h1>

            <!-- <center><img src="https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/t31.0-8/p600x600/10257116_10202295769100492_2438594605053717342_o.jpg" height="400" width="300" class="sujeet-pic" alt="Sujeet pic" /></center> -->

<p>(This post is still in progress.)</p>
<h1 id="ball-weighing">Ball-Weighing</h1>
<p>Here’s a puzzle (paraphrased from David JC MacKay’s wonderful textbook on Information Theory):</p>
<blockquote>
<p>You are given 12 balls all weighing the same, except for one which is lighter or heavier (you don’t know which). You are also given a weighing scale that can tell you whether the left pan is heavier or the right pan is heavier or whether they are equal.</p>
<p>What is the least number of weighings to identify the odd ball and whether it is lighter or heavier?</p>
</blockquote>
<p>How would you do this? Take your best shot.</p>
<p>(spoiler space)</p>
<p><br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /></p>
<p>Did you get it? The answer is 3 weighings. Can you prove how? I was stuck on this for a while.</p>
<p>What are our options? On the first weighing, we can choose to weigh 6 balls on each pan or 5 or 4 or 3 or 2 or 1.</p>
<p>We want to find the correct ball (along with its relative weight) as quickly as possible. Now, we can do this in an ad hoc manner, just going with our hunches. But let’s instead try to use a more systematic method. How can we frame this in terms of information theory?</p>
<h1 id="information-is-wealth">Information is Wealth</h1>
<p>Well, we have 24 hypotheses: H1 says that ball 1 is different from the rest and is lighter; H1’ says that ball 1 is different and is heavier; H2 says that ball 2 is different and lighter; H2’ says ball 2 is different and lighter; and so on till H12 and H12’. Which of them is the correct one? We don’t know. So we assign the same probability to all of them: 1/24.</p>
<p>Our job is now to get the correct hypothesis to probability 1. How? By getting evidence and thus <a href="./reducing-information-theory.html">multiplying our confidence</a> in the correct hypothesis. And the fastest way to do that is by seeking high-entropy experiments. We have six possible types of experiments: 6 balls on each side, 5 balls on each side, etc. Right now, it doesn’t matter which balls we choose for the experiment because they are all the same to us.</p>
<p>What is the entropy of these 6 types of experiments? The entropy of an experiment (or ensemble or whatever it is you call something you’re unsure about) depends only on the probabilities you assign to the different outcomes. Nothing else matters. All your knowledge about your hypotheses and your distribution of confidence in them is captured in your final probability distribution over the outcomes.</p>
<p>So, we have to calculate our probability distribution over the 3 possible outcomes of a weighing: left-heavier, equal, and right-heavier.</p>
<p>(I could be off with the calculations here; been a while since I’ve done probability theory counting problems - they’re always tricky (for me).)</p>
<p>If we took 6 balls on each side, the probability of left-heavier is the probability of the odd ball being heavier and being on the left pan or the odd ball being lighter and being on the right side, which is 6/12 = 1/2. Probability of right-heavier is the same 1/2. Probability of equal is 0 (since one of the balls is different).</p>
<p>For 5 balls a side, the probability of left-heavier is 5/12, right-heavier is 5/12, and equal is 2/12 = 1/6. The quick way to calculate this is to just see that there are 5 spots for the odd ball on the left, 5 spots on the right, and 2 spots outside. Also, the ball can be heavier or lighter than the rest. So, there are 24 possibilities. You get left-heavier when the odd ball is on the left and is heavier, or when it is on the right and is lighter. So, there are 10 situations when you get left-heavier. Similarly, for right-heavier. The rest will give you a message of “equal”. Hence, we get odds of 10:10:4 and thus probabilities of 10/24, 10/24, and 4/24.</p>
<p>For 4 balls a side, the odds are 8:8:8 and so the probabilities are 1/3, 1/3, and 1/3.</p>
<p>For 3 balls a side, the odds are 6:6:12 and so the probabilities are 3/12, 3/12, and 6/12.</p>
<p>For 2 balls a side, the odds are 4:4:16 and so the probabilities are 2/12, 2/12, and 8/12.</p>
<p>For 1 balls a side, the odds are 2:2:20 and so the probabilities are 1/12, 1/12, and 10/12.</p>
<p>For quick reference, the various probability distributions are:</p>
<pre><code>6 balls: (1/2, 1/2, 0)
5 balls: (5/12, 5/12, 1/6)
4 balls: (1/3, 1/3, 1/3)
3 balls: (1/4, 1/4, 1/2)
2 balls: (1/6, 1/6, 2/3)
1 balls: (1/12, 1/12, 5/6)</code></pre>
<p>Which experiment will you pick? Well, the one that has highest entropy. Now, you could sit and calculate entropy using the formula <span class="math display">\[ \sum\nolimits -p\log_2(p) \]</span></p>
<p>Or you could just note that entropy is maximum when all the probabilities are equal. If there are n outcomes, then they will all have probability 1/n. Then, entropy becomes Sum( log2(n^(1/n))) which is just log2(n). So, the maximum entropy for an experiment of n outcomes is log2(n).</p>
<p>From the above experiments, the one with 4 balls on each pan has equal probability for all 3 outcomes, and thus the maximum entropy. We will pick that one.</p>
<h1 id="second-weighing-the-equal-case">Second Weighing: The equal case</h1>
<p>Now, you can get the messages “left-heavier”, “right-heavier”, or “equal” with equal probability.</p>
<p>Let’s take the “equal” case. This means that the odd ball is in the 4 balls not kept on the pan. This eliminates the hypotheses saying that the odd ball was on one of the pans. Now, we have to find which one it is and whether it is lighter or heavier. Again, what are our choices? We can put all these 4 balls from the odd group on one pan and the previous 4 balls from the normal group on the other pan. Or, we could put 3 from the odd group and 3 from the normal group. Or 2 odd and 2 normal. Or 2 odd and 2 odd. Or 1 odd and 1 odd.</p>
<p>The chief question is, of course, what is the entropy of these experiments? We want the one with highest entropy, remember. And that is satisfied when the probabilities of the three messages are as uniform as possible.</p>
<p>There’s no point putting all normal balls on the pans - we know for sure that the message will be “equal”, so there’s zero entropy there.</p>
<p>For the 4 odd and 4 normal trial, with the 4 odd on the left and the 4 normal on the right, we will get left-heavier if the odd ball is heavy, right-heavier if the odd ball is light, but we will never get “equal” because one ball is odd. This should be our first red flag. We want high entropy, where the probabilities are as nearly equal as we can manage (meaning, they are each close to 1/n). And here, one of the probabilities is zero! Anyway, the entropy is <code>-1/2 x log(1/2) + -1/2 x log(1/2)</code> = 1 bit.</p>
<p>Let’s see if the other experiments do any better. For 3 odd and 3 normal, again, the probability of equal is zero. This is not good. Again, since left-heavier and right-heavier have equal probability (1/2), the entropy is 1 bit as earlier. Ditto for 2 odd and 2 normal or 1 odd and 1 normal - 1 bit of entropy.</p>
<p>For the 2 odd and 2 odd trial, there are 4 possible spots for the odd ball, and 2 possible weights (lighter or heavier). So, 8 possibilities. We will get left-heavier if the odd ball is heavier and is on the left, or if it is lighter and is on the right. So, that is 4 out of 8 possibilities. Right-heavier is the same. There is no chance of equal again, since there is an odd ball. So, this experiment too has two outcomes with probabilities 1/2 and 1/2 and thus entropy of 1 bit.</p>
<p>For the 1 odd and 1 odd trial, there are 4 possible spots for the odd ball (on the left, on the right, and outside) and 2 possible weights. So, 8 possibilities. We get left-heavier when the odd ball is on the left and is heavier, or when it is on the right and is lighter - so, 2 possibilities. Similarly for right-heavier - 2 possibilities. The rest give “equal”. So, we get odds of 2:2:4 and thus probabilities of 1/4, 1/4, and 1/2 and thus entropy of <code>-1/4log2(1/4) + -1/4log2(1/4) + -1/2log2(1/2)</code> = 1.5 bits. Nice.</p>
<p>Can we do better? What about 1 odd and 1 normal on the left pan and 2 odd on the right pan? Then, there are 4 possible spots for the odd ball (on the left, on the right, and outside) and therefore 8 possibilities. We get left-heavier when the odd ball is on the left and is heavier (1 possibility), or when it is on the right and is lighter (2 possibilities) - so, 3 total possibilities. Similarly for right-heavier - 3 possibilities. The rest give “equal”. So, we get odds of 3:3:2 and thus probabilities of 3/8, 3/8, and 1/4 and thus entropy of <code>-3/8log2(3/8) + -3/8log2(3/8) + -1/4log2(1/4)</code> = ~1.56 bits. Nice. We pick this one.</p>
<p>We can see how the 1 odd and 1 odd trial and the 1 odd plus 2 odd trial were the only trials that even allowed the “equal” probability and thus got more uniform distributions, leading to higher entropy.</p>
<p>We can now get three possible messages again. If you get left-heavier, then either the left odd ball is heavier or one of the right odd balls is lighter. Now, we can compare the two right balls - if you get “equal”, then the left ball above was odd and heavier; if you get “left-heavier”, then the right ball is odd and lighter; if you get right-heavier, then the left ball is odd and lighter (since we saw above that one of them is lighter). This way, we get to full certainty with just three weighings.</p>
<p>We can do the same thing if we got right-heavier. And if we get equal, then it just means that the one ball left outside is odd. We can just test it with a normal ball to find out it if is lighter or heavier - again making for a total of 3 weighings.</p>
<p>Long story short, you can do the same for the other messages in the original weighing. TODO: If you got left-heavier for the 4-4 weighing, then you can choose …</p>
<h1 id="notes">Notes</h1>
<p>You want to maximize information gained. Expected information gain is nothing but entropy. And that is maximum when all outcomes are equally likely.</p>
<p>Intuitive way to calculate entropy: the flatter the probability distribution, the more the entropy; the spikier the distribution, the less the entropy. You want a spiky distribution. Insert the argument that if you get to a spiky distinction lawfully, you can’t expect to flatten again. You’re (on expectation) golden.</p>
<p>TODO: information = reduction in entropy of the ensemble; also, information in the case of maximum entropy; how to count number of weighings needed (and how you can always conjure an ideal code that gives you maximum entropy on each turn); application to real life.</p>
<p>Note: This is, in my opinion, the main missing piece in reasoning. Just knowing that you have to use Bayesian inference is not enough. It can be hard to know what to do - there are so many options. Entropy guides your search. Moreover, you don’t have to calculate it perfectly - more entropy is better than less. So, I suspect, you can use it as a compass to make your information-seeking choices. (How does it compare to the idea of strong inference, where you always aim to get differing predictions between hypotheses? Which one is better, more practical?)</p>
<h1 id="entropy-to-compare-questions">Entropy to Compare Questions</h1>
<p>In the 2-4-6 task, for example, you don’t need to generate a perfect list of hypotheses (all possible sets of triplets, which is infinite) or calculate entropy formally over all the questions you could possibly ask. Just use entropy to decide between two questions - which one should you ask (4, 6, 8) or (-3, 345, 6)?</p>
<p>And you don’t even need to assign formal probabilities. Just use your (hopefully calibrated) probability estimations. In the former, you expect “yes” with quite high probability. In the latter, you expect “no” with quite high probability. (Hmmm… they seem to have the same uncertainty… is that correct?) Maybe the problem is that you’re not considering any concrete alternative hypotheses and so your estimations are way off.</p>
<p>Maybe just consider two hypotheses at any time. For example, the even plus increment of two rule is one hypothesis. The alternative hypothesis is just any other rule. You believe the former with high confidence. So, to get a high-entropy question, you need…</p>
<p>Wait. That’s the problem. Once you believe something with high confidence, you <em>cannot</em> have a question with high entropy! (Note: unless that belief makes uniform predictions over the outcomes). Bring in the rule about the high confidence hypothesis dragging P(E) towards itself, and therefore not allowing a large change. The whole argument about seeking high entropy rests on the platform that you will reach a spiky confidence distribution in your hypotheses only when you’ve updated on the evidence lawfully. Here, we have unjustifiably come to high confidence in the even-plus-two hypothesis. We cannot <em>expect</em> to lose confidence in it.</p>
<p>If this is correct, then it makes it imperative for us to start with justified confidence in everybody and update lawfully. If you start with a wrong high confidence in some hypothesis, you may have already lost the game. (Can you recover, with enough evidence?)</p>
<p>Instead of locating the even-plus-two hypothesis right off, start with uniform hypotheses and then update on the initial “2-4-6” example as evidence.</p>
<h1 id="one-test-information-reduction-in-uncertainty">One test: Information = reduction in uncertainty</h1>
<p>Look at your uncertainty before and after. Did you <em>really</em> get that much information?</p>
<h1 id="suitcase">Suitcase</h1>
<p>Take the simple case when your hypotheses correspond to that of each possible outcome. For example, suppose your old suitcase has a three-digit number lock. There are a 1000 possible codes - 000 to 999. You can consider each code as a hypothesis - H0 saying that the code is 000, H1 saying that the code is 001, etc.</p>
<p>Now, you’ve forgotten the code. Fortunately, you’d written it somewhere. However, part of the written code has been erased. All you can see is “-3-”. Even though it doesn’t tell you everything, it does give you some information. This eliminates 900 hypotheses - all those that didn’t have “3” as their second digit.</p>
<p>In this case, the information given by the partial code equals reduction in uncertainty. Before, you had 1000 hypotheses, each of which was equally likely, giving you log2(1000) = 9.96 bits of entropy. Afterwards, you had 100 hypotheses, again each of which was equally likely, giving you log2(100) = 6.64 bits of entropy. The difference is ~3.32 bits. How many bits of information did you get from that message? Well, information = log2(1/P(E)). How much did you predict that the middle digit would be 3? 1 in 10 hypotheses said so - therefore, a probability of 1/10 and information of ~3.32. It works out.</p>
<h1 id="general-case">General case</h1>
<p>But this feels like a toy example to me. What about the general case where you have large, hairy hypotheses that talk about several experiments, not just this one? What about the case when you get just one message from the possible messages, instead of getting a partial message like above? Is information still reduction in entropy then?</p>
<p>Well, first, note the terms. The information refers to the information content of the message you receive. We calculate it using log2(1/P(E)). And entropy talks about a probability distribution - remember how the formula for entropy used only the probabilities of the various outcomes, nothing else.</p>
<p>We can use of the term entropy to talk about our probability distribution over the <em>outcomes</em> of an experiment - like the entropy of one ball-weighing in the initial example. As we have seen, this tells us how much the outcome of that experiment will on average multiply our confidence in the correct hypothesis.</p>
<p>We can also use entropy to talk about our probability distribution over our <em>hypotheses</em>. In this scenario, we refer to entropy as “uncertainty”. This tells us how far we think we are from reaching full certainty in the correct hypothesis H0.</p>
<p>TODO: Unsure how to derive the above. Test whether information here really is equal to reduction in uncertainty. Get one example.</p>

<div class="info">Created: January 10, 2016</div>
<div class="info">Last modified: January 28, 2016</div>
<div class="info">Status: in-progress</div>
<div class="info"><b>Tags</b>: reduction, information theory</div>

<br />
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'spkrationalitytrainingground'; // required: replace example with your forum shortname
    var disqus_identifier = '/seek-high-entropy.html';
    var disqus_title = 'Seek High-Entropy Situations';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
