<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<link rel="icon" type="image/png" href="./images/favicon-32x32.png" sizes="32x32" />
	<link rel="icon" type="image/png" href="./images/favicon-16x16.png" sizes="16x16" />
        <title>Seek High-Entropy Situations - SPK's Rationality Essays</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />
        <link rel="stylesheet" type="text/css" href="./css/highlight.css" />

	<!-- <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script> -->
	<!-- <script type="text/javascript" src="/js/header-links.js"></script> -->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

	<link href="atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed" />

	<!-- Google Analytics stuff -->
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-51321929-1', 'pradeep90.github.io');
	  ga('send', 'pageview');

	</script>

    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">Rationality Essays</a>
            </div>
            <div id="navigation">
                <a href="./">Home</a>
                <a href="./notes.html">Notes</a>
                <!-- <a href="/about.html">About</a> -->
                <a href="./archive.html">Archive</a>
		<a href="./atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed">RSS</a>
            </div>
        </div>

        <div id="content">
          <h1 id="post-title">Seek High-Entropy Situations</h1>

            <!-- <center><img src="https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/t31.0-8/p600x600/10257116_10202295769100492_2438594605053717342_o.jpg" height="400" width="300" class="sujeet-pic" alt="Sujeet pic" /></center> -->

<p>(This post is still in progress.)</p>
<h1 id="ball-weighing">Ball-Weighing</h1>
<p>Here’s a puzzle (paraphrased from David JC MacKay’s wonderful textbook on Information Theory):</p>
<blockquote>
<p>You are given 12 balls all weighing the same, except for one which is lighter or heavier (you don’t know which). You are also given a weighing scale that can tell you whether the left pan is heavier or the right pan is heavier or whether they are equal.</p>
<p>What is the least number of weighings to identify the odd ball and whether it is lighter or heavier?</p>
</blockquote>
<p>How would you do this? Take your best shot.</p>
<p>(spoiler space)</p>
<p><br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /></p>
<p>Did you get it? The answer is 3 weighings. Can you prove how? I was stuck on this for a while.</p>
<p>What are our options? On the first weighing, we can choose to weigh 6 balls on each pan or 5 or 4 or 3 or 2 or 1.</p>
<p>We want to find the correct ball (along with its relative weight) as quickly as possible. Now, we can do this in an ad hoc manner, just going with our hunches. But let’s instead try to use a more systematic method. How can we frame this in terms of information theory?</p>
<h1 id="information-is-wealth">Information is Wealth</h1>
<p>Well, we have 24 hypotheses: H1 says that ball 1 is different from the rest and is lighter; H1’ says that ball 1 is different and is heavier; H2 says that ball 2 is different and lighter; H2’ says ball 2 is different and lighter; and so on till H12 and H12’. Which of them is the correct one? We don’t know. So we assign the same probability to all of them: 1/24.</p>
<p>Our job is now to get the correct hypothesis to probability 1. How? By getting evidence and thus <a href="./reducing-information-theory.html">multiplying our confidence</a> in the correct hypothesis. And the fastest way to do that is by seeking high-entropy experiments. We have six possible types of experiments: 6 balls on each side, 5 balls on each side, etc. Right now, it doesn’t matter which balls we choose for the experiment because they are all the same to us.</p>
<p>What is the entropy of these 6 types of experiments? The entropy of an experiment (or ensemble or whatever it is you call something you’re unsure about) depends only on the probabilities you assign to the different outcomes. Nothing else matters. All your knowledge about your hypotheses and your distribution of confidence in them is captured in your final probability distribution over the outcomes.</p>
<p>So, we have to calculate our probability distribution over the 3 possible outcomes of a weighing: left-heavier, equal, and right-heavier.</p>
<p>(I could be off with the calculations here; been a while since I’ve done probability theory counting problems - they’re always tricky (for me).)</p>
<p>If we took 6 balls on each side, the probability of left-heavier is the probability of the odd ball being heavier and being on the left pan or the odd ball being lighter and being on the right side, which is 6/12 = 1/2. Probability of right-heavier is the same 1/2. Probability of equal is 0 (since one of the balls is different).</p>
<p>For 5 balls a side, the probability of left-heavier is 5/12, right-heavier is 5/12, and equal is 2/12 = 1/6. The quick way to calculate this is to just see that there are 5 spots for the odd ball on the left, 5 spots on the right, and 2 spots outside. Also, the ball can be heavier or lighter than the rest. So, there are 24 possibilities. You get left-heavier when the odd ball is on the left and is heavier, or when it is on the right and is lighter. So, there are 10 situations when you get left-heavier. Similarly, for right-heavier. The rest will give you a message of “equal”. Hence, we get odds of 10:10:4 and thus probabilities of 10/24, 10/24, and 4/24.</p>
<p>For 4 balls a side, the odds are 8:8:8 and so the probabilities are 1/3, 1/3, and 1/3.</p>
<p>For 3 balls a side, the odds are 6:6:12 and so the probabilities are 3/12, 3/12, and 6/12.</p>
<p>For 2 balls a side, the odds are 4:4:16 and so the probabilities are 2/12, 2/12, and 8/12.</p>
<p>For 1 balls a side, the odds are 2:2:20 and so the probabilities are 1/12, 1/12, and 10/12.</p>
<p>For quick reference, the various probability distributions are:</p>
<pre><code>6 balls: (1/2, 1/2, 0)
5 balls: (5/12, 5/12, 1/6)
4 balls: (1/3, 1/3, 1/3)
3 balls: (1/4, 1/4, 1/2)
2 balls: (1/6, 1/6, 2/3)
1 balls: (1/12, 1/12, 5/6)</code></pre>
<p>Which experiment will you pick? Well, the one that has highest entropy. Now, you could sit and calculate entropy using the formula <span class="math display">\[ \sum\nolimits -p\log_2(p) \]</span></p>
<p>Or you could just note that entropy is maximum when all the probabilities are equal. If there are n outcomes, then they will all have probability 1/n. Then, entropy becomes Sum( log2(n^(1/n))) which is just log2(n). So, the maximum entropy for an experiment of n outcomes is log2(n).</p>
<p>From the above experiments, the one with 4 balls on each pan has equal probability for all 3 outcomes, and thus the maximum entropy. We will pick that one.</p>
<h1 id="second-weighing-the-equal-case">Second Weighing: The equal case</h1>
<p>Now, you can get the messages “left-heavier”, “right-heavier”, or “equal” with equal probability.</p>
<p>Let’s take the “equal” case. This means that the odd ball is in the 4 balls not kept on the pan. This eliminates the hypotheses saying that the odd ball was on one of the pans. Now, we have to find which one it is and whether it is lighter or heavier. Again, what are our choices? We can put all these 4 balls from the odd group on one pan and the previous 4 balls from the normal group on the other pan. Or, we could put 3 from the odd group and 3 from the normal group. Or 2 odd and 2 normal. Or 2 odd and 2 odd. Or 1 odd and 1 odd.</p>
<p>The chief question is, of course, what is the entropy of these experiments? We want the one with highest entropy, remember. And that is satisfied when the probabilities of the three messages are as uniform as possible.</p>
<p>There’s no point putting all normal balls on the pans - we know for sure that the message will be “equal”, so there’s zero entropy there.</p>
<p>For the 4 odd and 4 normal trial, with the 4 odd on the left and the 4 normal on the right, we will get left-heavier if the odd ball is heavy, right-heavier if the odd ball is light, but we will never get “equal” because one ball is odd. This should be our first red flag. We want high entropy, where the probabilities are as nearly equal as we can manage (meaning, they are each close to 1/n). And here, one of the probabilities is zero! Anyway, the entropy is <code>-1/2 x log(1/2) + -1/2 x log(1/2)</code> = 1 bit.</p>
<p>Let’s see if the other experiments do any better. For 3 odd and 3 normal, again, the probability of equal is zero. This is not good. Again, since left-heavier and right-heavier have equal probability (1/2), the entropy is 1 bit as earlier. Ditto for 2 odd and 2 normal or 1 odd and 1 normal - 1 bit of entropy.</p>
<p>For the 2 odd and 2 odd trial, there are 4 possible spots for the odd ball, and 2 possible weights (lighter or heavier). So, 8 possibilities. We will get left-heavier if the odd ball is heavier and is on the left, or if it is lighter and is on the right. So, that is 4 out of 8 possibilities. Right-heavier is the same. There is no chance of equal again, since there is an odd ball. So, this experiment too has two outcomes with probabilities 1/2 and 1/2 and thus entropy of 1 bit.</p>
<p>For the 1 odd and 1 odd trial, there are 4 possible spots for the odd ball (on the left, on the right, and outside) and 2 possible weights. So, 8 possibilities. We get left-heavier when the odd ball is on the left and is heavier, or when it is on the right and is lighter - so, 2 possibilities. Similarly for right-heavier - 2 possibilities. The rest give “equal”. So, we get odds of 2:2:4 and thus probabilities of 1/4, 1/4, and 1/2 and thus entropy of <code>-1/4log2(1/4) + -1/4log2(1/4) + -1/2log2(1/2)</code> = 1.5 bits. Nice.</p>
<p>Can we do better? What about 1 odd and 1 normal on the left pan and 2 odd on the right pan? Then, there are 4 possible spots for the odd ball (on the left, on the right, and outside) and therefore 8 possibilities. We get left-heavier when the odd ball is on the left and is heavier (1 possibility), or when it is on the right and is lighter (2 possibilities) - so, 3 total possibilities. Similarly for right-heavier - 3 possibilities. The rest give “equal”. So, we get odds of 3:3:2 and thus probabilities of 3/8, 3/8, and 1/4 and thus entropy of <code>-3/8log2(3/8) + -3/8log2(3/8) + -1/4log2(1/4)</code> = ~1.56 bits. Nice. We pick this one.</p>
<p>We can see how the 1 odd and 1 odd trial and the 1 odd plus 2 odd trial were the only trials that even allowed the “equal” probability and thus got more uniform distributions, leading to higher entropy.</p>
<p>We can now get three possible messages again. If you get left-heavier, then either the left odd ball is heavier or one of the right odd balls is lighter. Now, we can compare the two right balls - if you get “equal”, then the left ball above was odd and heavier; if you get “left-heavier”, then the right ball is odd and lighter; if you get right-heavier, then the left ball is odd and lighter (since we saw above that one of them is lighter). This way, we get to full certainty with just three weighings.</p>
<p>We can do the same thing if we got right-heavier. And if we get equal, then it just means that the one ball left outside is odd. We can just test it with a normal ball to find out it if is lighter or heavier - again making for a total of 3 weighings.</p>
<p>Long story short, you can do the same for the other messages in the original weighing. TODO: If you got left-heavier for the 4-4 weighing, then you can choose …</p>
<h1 id="notes">Notes</h1>
<p>You want to maximize information gained. Expected information gain is nothing but entropy. And that is maximum when all outcomes are equally likely.</p>
<p>Intuitive way to calculate entropy: the flatter the probability distribution, the more the entropy; the spikier the distribution, the less the entropy. You want a spiky distribution. Insert the argument that if you get to a spiky distinction lawfully, you can’t expect to flatten again. You’re (on expectation) golden.</p>
<p>TODO: information = reduction in entropy of the ensemble; also, information in the case of maximum entropy; how to count number of weighings needed (and how you can always conjure an ideal code that gives you maximum entropy on each turn); application to real life.</p>
<p>Note: This is, in my opinion, the main missing piece in reasoning. Just knowing that you have to use Bayesian inference is not enough. It can be hard to know what to do - there are so many options. Entropy guides your search. Moreover, you don’t have to calculate it perfectly - more entropy is better than less. So, I suspect, you can use it as a compass to make your information-seeking choices. (How does it compare to the idea of strong inference, where you always aim to get differing predictions between hypotheses? Which one is better, more practical?)</p>
<h1 id="entropy-to-compare-questions">Entropy to Compare Questions</h1>
<p>In the 2-4-6 task, for example, you don’t need to generate a perfect list of hypotheses (all possible sets of triplets, which is infinite) or calculate entropy formally over all the questions you could possibly ask. Just use entropy to decide between two questions - which one should you ask (4, 6, 8) or (-3, 345, 6)?</p>
<p>And you don’t even need to assign formal probabilities. Just use your (hopefully calibrated) probability estimations. In the former, you expect “yes” with quite high probability. In the latter, you expect “no” with quite high probability. (Hmmm… they seem to have the same uncertainty… is that correct?) Maybe the problem is that you’re not considering any concrete alternative hypotheses and so your estimations are way off.</p>
<p>Maybe just consider two hypotheses at any time. For example, the even plus increment of two rule is one hypothesis. The alternative hypothesis is just any other rule. You believe the former with high confidence. So, to get a high-entropy question, you need…</p>
<p>Wait. That’s the problem. Once you believe something with high confidence, you <em>cannot</em> have a question with high entropy! (Note: unless that belief makes uniform predictions over the outcomes). Bring in the rule about the high confidence hypothesis dragging P(E) towards itself, and therefore not allowing a large change. The whole argument about seeking high entropy rests on the platform that you will reach a spiky confidence distribution in your hypotheses only when you’ve updated on the evidence lawfully. Here, we have unjustifiably come to high confidence in the even-plus-two hypothesis. We cannot <em>expect</em> to lose confidence in it.</p>
<p>If this is correct, then it makes it imperative for us to start with justified confidence in everybody and update lawfully. If you start with a wrong high confidence in some hypothesis, you may have already lost the game. (Can you recover, with enough evidence?)</p>
<p>Instead of locating the even-plus-two hypothesis right off, start with uniform hypotheses and then update on the initial “2-4-6” example as evidence.</p>
<h1 id="one-test-information-reduction-in-uncertainty">One test: Information = reduction in uncertainty</h1>
<p>Look at your uncertainty before and after. Did you <em>really</em> get that much information?</p>
<h1 id="suitcase">Suitcase</h1>
<p>Take the simple case when your hypotheses correspond to that of each possible outcome. For example, suppose your old suitcase has a three-digit number lock. There are a 1000 possible codes - 000 to 999. You can consider each code as a hypothesis - H0 saying that the code is 000, H1 saying that the code is 001, etc.</p>
<p>Now, you’ve forgotten the code. Fortunately, you’d written it somewhere. However, part of the written code has been erased. All you can see is “-3-”. Even though it doesn’t tell you everything, it does give you some information. This eliminates 900 hypotheses - all those that didn’t have “3” as their second digit.</p>
<p>In this case, the information given by the partial code equals reduction in uncertainty. Before, you had 1000 hypotheses, each of which was equally likely, giving you log2(1000) = 9.96 bits of entropy. Afterwards, you had 100 hypotheses, again each of which was equally likely, giving you log2(100) = 6.64 bits of entropy. The difference is ~3.32 bits. How many bits of information did you get from that message? Well, information = log2(1/P(E)). How much did you predict that the middle digit would be 3? 1 in 10 hypotheses said so - therefore, a probability of 1/10 and information of ~3.32. It works out.</p>
<h1 id="general-case">General case</h1>
<p>But this feels like a toy example to me. What about the general case where you have large, hairy hypotheses that talk about several experiments, not just this one? What about the case when you get just one message from the possible messages, instead of getting a partial message like above? Is information still reduction in entropy then?</p>
<p>Well, first, note the terms. The information refers to the information content of the message you receive. We calculate it using log2(1/P(E)). And entropy talks about a probability distribution - remember how the formula for entropy used only the probabilities of the various outcomes, nothing else.</p>
<p>We can use of the term entropy to talk about our probability distribution over the <em>outcomes</em> of an experiment - like the entropy of one ball-weighing in the initial example. As we have seen, this tells us how much the outcome of that experiment will on average multiply our confidence in the correct hypothesis.</p>
<p>We can also use entropy to talk about our probability distribution over our <em>hypotheses</em>. In this scenario, we refer to entropy as “uncertainty”. This tells us how far we think we are from reaching full certainty in the correct hypothesis H0.</p>
<p>TODO: Unsure how to derive the above. Test whether information here really is equal to reduction in uncertainty. Get one example.</p>
<h1 id="summary">Summary</h1>
<p>In general, you want to get the most information. When choosing, pick the experiment with higher entropy. Like the ball-weighing example. Entropy is higher when the distribution is more uniform (how do we judge that informally?), and maximum when the distribution is completely uniform.</p>
<p>Similarly, in the 2-4-6 task, look at the question that has most entropy. You may be able to assign probabilities using your intuition. However, you could have a problem if you haven’t lawfully come to those probabilities. I think this only happens when you’re not calibrated. Then, is it that the highest-entropy questions aren’t really the highest-entropy ones? How can you fix that (at least practically)?</p>
<p>Where do we make a poor choice of experiments? Where are we inefficient with our information-seeking? How much more efficient can we get? We are inefficient whenever we seek lower-entropy questions. Where can we change our behaviour now that we know about entropy? Where do you need to make a choice between experiments? Talking to people, running experiments, etc. Also, when learning - we need to choose the right books, instead of reading things that are not very surprising. Entropy is nothing but expected surprise, so you should be quite surprised on average when you read books. If not, you’re not being efficient. Ditto for solving problems - you should ask questions that get to the heart of the problem. (TODO: examples for everything.)</p>
<p>How should we apply this to our lives? I suspect we should practice the skill of noticing the entropy of any given question or the uncertainty of our hypotheses or</p>
<p>What about when you’re presenting an experiment? Should you present the highest-entropy experiments (according to them)? (Remember, you can’t ask for information. You can only ask for expected information aka entropy.)</p>
<p>How many questions of a certain entropy do we need to come to full certainty about an ensemble? I think this is just entropy of the ensemble divided by the entropy of the question. Here, information does seem to be like reduction in uncertainty. And questions with higher entropy are better because you will need fewer of them to get to certainty.</p>
<p>TODO: How many questions of a given entropy do we need to come to full certainty about our hypotheses? If you say you have “x bits of uncertainty” about your hypotheses, that means it will take x questions of 1 bit entropy to get to the answer or x/2 questions of 2 bits of entropy (really?). (Remember, in the best case, you just need one compound message of probability 1/P(H0) to get to certainty, and so one compound experiment of entropy x.)</p>
<p>Is it really that x bits of uncertainty means x questions of 1 bit entropy?</p>
<p>TODO: A message of information content x bits is simply an outcome to which you assign 1/(2^x) probability. A question of entropy x bits is one that will give you (2<sup>x)</sup>n confidence-multiplier for n questions. A distribution of uncertainty x bits is simply one that requires x questions of 1 bit entropy. (example?)</p>
<p>What you need to test is whether information = reduction in uncertainty about your hypotheses. Also, whether expected information aka entropy of a question = expected reduction in uncertainty about your hypotheses.</p>
<p>Look at the example where you have two hypotheses H1 with probability 0.25 and H2 with probability 0.75. You have an experiment with two outcomes heads or tails. H1 predicts heads with probability 1 and H2 predicts heads with probability 0.33 and tails with probability 0.67. So, your total probability for heads is 0.25 x 1 + 0.75 x 0.33 = ~0.5. Say, heads comes up. Now your confidence in H1 is multiplied by 1/0.5 = 2 making it have a probability of 2 x 0.25 = 0.5. So, your posterior probabilities over your hypotheses is 0.5 in H1 and 0.5 in H2.</p>
<p>Your initial uncertainty about your hypotheses is entropy(0.25, 0.75) = 0.81 bits. And your final uncertainty is entropy(0.5, 0.5) = 1 bit. But, the information you got was 1 bit. So, information is not reduction in uncertainty. However, the entropy of the experiment was entropy(0.5, 0.5) = 1 bit. So, reduction in uncertainty is not entropy of the experiment either. What but expected reduction in uncertainty? With probability 0.5, you expected heads, which would lead to a final uncertainty of 1 bit, and with probability 0.5, you expected tails, which would lead to a final uncertainty of 0 bits (since H1 will go down to probability 0 and H2 up to probability 1). That leads to an “expected final uncertainty” of 0.5 bits.</p>
<p>TODO: I suspect my whole calculation above is thrown off by the lack of other hypotheses. The only reason H2 got to posterior 1.0 for tails was because the only other hypothesis H1 put zero probability mass on the correct outcome. If the correct hypothesis H0 were present, it would have predicted tails with probability 1.0 and therefore H2 would not have got to posterior probability 1.0 and you would have a different final uncertainty. Then, the uncertainty we calculated earlier is not our true uncertainty. Perhaps with the true uncertainty, you will get information = reduction in uncertainty. (example? I think for any given sequence of outcomes, your hypothesis set should have a hypothesis that predicts it with 100% accuracy.) In this case, your true uncertainty about your hypotheses becomes hard to compute, since you have a very large number of hypotheses, and if omitting hypotheses leads to big errors, then this uncertainty variable may not be worth much in practice.</p>
<p>And having x+1 bits of uncertainty is worse than having x bits of uncertainty because you need an extra one-bit question. Having more uncertainty is worse because, given questions of any entropy, you will need more questions to get to certainty.</p>
<p>The question I want to answer is: why is a less uncertain hypothesis set better than a more uncertain one? Remember our central aim: to get to full confidence in H0. How does a less uncertain hypothesis set lead you to H0 faster than a more uncertain one? “Asked in such fashion, the question answers itself.” Well, remember that you can never decrease your confidence in H0. It will always make correct predictions and thus never fall in confidence. Look at a series of experiments. H0 will of course make the right prediction each time and thus rise in confidence.</p>
<p>TODO: Remember, you always have a code that will give you maximum entropy (a code with one message per hypothesis). But this doesn’t take into account the prior probabilities of the hypotheses (?).</p>
<p>Can you always construct a code to give you an experiment of any entropy (up to the maximum)? If so, the question becomes: given an experiment of some entropy, how many experiments will you need to get to full certainty? Hmmm… the key idea here is that there is no definite number of experiments. It is probabilistic. The number of experiments itself is not meaningful. The question is actually how many experiments of a given entropy you <strong>expect</strong> to need to get to full certainty.</p>
<p>What do I mean by “expect” here? I think I mean that if you had n hypothesis sets with the same probability distribution, then I would need <code>n x uncertainty</code> bits of information. So, for a less uncertain set of hypotheses, I need fewer bits of information to get to certainty. (shocker, I know. But you still had to prove that our colloquial idea of “uncertainty” matched this mathematical term “uncertainty”.)</p>
<p>I was confused about the part about “expecting fewer bits of information” because I was unable to imagine multiple sets of hypotheses. When it came to the entropy of an experiment, I could easily see it as the expected information content of its outcome because I could think of n experiments with the same probability distribution and thus the total information gained. But when it came to hypotheses, I was thinking of the grand set of hypotheses that an agent would have about the world and there would only be one such set and just one sequence of messages that you get (I thought). How could you do this several times and get the average? There’s only one world and you only get one shot to do things. Note how I completely forgot that the world has several agents and that you could very well have agents with the same probability distribution over their hypotheses and then take the average over their experiences. (“You have a blind spot around strategies that involve <del>doing nice things for</del> other people.”)</p>
<p>(Still not sure, though, about how to define uncertainty in terms of Bayes Theorem the way I did with entropy and n experiments.)</p>
<hr />
<p>What is the relation between your confidence in H0 and your uncertainty about your hypotheses? Obviously, when you’re fully confident in H0, you will have an uncertainty of 0. Also, you can’t lawfully get to zero confidence in H0.</p>
<p>For now, the best I can think of is that H_i is the correct hypothesis with probability p_i. So, with probability p_i, you would need information of 1/p_i bits to get to full confidence in the correct hypothesis. On average, you would need information of “uncertainty” bits to get to full confidence in whatever the correct hypothesis turns to be. (Average over what?)</p>
<p>If you know the entropy of a bunch of books (treating them as one compound message), then can you figure out how many books you need to “understand” a topic fully?</p>
<p>You need to look for questions for whom all answers are equally likely (and the more answers the better).</p>
<h2 id="key-insight">Key insight</h2>
<p>Think in terms of the correct hypothesis H0.</p>

<div class="info">Created: January 10, 2016</div>
<div class="info">Last modified: February  2, 2016</div>
<div class="info">Status: in-progress</div>
<div class="info"><b>Tags</b>: reduction, information theory</div>

<br />
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'spkrationalitytrainingground'; // required: replace example with your forum shortname
    var disqus_identifier = '/seek-high-entropy.html';
    var disqus_title = 'Seek High-Entropy Situations';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
