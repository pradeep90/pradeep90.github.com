<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<link rel="icon" type="image/png" href="./images/favicon-32x32.png" sizes="32x32" />
	<link rel="icon" type="image/png" href="./images/favicon-16x16.png" sizes="16x16" />
        <title>Seek High-Entropy Situations - SPK's Rationality Essays</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />
        <link rel="stylesheet" type="text/css" href="./css/highlight.css" />

	<!-- <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script> -->
	<!-- <script type="text/javascript" src="/js/header-links.js"></script> -->

	<!-- Google Analytics stuff -->
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-51321929-1', 'pradeep90.github.io');
	  ga('send', 'pageview');

	</script>

    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">Rationality Essays</a>
            </div>
            <div id="navigation">
                <a href="./">Home</a>
                <!-- <a href="/sequences.html">Sequences</a> -->
                <a href="./notes.html">Notes</a>
                <!-- <a href="/about.html">About</a> -->
                <a href="./archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
          <h1 id="post-title">Seek High-Entropy Situations</h1>

            <!-- <center><img src="https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/t31.0-8/p600x600/10257116_10202295769100492_2438594605053717342_o.jpg" height="400" width="300" class="sujeet-pic" alt="Sujeet pic" /></center> -->

<h1 id="ball-weighing">Ball-Weighing</h1>
<p>Take this example (adapted from David JC MacKay’s wonderful textbook on Information Theory):</p>
<blockquote>
<p>You are given 12 balls all weighing the same, except for one which is lighter or heavier (you don’t know which). You are also given a weighing scale that can tell you whether the left pan is heavier or the right pan is heavier or they are equal. Find out which one it is in the least number of weighings.</p>
</blockquote>
<p>I was stuck on this for a while.</p>
<p>What are our options? We can choose to weigh 6 balls on each pan or 5 or 4 or something.</p>
<p>We want to find the correct ball (along with its relative weight) as quickly as possible. How can we frame this in terms of information theory?</p>
<p>Well, we have 24 hypotheses: H1 says that ball 1 is different from the rest and is lighter; H1’ says that ball 1 is different and is heavier; H2 says that ball 2 is different and lighter; H2’ says ball 2 is different and lighter; and so on till H12 and H12’. Which of them is the correct one? We don’t know. So we assign the same probability to all of them: 1/24.</p>
<p>Our job is now to get the correct hypothesis to probability 1 by getting evidence and thus multiplying our confidence. And the fastest way to do that is by seeking high-entropy experiments. We have six possible types of experiments: 6 balls on each side, 5 balls on each side, etc. Right now, it doesn’t matter which balls we choose for the experiment because they are all the same to us.</p>
<p>What is the entropy of these 6 types of experiments? For that we have to calculate our probability distribution over the 3 possible outcomes: left-heavier, equal, and right-heavier.</p>
<p>(I could be off with the calculations here; been a while since I’ve done probability theory counting problems.)</p>
<p>TODO: You’re not thinking about the relative weight of the ball. Factor that in.</p>
<p>If we took 6 balls on each side, the probability of left-heavier is the probability of the odd ball being on the left pan, which is 6/12 = 1/2. Probability of right-heavier is the same 1/2. Probability of equal is 0 (since one of the balls is different).</p>
<p>For 5 balls a side, the probability of left-heavier is 5/12, right-heavier is 5/12, and equal is 2/12 = 1/6. The quick way to calculate this is to just see that there are 5 spots for the odd ball on the left, 5 spots on the right, and 2 spots outside. Hence, we get odds of 5:5:2 and thus probabilities of 5/12, 5/12, and 2/12.</p>
<p>For 4 balls a side, the odds are 4:4:4 and so the probabilities are 1/3, 1/3, and 1/3.</p>
<p>For 3 balls a side, the odds are 3:3:6 and so the probabilities are 3/12, 3/12, and 6/12.</p>
<p>For 2 balls a side, the odds are 2:2:8 and so the probabilities are 2/12, 2/12, and 8/12.</p>
<p>For 1 balls a side, the odds are 1:1:10 and so the probabilities are 1/12, 1/12, and 10/12.</p>
<p>Which experiment will you pick? Well, the one that has highest entropy. Now, you could sit and calculate entropy using the formula Sum( log2((1/p_i)^p_i) ).</p>
<p>Or you could just note that entropy is maximum when all the probabilities are equal. If there are n outcomes, then they will all have probability 1/n. Then, entropy becomes Sum( log2(n^(1/n))) which is just log2(n). So, the maximum entropy for an experiment of n outcomes is log2(n).</p>
<p>From the above experiments, the one with 4 balls on each pan has equal probability for all 3 outcomes, and thus the maximum entropy. We will pick that one.</p>
<p>TODO: information = reduction in entropy of the ensemble; also, information in the case of maximum entropy; how to count number of weighings needed; application to real life.</p>
<h1 id="notes">Notes</h1>
<p>You want to maximize information gained. Expected information gain is nothing but entropy. And that is maximum when all outcomes are equally likely.</p>

<div class="info">Created: January 10, 2016</div>
<div class="info">Last modified: January 10, 2016</div>
<div class="info">Status: in-progress</div>
<div class="info"><b>Tags</b>: reduction, information theory</div>

<!-- <div id="sequence-navigation" style="text-align: right"> -->
<!--   <p>Part of <a href="./sequences.html"><i>No Sequence</i></a> -->

<!--   <p>Previous post: "<a href="">Start of Sequence</a>" -->

<!--   <p>Next post: "<a href="">Head of Sequence</a>" -->
<!-- </div> -->

<br />
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'spkrationalitytrainingground'; // required: replace example with your forum shortname
    var disqus_identifier = '/seek-high-entropy.html';
    var disqus_title = 'Seek High-Entropy Situations';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
