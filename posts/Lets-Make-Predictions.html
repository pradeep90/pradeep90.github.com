<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Rationality Essays - Let's Make Predictions</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <link rel="stylesheet" type="text/css" href="../css/highlight.css" />

	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
	<script type="text/javascript" src="../js/hello-world.js"></script>
	<script type="text/javascript" src="../js/header-links.js"></script>

	<!-- Google Analytics stuff -->
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-51321929-1', 'pradeep90.github.io');
	  ga('send', 'pageview');

	</script>

    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">Rationality Essays</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
                <a href="../sequences.html">Sequences</a>
                <!-- <a href="/about.html">About</a> -->
                <a href="../archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
            <h1>Let's Make Predictions</h1>

            <center><img src="https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/t31.0-8/p600x600/10257116_10202295769100492_2438594605053717342_o.jpg" height="400" width="300" class="sujeet-pic" alt="Sujeet pic" /></center>

<p><strong>Status</strong>: In Progress</p>
<hr />
<h1 id="the-great-predictions-experiment">The Great Predictions Experiment</h1>
<p>I need to make more and more predictions.</p>
<h1 id="why">Why?</h1>
<p>Cos your <a href="./The-Prediction-Game.html">Prediction Score</a> determines your success in life <a href="./The-Prediction-Game.html#the-most-important-thing-tm">completely</a>.</p>
<h1 id="how">How?</h1>
<p>How about this? I make at least 10 predictions every day, with probability estimates.</p>
<p>This is just Version 1.0. We’ll do this for a week and take it from there.</p>
<h1 id="wait-a-minute.">Wait a minute.</h1>
<p>Aren’t Measurements supposed to Matter? Shouldn’t there be some Decision to which this Measurement matters? What’s the point of making predictions (a form of measurement) in the air?</p>
<p>Wait. Are Predictions a form of Measurement?</p>
<p>A measurement is the quantitative reduction of uncertainty about something.</p>
<p>A prediction is a probability distribution over the possible outcomes in some event, as given by some hypothesis.</p>
<p>Why do I want to record my predictions and then get feedback on them?</p>
<h1 id="i-want-to-test-my-hypotheses">I want to test my hypotheses</h1>
<p>A button affords pushing. A hypothesis affords testing.</p>
<p>One way or another, I will get to know something new about my hypotheses. I will be able to update them. Bad hypotheses get thrown out. Good ones get upgraded.</p>
<p>By default, we never make predictions. We never test our hypotheses. And so, we rarely ever get direct evidence about our hypotheses. We can rest safely knowing that our cherished (and most probably, false) hypothesis can never be falsified.</p>
<p>Also, costly and hard-to-test predictions are useless to us.</p>
<p>So: Make easily testable predictions.</p>
<h1 id="i-want-to-calibrate-my-estimates">I want to calibrate my estimates</h1>
<p>The previous case was about making predictions based on your hypotheses and, then, updating belief-levels of the hypotheses based on the results.</p>
<p>This is about <em>making</em> predictions in the first place from hypotheses.</p>
<blockquote>
<p>Yo, won’t it be clear what predictions a hypothesis makes?</p>
</blockquote>
<p>It would, ideally. But our head is hardly an ideal place. We keep hypotheses around implicitly. We don’t even know exactly which hypotheses we have, let alone what our probability estimates for them are.</p>
<p>Our overall prediction comes out from the predictions of all our hypotheses weighted by our beliefs in them.</p>
<p>But we neither know the quantitative predictions made by each hypothesis nor our belief-level for the hypothesis.</p>
<p>Then, how are we justified in pulling predictions out of our ass?</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>(Rhetorical)</p>
<h1 id="feeling-quantitatively-and-reasoning-quantitatively">Feeling quantitatively and Reasoning quantitatively</h1>
<p>Ok. So shit’s bad right now. How do we fix things?</p>
<p>We need to calibrate our probability estimates. i.e., we need to know how to translate implicit belief-levels in our minds into actual probabilities.</p>
<p>i.e., intuitive feeling (“I’m pretty sure”) -&gt; a quantitative probability (95%).</p>
<p>We need to know the hypotheses we have, know what quantitative predictions they make, know how much we believe in each hypothesis, and aggregate all that to come up with our actual predictions. That is the ideal reasoning process.</p>
<p>i.e., make our reasoning process more explicit and quantitative.</p>
<p>Alternatively, we could skip all that quantitative reasoning thing and just go with our final gut feeling, the <em>implicit prediction</em> in our head, and translate that into a quantitative prediction.</p>
<h1 id="wrong-translation-vs-wrong-beliefs">Wrong Translation vs Wrong Beliefs</h1>
<p>I was finding it difficult to separate the two.</p>
<p>You could have the wrong belief-levels about some hypotheses - an honest mistaken belief about the world - and thus make a wrong prediction. You might believe Kohli is not playing in the match (i.e., you have low belief-level in the hypothesis that Kohli is playing) and predict that India won’t chase the total down, whereas he <em>is</em> playing and scores a century.</p>
<p>Or, you could have correct belief-levels about your hypotheses but go wrong in translating the belief-levels into probability - and thus make a wrong prediction. e.g., “not too sure whether it is A or B” into 30%-70% when it should be 50%-50%.</p>
<p>Whether it is a wrong translation or wrong belief, in both cases, the <em>predictions</em> you make will be wrong. How will you know where you went wrong? Was your hypothesis wrong or was your translation wrong?</p>
<p>Where do these two hypotheses (Wrong beliefs vs Wrong translation) differ in their predictions?</p>
<h2 id="will-they-lead-to-different-prediction-results">Will they lead to different prediction results?</h2>
<p>Right belief + Right translation: Your explicit predictions will be correct.</p>
<p>Right belief + Wrong translation: Your explicit predictions will be wrong.</p>
<p>Wrong belief + Right translation: Your explicit predictions will be wrong.</p>
<p>Wrong belief + Wrong translation: Your explicit predictions may be right or wrong.</p>
<p>So, all that you can tell from wrong predictions is that you <em>don’t</em> have both the right belief and the right translation (else, the prediction would definitely have been right). It doesn’t tell you whether it was your belief that was wrong or your translation or both.</p>
<p>Note, even if you get the right predictions, it could still mean that your wrong belief and wrong translation have cancelled out luckily. But, that is highly unlikely to happen, right? So, it’s <a href="./Bayes-Theorem.html#strong-and-weak-evidence">strong evidence</a> that you have both the right belief and the right translation.</p>
<h2 id="will-they-lead-to-different-levels-of-surprise">Will they lead to different levels of surprise?</h2>
<p>I’m assuming that your mind makes you feel surprise <em>only</em> when things go against your implicit prediction. Else, you’re not surprised. Everything feels normal.</p>
<p>Even if something is against your prediction and should surprise you, you may still not notice it. That’s fine. Humans are dumb. But, when you <em>do</em> feel surprised, it is because it countered your implicit prediction.</p>
<p><strong>Note</strong>: Your explicit predictions (the translation of your belief-levels to probabilities) have no effect on your surprise levels. The surprise is about the stuff in your brain. The numbers you wrote down have no effect there, in general.</p>
<p>Right belief: You won’t feel surprised at all, cos <em>all</em> your implicit predictions will come true. (say, when you are a technical expert in a specific domain)</p>
<p>Wrong belief: If you feel surprise at all, it must be because of wrong beliefs. Wrong belief means that your implicit prediction was different from the correct prediction.</p>
<p>It is possible to have Wrong beliefs and still not be surprised by anything you see that contradicts your beliefs (see: religious people). But, if you do get surprised, it has to come from Wrong beliefs.</p>
<p>I think that <em>Noticing Surprises</em> is a skill and can be trained. If you force yourself to look actively for surprises, if you consciously bring the outcome to your awareness and test it against your implicit prediction, I think you will know whether you are surprised or not. In most cases, we aren’t even aware that our model of the world has made a prediction and so we let surprises pass under our radar. But since this whole discussion is about <em>explicit</em> predictions, we can always do a conscious mental check and ask if we are surprised.</p>
<p>In <em>this</em> case, surprise becomes proper strong evidence. If you are <em>surprised</em> by the outcome, then your belief was wrong. End of story. If you aren’t <em>surprised</em>, then your belief was right.</p>
<p>(TODO Question: How do judge the level of surprise (especially in cases where the outcomes have a range)? Example: what will the final score be? 250 runs? 270? 300? 280-290 runs? How surprised will you be if you say 280-290 and you get 270? What if you get 324?)</p>
<h2 id="what-can-we-say-about-our-translation-accuracy">What can we say about our translation accuracy?</h2>
<p><strong>Cool</strong>! We have a differing prediction between Right beliefs and Wrong beliefs. And, since it is independent of our translation accuracy (and thus our explicit predictions), we can use this to help solve the mystery of whether a wrong prediction was caused by a Wrong belief or a Wrong translation of belief-levels.</p>
<p>Now, what does this tell us about explicit predictions (and thus our translation accuracy)?</p>
<p>Say we got a wrong explicit prediction. We don’t know if it was cos of a wrong belief or cos of wrong translation.</p>
<p>So, we check our surprise level. If we did get surprised, it means that our belief was wrong. If not, our belief was right.</p>
<p>Now, let’s try out the different possibilities:</p>
<h4 id="right-belief-confirmed">Right belief (confirmed)</h4>
<p>Right Translation: Right explicit prediction</p>
<p>Wrong Translation: Wrong explicit prediction</p>
<h4 id="wrong-belief-confirmed">Wrong belief (confirmed)</h4>
<p>Right Translation: Wrong explicit prediction</p>
<p>Wrong Translation: Most probably Wrong explicit prediction (cos it’s unlikely that things cancel out the right way)</p>
<hr />
<p>When you have a right belief (as confirmed by the surprise test), the correctness of the explicit prediction tells you <strong>perfectly</strong> whether your translation of belief-levels was right or wrong. Correct explicit prediction means correct translation. Wrong explicit prediction means wrong translation.</p>
<p>When you have a wrong belief (as confirmed by the surprise test), the explicit prediction result tells you <strong>pretty much nothing</strong>. In fact, in the unlikely case that you are surprised but your explicit prediction was <em>correct</em>, it means your <em>translation</em> was wrong. Weird, huh!</p>
<h1 id="calibration">Calibration</h1>
<p>I think they call “translation accuracy” as your level of calibration. Calibration is about correctly translating belief-levels into quantitative estimates. This includes correcting common errors that humans make in this process. The more calibrated you are, the better you will be at translating your belief-levels into quantitative estimates (explicit predictions).</p>
<p>If my above theory is true (about the surprise thing and the connection between implicit predictions and translation accuracy), then we now know some things about how to calibrate.</p>
<p>Most importantly, there is no point trying to calibrate using explicit predictions when you have Wrong beliefs about the thing. If the explicit prediction is wrong, then you can’t say anything about your translation, because your belief itself is wrong in the first place. If the explicit prediction is right, then, it means your translation was wrong - but this case will be pretty rare, IMHO.</p>
<p>So, you need to calibrate yourself (aka improve your translation accuracy) by making predictions only about things where you have <em>correct</em> beliefs.</p>
<p>And how would you know that? Well, you won’t be surprised by stuff that happens in that domain, even when you carefully take every event and consciously check whether you’re surprised or not. That is the only way to see if your implicit predictions are correct or not.</p>
<p>I’m not sure how this would work. Are there even <em>any</em> fields where I have correct beliefs?</p>
<p>Well, you don’t need perfectly correct beliefs. If you have reasonably correct beliefs, you will make mostly correct implicit predictions. So, you will have few surprises when you check consciously. That’s good enough. We can then go ahead and make explicit predictions and correct our translation processes.</p>
<p>Need to test out these hypotheses. Will do it soon.</p>
<hr />
<p><strong>Tentative solution</strong>: Make predictions about a bunch of events (could be trivia, college studies, whatever). Now, pick out those for which you were <em>not</em> surprised by the answer.</p>
<p>Those are predictions for which your implicit belief was right (or, at least, not wrong). Any error would be due to your translation. Fix it! Calibrate yourself.</p>
<h1 id="fixing-your-prediction-process">Fixing your Prediction Process</h1>
<p>We saw how we could calibrate our translation processes and get quantitative predictions that are close to our beliefs. Our explicit predictions will be wrong just as often as we are surprised. They will be closer and closer to our implicit predictions.</p>
<p>So far, it’s all about translating the final implicit prediction that our mind gives us into an explicit prediction.</p>
<p>Cool. But what if our implicit predictions are wrong in the first place?</p>
<p>How do we correct the prediction process by which our mind comes up with its implicit prediction?</p>
<p>The default process our mind uses is evidently not even close to the ideal way to do it.</p>
<p>How should we come to a prediction (ideally)?</p>
<p>Assuming you have a bunch of hypotheses in your head, and a bunch of belief-levels about those hypotheses, and a bunch of predictions that each hypothesis makes, what you need to do is take the average of all those predictions weighted by the belief-levels of the hypotheses. <em>That</em> is the final prediction, using the full force of your knowledge.</p>
<p>Which means, you need to be able to enumerate your hypotheses correctly. You need to translate their belief-levels accurately into probabilities. You need to enumerate the outcomes of an event and translate each hypothesis’ likelihood for each outcome into a probability. And, then, you need to calculate the weighted-average.</p>
<p>(Of course, all this is assuming that your hypotheses’ belief-levels and predictions are correct in the first place. God help you if you haven’t been updating properly on evidence, as per Bayes Theorem.)</p>
<p>So, these are the skills you need to improve to fix your prediction process. Converting belief-levels into probabilities calls again upon our old friend, calibration.</p>
<p>There’s lots more work to be done here. Will do it soon.</p>
<hr />
<p><strong>Note</strong>: Correct Beliefs doesn’t mean that you know what’s gonna happen with 100% certainty. It just means that when you say (implicitly) “X is gonna happen with 30% probability”, then the predicted outcome will happen around 30 times out of 100 such events.</p>
<h1 id="resources-for-making-predictions">Resources for Making Predictions</h1>
<h2 id="gwerns-advice-on-making-predictions">Gwern’s advice on Making Predictions</h2>
<p>He <a href="http://www.gwern.net/Prediction%20markets#how-i-make-predictions">talks</a> about 3 parts of making predictions: specifying the prediction, deciding the due-date, and assigning a probability to the prediction.</p>
<p>The prediction should be “a statement on an objective and easily checkable fact”, so that even an arch-enemy trying to poke holes would <em>have</em> to agree when you claim that your prediction was right (or wrong).</p>
<p>He goes into great detail on how to make calibrated probability estimates. Will delve into those after some time. I just want to get started now.</p>
<h2 id="douglas-hubbard---how-to-measure-anything">Douglas Hubbard - How to Measure Anything</h2>
<p>He gives lots of brilliant techniques for making calibrated predictions. Will go into all of them soon.</p>
<h2 id="calibration-1">Calibration</h2>
<ul>
<li><a href="http://lesswrong.com/r/lesswrong/lw/1f8/test_your_calibration/">Test your Calibration!</a></li>
</ul>
<hr />
<h1 id="ps-ideas">PS: Ideas</h1>
<ul>
<li>Wrong beliefs vs Wrong translation</li>
</ul>

<div class="info">Posted on November  3, 2014</div>
<div class="info"><b>Tags</b>: prediction</div>

<div style="text-align: right">
  <p>Part of <a href="../sequences.html"><i>The Truth Sequence</i></a>

  <p>Previous post: &quot;<a href="Bayes-Theorem.html">SPK's Intuitive Explanation of Bayes Theorem</a>&quot;

  <p>Next post: &quot;<a href>Head of Sequence</a>&quot;
</div>

<br />
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'spkrationalitytrainingground'; // required: replace example with your forum shortname
    var disqus_identifier = '/posts/Lets-Make-Predictions.html';
    var disqus_title = 'Let\'s Make Predictions';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
        <div id="footer">
	  All artwork by <a href="http://blog.sujeet.me/">Sujeet Gholap</a>.
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
