<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<link rel="icon" type="image/png" href="./images/favicon-32x32.png" sizes="32x32" />
	<link rel="icon" type="image/png" href="./images/favicon-16x16.png" sizes="16x16" />
        <title>Bits, Multiple Messages, and Complete Faith - SPK's Rationality Essays</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />
        <link rel="stylesheet" type="text/css" href="./css/highlight.css" />

	<!-- <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script> -->
	<!-- <script type="text/javascript" src="/js/header-links.js"></script> -->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

	<link href="atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed" />

	<!-- Google Analytics stuff -->
	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-DEWF2J5BG8"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-DEWF2J5BG8');
	</script>

	<script type="text/javascript" src="https://fast.fonts.net/jsapi/f7f47a40-b25b-44ee-9f9c-cfdfc8bb2741.js"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">SPK's Rationality Essays</a>
            </div>
            <div id="navigation">
                <a href="./">Home</a>
                <a href="./notes.html">Notes</a>
                <!-- <a href="/about.html">About</a> -->
                <a href="./archive.html">Archive</a>
		<a href="./atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed">RSS</a>
            </div>
        </div>

        <div id="content">
          <h1 id="post-title">Bits, Multiple Messages, and Complete Faith</h1>

            <!-- <center><img src="https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/t31.0-8/p600x600/10257116_10202295769100492_2438594605053717342_o.jpg" height="400" width="300" class="sujeet-pic" alt="Sujeet pic" /></center> -->

<h1 id="information-in-multiple-messages">Information in Multiple Messages</h1>
<p>What if there are multiple messages? How do we calculate their information content?</p>
<p>Let’s say you get n messages. The problem here is that your probability distribution will change as your confidence in your hypotheses changes. After the first message, you will update the posterior probabilities of all the hypotheses using Bayes Theorem. Then, you will <em>those</em> probabilities to predict the probability of the next message, and then you’ll update again, and so on. And since we calculate the information content of a message using the probability we assigned it, then it seems that we have to update on the first n-1 messages before we can assign a probability to the nth message and find its information content. Is there a quicker way?</p>
<p>Well, all we’re interested in is the amount by which our confidence in <span class="math inline">\(H_0\)</span> is multiplied. That will look like this:</p>
<pre><code>P(H_0|E1,E2,...,En) = P(H_0) x 1/P(E1) x 1/P(E2|E1) x ... x 1/P(En|E1,E2,...,En-1)

where P(E2|E1) means the probability of E2 after we have updated on E1, and so on.</code></pre>
<p>That looks pretty complicated. However, we can collapse the product on the right side:</p>
<pre><code>P(H_0|E1,E2,...,En) = P(H_0) x 1/P(E1,E2,...,En)</code></pre>
<p>Basically, we just consider the sequence of messages E1, E2, etc. as one giant message E.</p>
<pre><code>P(H_0|E) = P(H_0) x 1/P(E)</code></pre>
<p>More importantly, we can judge the probability of the total message E using our initial probability distribution! We don’t have to update on every message piecemeal. Instead of doing the tedious and complicated calculation for each message, we can just figure out the final probability in one shot and update our confidence in <span class="math inline">\(H_0\)</span>.</p>
<h1 id="why-can-you-add-bits">Why can you add bits?</h1>
<p>This used to be the most mystifying part of information theory. Not only did bits seem very different from probabilities, you could <em>add</em> bits and you could add them in any order. They had no identity saying that this bit came from this probability distribution, and those three bits came that other one. They were just bits. They were like dollars, you could add or subtract them without specifying where they came from - this dollar came from my salary, that one came from selling my bike. It was independent of the source.</p>
<p>How can we explain bits in terms of probability theory?</p>
<p>Well, remember our giant message E made out of smaller messages E1, E2, etc.? Well, that needn’t be the only way to break it up. You could have any sequence of messages as long as their product was the same P(E) and the effect would still be the same. It’s the same multiplication of confidence whether it came from one giant message, or ten smaller messages, or even a thousand.</p>
<pre><code>P(E) = P(E1,E2,...,En) = P(E1) x P(E2|E1) x ... x P(En|E1,E2,...,En-1)</code></pre>
<p>Take the logarithm everywhere.</p>
<pre><code>log2(P(E)) = log2(P(E1,E2,...,En)) = log2(P(E1)) + log2(P(E2|E1)) + ... + log2(P(En|E1,E2,...,En-1))</code></pre>
<p>So,</p>
<pre><code>inf(E) = inf(E1,E2,...,En) = inf(E1) + inf(E2|E1) + ... + inf(En|E1,E2,...,En-1)

where inf(E) is the information content of E in bits.</code></pre>
<p>This means that you can either calculate the information content <code>inf(E)</code> of the giant message as a whole, using your initial confidence levels in your hypotheses. Or you can update on each message one at a time, and then calculate the information content of the next message <code>inf(E2|E1)</code> using the current confidence levels. It works out the same either way.</p>
<p>You can add bits because you can multiply probabilities. There’s nothing special about bits. In fact, this is probably why we denoted information as the logarithm of 1/P(E) - so that we can add it easily and don’t have to deal with very small fractions.</p>
<h1 id="notes">Notes</h1>
<p><strong>Lesson</strong>: When you come across important terms, rename them with nonsense words so that you don’t confuse them with existing words. Like how we confuse the information from information theory with the information from common sense. This way you can truly reduce it to its core.</p>
<p>Note: There exists one correct hypothesis that predicts all the past evidence with perfect accuracy and will predict all future evidence too. Of course, you may have several such candidates and would need more evidence to eliminate the rest. However, if you don’t have <em>any</em> hypothesis at all that has a perfect score so far - assigning a likelihood of 1 to the correct outcome each time - then that means your hypothesis set is lacking.</p>
<hr />
<p>Even the notion that the source can only give you n kinds of messages is information. You can’t just start with that assumption because that means you’re sneaking in information without declaring it. You have to get that indirectly by asking your hypotheses what they predict and ignoring messages that get zero (or extremely low) probability.</p>
<h1 id="complete-faith">Complete Faith</h1>
<p>Ok. We update on all the evidence and put all our probability mass in one hypothesis. But what if it’s wrong?! Why should we trust that all this Bayesian hocus-pocus will get us to the right hypothesis? It’s like a man stating with 100% confidence that the sun won’t rise tomorrow; he’s confident, all right, but he’s also almost dead wrong.</p>
<p>Our situation is different. Once you lawfully update on evidence and get to 100% confidence in one hypothesis, you can never expect that situation to change. None of the other hypotheses can get back into the game because P(H) is 0 and thus P(H|E) will be 0 no matter what the evidence and what the likelihood assigned by this hypothesis. Similarly, <span class="math inline">\(H_0\)</span> cannot lose because P(E) will become equal to P(E|<span class="math inline">\(H_0\)</span>) - <span class="math inline">\(H_0\)</span> is the only one whose vote counts - and thus it will remain at full confidence P(<span class="math inline">\(H_0\)</span>) = 1 forever. (Note that, for this very reason, in practice, you will get arbitrarily close to 100% but never actually at 100%. You don’t have enough evidence to rule out the other hypotheses that were similar so far. Something could always go wrong.)</p>
<p>In short, your probabilities are a reflection of what you expect to happen. When you lawfully update as per the laws of probability, you can’t expect something else to happen. If you say you have 99% confidence in a hypothesis, that means that 99 times out of 100 when you say this you expect to be right and 1 time out of 100 you expect to be wrong. There’s no way to cheat. The only way a hypothesis can win high confidence is by making correct predictions when eventually all else were wrong. And if and when you get a hypothesis like that, you have no reason to expect anything unexpected to happen. It would indeed be a time for celebration.</p>
<h1 id="other-thoughts---todo">Other thoughts - TODO</h1>
<p>Fractional bits: when does the information-as-length-of-message analogy get really stretched? When 1/P(E) is not a power of two.</p>
<p>My reduction of entropy to probability is not as clean as the one of information. You need to posit a large number of messages. I think that’s true even with the message-length definition of entropy - it only works out in the ideal case, with a large number of messages. No, actually, it is pretty simple there. It’s just the expected information content of messages. However, that leaves the question: why do you want to maximize this “entropy” thing? Because you want to maximize “information”? Is that satisfactory? TODO</p>
<p>Information vs evidence.</p>
<p>What does one bit feel like? What about 10 bits?</p>
<p>Even the notion that the source can only give you n kinds of messages is information. You can’t just start with that assumption because that means you’re sneaking in information without declaring it. You have to get that indirectly by asking your hypotheses what they predict and ignoring messages that get zero (or extremely low) probability.</p>

<div class="info">Created: January 20, 2016</div>
<div class="info">Last modified: February  4, 2016</div>
<div class="info">Status: in-progress</div>
<div class="info"><b>Tags</b>: reduction, information theory</div>

<br />
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'spkrationalitytrainingground'; // required: replace example with your forum shortname
    var disqus_identifier = '/bits-multiple-and-complete-faith.html';
    var disqus_title = 'Bits, Multiple Messages, and Complete Faith';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<script type="text/javascript" src="https://fast.fonts.net/jsapi/f7f47a40-b25b-44ee-9f9c-cfdfc8bb2741.js"></script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
