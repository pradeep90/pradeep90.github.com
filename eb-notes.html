<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<link rel="icon" type="image/png" href="./images/favicon-32x32.png" sizes="32x32" />
	<link rel="icon" type="image/png" href="./images/favicon-16x16.png" sizes="16x16" />
        <title>EB Notes - SPK's Rationality Essays</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />
        <link rel="stylesheet" type="text/css" href="./css/highlight.css" />

	<!-- <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script> -->
	<!-- <script type="text/javascript" src="/js/header-links.js"></script> -->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

	<link href="atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed" />

	<!-- Google Analytics stuff -->
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-51321929-1', 'pradeep90.github.io');
	  ga('send', 'pageview');

	</script>

    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">Rationality Essays</a>
            </div>
            <div id="navigation">
                <a href="./">Home</a>
                <a href="./notes.html">Notes</a>
                <!-- <a href="/about.html">About</a> -->
                <a href="./archive.html">Archive</a>
		<a href="./atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed">RSS</a>
            </div>
        </div>

        <div id="content">
          <h1 id="post-title">EB Notes</h1>

            <!-- <center><img src="https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/t31.0-8/p600x600/10257116_10202295769100492_2438594605053717342_o.jpg" height="400" width="300" class="sujeet-pic" alt="Sujeet pic" /></center> -->

<h1 id="automate-scientific-discovery">Automate Scientific Discovery</h1>
<p>“From Causal Inference and Data Fusion to an Automated Scientist”, Elias Bareinboim, 1541-1672/16/$33.00 (c) 2016 IEEE. IEEE INTELLIGENT SYSTEMS. Published by the IEEE Computer Society.</p>
<p>Graphical models are the bee’s knees:</p>
<blockquote>
<p>The advent of graphical methods of causal and counterfactual inference has made it possible to tackle some of the most challenging problems in scientific methodology.</p>
</blockquote>
<p>To take advantage of big data, we need to use two more things on top of graphical models:</p>
<blockquote>
<p>the ability to distinguish causal from associational relationships, and the ability to integrate data from multiple, heterogeneous sources.</p>
</blockquote>
<p>Apparently, this is called the “data fusion” problem. Once we use his theoretical framework, we can let AIs generalize their causal models like human scientists.</p>
<p>Note how the overall theme echoes that of the 1987 book “Scientific Discovery” by the AI researchers Langley, Simon, et al. We all want to automate scientific discovery.</p>
<h1 id="causal-inference-and-the-data-fusion-problem">Causal Inference and the Data-Fusion Problem</h1>
<p>Hypothesis: Confounding bias -&gt; P(y|x) != P(y|do(x)), because something could cause both x and y (including the cases where one is the ancestor of the other).</p>
<p>Admissible sets: Z is not a descendant of X means that you can’t accidentally condition on a collider and get a spurious correlation. Z blocks all backdoor paths from X to Y means that it will prevent Y from having an effect as an ancestor of X or from something confounder causing both X and Y. That way, you block the other two causes of correlations. The only correlation left between X and Y is causative correlation. If I’m right, you’ve converted a question of intervention into a question of correlation simply by averting the other causes of correlation.</p>
<p>Corollary: Admissible set -&gt; remove confounding bias relative to the effect of X on Y.</p>
<p>Front-door criterion:</p>
<pre><code>   u
  / \
 /   \
x-&gt;z-&gt;y</code></pre>
<p>Note that x d-separates u and z because y is a collider, and u and z d-separate x and y. So, P(u|z,x) = P(u|x) and P(y|x,z,u) = P(y|z,u). Substitute them into the formula for P(y|do(x))</p>
<p>Ah! Front-door criterion = back-door criterion applied twice!</p>
<p>For the causal effect of x on z, the collider y blocks all back-door paths from x to z and makes the admissible set {}. So, P(z|do(x)) = P(z|x).</p>
<p>For the causal effect of z on y, x blocks all back-door paths from z to y. So, P(y|do(z)) = Sum_x’ P(y|x’,z)P(x’)</p>
<p>So, overall effect of x on y is <code>Sum_z (P(z|x) (Sum_x' P(y|x',z)P(x')))</code>.</p>
<h1 id="back-door-paths">Back-Door Paths</h1>
<p>Hypothesis: Back-door path =&gt; confounder. No back-door path =&gt; no confounder.</p>
<h1 id="papers-i-need-to-understand">Papers I need to Understand</h1>
<p>Pearl conference presentation on Data Fusion problem</p>
<p>Causal inference and the data-fusion problem</p>
<p>Causality the book</p>
<p>Recovering from Selection Bias in Causal and Statistical Inference</p>
<p>Recovering Causal Effects From Selection Bias (?)</p>
<h1 id="my-core-ideas">My Core Ideas</h1>
<p>Locality of causality</p>
<p>Expected value of information</p>
<p>Entropy</p>
<h1 id="eb-cs590-past-project-ideas-and-papers">EB CS590 Past Project Ideas and Papers</h1>
<h2 id="my-own-research-interests">My Own Research Interests</h2>
<p>Scientific discovery.</p>
<p>Programming.</p>
<p>How humans learn. The causal models that are inherent in the textbooks we study and in the categories we have of the world around us.</p>
<p><strong>Hypothesis</strong>: I’m interested in how to scale models from the micro-level to the macro-level. How do we form higher-level concepts?</p>
<p>For example, climate science (as in the thesis). Or going from our understanding of individual functions in a program to the overall properties of the program. Ditto for a car.</p>
<p>Hypothesis: I believe that we shouldn’t limit ourselves to the variables given in the data (as you keep mentioning, there could be millions of such low-level variables). How do we get to the high-level variables we care about? How would an automated program identify high-level “variables”?</p>
<p>Feature extraction: Instead of just feature selection, I want category creation. Build high level abstractions out of low level variables. I believe that will reduce our uncertainty about the joint distribution and make it cheaper to calculate the queries we are interested in. We are interested in high level variables.</p>
<p>Hypothesis: I think that is related to the decisions that we face - what interventions are we uncertain about? Also depends on the granularity of our tools. (example: macro-economics works with things like interest rates and tax rates, not individual firms.)</p>
<h2 id="inference-data-fusion">Inference / Data-fusion</h2>
<p>Causal inference and the data-fusion problem.</p>
<h2 id="bounding">Bounding</h2>
<blockquote>
<p>[T]he assessment of treatment efficacy in the face of imperfect compliance.</p>
<p>…</p>
<p>Methodologically, the message of this chapter has been to demonstrate that, even in cases where causal quantities are not identifiable, reasonable assumptions about the structure of causal relationships in the domain can be harnessed to yield useful quantitative information about the strengths of those relationships. Once such assumptions are articulated in graphical form and re-encoded in terms of canonical partitions, they can be submitted to algebraic methods that yield informative bounds on the quantities of interest. The canonical partition further allows us to supplement structural assumptions with prior beliefs about the population under study and invite Gibbs sampling techniques to facilitate Bayesian estimation of the target quantities.</p>
<p>– Chapter 8, Causality</p>
</blockquote>
<h2 id="instrumental-variables-linear-models">Instrumental Variables / Linear Models</h2>
<blockquote>
<p>In this paper, we extend graph-based identification methods by allowing background knowledge in the form of non-zero parameter values. Such informa- tion could be obtained, for example, from a previ- ously conducted randomized experiment, from sub- stantive understanding of the domain, or even an identification technique. To incorporate such in- formation systematically, we propose the addition of auxiliary variables to the model, which are con- structed so that certain paths will be conveniently cancelled. This cancellation allows the auxiliary variables to help conventional methods of iden- tification (e.g., single-door criterion, instrumental variables, half-trek criterion), as well as model test- ing (e.g., d-separation, over-identification). More- over, by iteratively alternating steps of identifica- tion and adding auxiliary variables, we can improve the power of existing identification methods via a bootstrapping approach that does not require ex- ternal knowledge. We operationalize this method for simple instrumental sets (a generalization of in- strumental variables) and show that the resulting method is able to identify at least as many models as the most general identification method for linear systems known to date. We further discuss the ap- plication of auxiliary variables to the tasks of model testing and z-identification.</p>
<p>– Incorporating Knowledge into Structural Equation Models using Auxiliary Variables</p>
</blockquote>
<h2 id="adjustment-algorithms">Adjustment / Algorithms</h2>
<blockquote>
<p>We address the problem of nding a minimal separator in a di- rected acyclic graph (DAG), namely, nding a set Z of nodes that d-separates a given pair of nodes, such that no proper subset of Z d-separates that pair. We analyze several versions of this problem and o er polynomial algorithms for each. These include: nding a minimal separator from a restricted set of nodes, nding a minimum- cost separator, and testing whether a given separator is minimal. We con rm the intuition that any separator which cannot be reduced by a single node must be minimal.</p>
<p>– Finding Minimal D-separators</p>
</blockquote>
<blockquote>
<p>Covariate adjustment is a widely used approach to estimate total causal effects from observational data. Several graphical criteria have been de- veloped in recent years to identify valid covari- ates for adjustment from graphical causal mod- els. These criteria can handle multiple causes, latent confounding, or partial knowledge of the causal structure; however, their diversity is con- fusing and some of them are only sufficient, but not necessary. In this paper, we present a cri- terion that is necessary and sufficient for four different classes of graphical causal models: di- rected acyclic graphs (DAGs), maximum ances- tral graphs (MAGs), completed partially directed acyclic graphs (CPDAGs), and partial ancestral graphs (PAGs). Our criterion subsumes the ex- isting ones and in this way unifies adjustment set construction for a large set of graph classes.</p>
<p>– A Complete Generalized Adjustment Criterion</p>
</blockquote>
<h2 id="structural-learning">Structural Learning</h2>
<blockquote>
<p>Randomized controlled experiments are often described as the most reliable tool available to scien- tists for discovering causal relationships among quantities of interest. However, it is often unclear how many and which different experiments are needed to identify the full (possibly cyclic) causal structure among some given (possibly causally insufficient) set of variables. Recent results in the causal discovery literature have explored various identifiability criteria that depend on the assump- tions one is able to make about the underlying causal process, but these criteria are not directly constructive for selecting the optimal set of experiments. Fortunately, many of the needed construc- tions already exist in the combinatorics literature, albeit under terminology which is unfamiliar to most of the causal discovery community. In this paper we translate the theoretical results and apply them to the concrete problem of experiment selection. For a variety of settings we give explicit constructions of the optimal set of experiments and adapt some of the general combinatorics results to answer questions relating to the problem of experiment selection.</p>
<p>– Experiment Selection for Causal Discovery</p>
</blockquote>
<blockquote>
<p>We show that if any number of variables are allowed to be simultaneously and indepen- dently randomized in any one experiment, log 2 (N ) + 1 experiments are sufficient and in the worst case necessary to determine the causal relations among N &gt;= 2 variables when no latent variables, no sample selection bias and no feedback cycles are present. For all K, 0 &lt; K &lt; 2 1 N we provide an upper bound on the number experiments required to determine causal structure when each ex- periment simultaneously randomizes K vari- ables. For large N , these bounds are signifi- cantly lower than the N - 1 bound required when each experiment randomizes at most one variable. For k max &lt; N 2 , we show that N -1)+ 2k N log 2 (k max ) experiments are ( k max max sufficient and in the worst case necessary. We offer a conjecture as to the minimal number of experiments that are in the worst case suf- ficient to identify all causal relations among N observed variables that are a subset of the vertices of a DAG.</p>
<p>– On the Number of Experiments Sufficient and in the Worst Case Necessary to Identify All Causal Relations Among N Variables</p>
</blockquote>
<h2 id="reinforcement-learning-machine-learning">Reinforcement Learning / Machine Learning</h2>
<blockquote>
<p>Reinforcement learning (RL) agents have been de- ployed in complex environments where interactions are costly, and learning is usually slow. One promi- nent task in these settings is to reuse interactions performed by other agents to accelerate the learn- ing process. Causal inference provides a family of methods to infer the effects of actions from a combination of data and qualitative assumptions about the underlying environment. Despite its suc- cess of transferring invariant knowledge across do- mains in the empirical sciences, causal inference has not been fully realized in the context of transfer learning in interactive domains. In this paper, we use causal inference as a basis to support a prin- cipled and more robust transfer of knowledge in RL settings. In particular, we tackle the problem of transferring knowledge across bandit agents in settings where causal effects cannot be identified by do-calculus [Pearl, 2000] and standard learning techniques. Our new identification strategy com- bines two steps - first, deriving bounds over the arms distribution based on structural knowledge; second, incorporating these bounds in a dynamic allocation procedure so as to guide the search to- wards more promising actions. We formally prove that our strategy dominates previously known algo- rithms and achieves orders of magnitude faster con- vergence rates than these algorithms. Finally, we perform simulations and empirically demonstrate that our strategy is consistently more efficient than the current (non-causal) state-of-the-art methods.</p>
<p>– Transfer Learning in Multi-Armed Bandits</p>
</blockquote>
<h2 id="transfer-learning-machine-learning">Transfer Learning / Machine Learning</h2>
<blockquote>
<p>We provide a formal definition of the notion of “transportability,” or “external validity,” which we view as a license to transfer causal information learned in exper- imental studies to a different environment, in which only observational studies can be conducted. We introduce a formal representation called “selection diagrams” for expressing knowledge about differences and commonalities between populations of in- terest and, using this representation, we derive procedures for deciding whether causal effects in the target environment can be inferred from experimental findings in a dif- ferent environment. When the answer is affirmative, the procedures identify the set of experimental and observational studies that need be conducted to license the transport. We further demonstrate how transportability analysis can guide the transfer of knowl- edge among non-experimental studies to minimize re-measurement cost and improve prediction power. We further provide a causally principled definition of “surrogate endpoint” and show that the theory of transportability can assist the identification of valid surrogates in a complex network of cause-effect relationships.</p>
<p>– Transportability Across Studies</p>
</blockquote>
<h2 id="fairness-discrimination-machine-learning">Fairness-Discrimination / Machine Learning</h2>
<blockquote>
<p>Abstract Anti-discrimination is an increasingly important task in data science. In this paper, we investigate the problem of discover- ing both direct and indirect discrimination from the historical data, and removing the discriminatory effects before the data is used for predictive analysis (e.g., building classifiers). We make use of the causal network to capture the causal structure of the data. Then we model direct and indirect discrimination as the path-specific effects, which explicitly distinguish the two types of discrimination as the causal effects transmitted along different paths in the network. Based on that, we pro- pose an effective algorithm for discovering direct and indirect discrimination, as well as an algorithm for precisely removing both types of discrimination while retaining good data utility. Different from previous works, our approaches can ensure that the predictive models built from the modified data will not incur discrimination in decision making. Experiments us- ing real datasets show the effectiveness of our approaches.</p>
<p>Discrimination refers to unjustified distinctions in decisions against individuals based on their membership in a certain group. Federal Laws and regulations (e.g., the Equal Credit Opportunity Act of 1974) have been established to prohibit discrimination on several grounds, such as gender, age, sex- ual orientation, race, religion or belief, and disability or ill- ness, which are referred to as the protected attributes. Nowa- days various predictive models have been built around the collection and use of historical data to make important de- cisions like employment, credit and insurance. If the histor- ical data contains discrimination, the predictive models are likely to learn the discriminatory relationship present in the data and apply it when making new decisions. Therefore, it is imperative to ensure that the data goes into the predictive models and the decisions made with its assistance are not subject to discrimination.</p>
<p>– A Causal Framework for Discovering and Removing Direct and Indirect Discrimination</p>
</blockquote>
<h2 id="interference">Interference</h2>
<blockquote>
<p>Abstract. The term “interference” has been used to describe any set- ting in which one subject’s exposure may affect another subject’s out- come. We use causal diagrams to distinguish among three causal mech- anisms that give rise to interference. The first causal mechanism by which interference can operate is a direct causal effect of one individ- ual’s treatment on another individual’s outcome; we call this direct interference. Interference by contagion is present when one individ- ual’s outcome may affect the outcomes of other individuals with whom he comes into contact. Then giving treatment to the first individual could have an indirect effect on others through the treated individ- ual’s outcome. The third pathway by which interference may operate is allocational interference. Treatment in this case allocates individ- uals to groups; through interactions within a group, individuals may affect one another’s outcomes in any number of ways. In many settings, more than one type of interference will be present simultaneously. The causal effects of interest differ according to which types of interference are present, as do the conditions under which causal effects are iden- tifiable. Using causal diagrams for interference, we describe these dif- ferences, give criteria for the identification of important causal effects, and discuss applications to infectious diseases.</p>
<p>– Causal Diagrams for Interference</p>
</blockquote>
<h2 id="effect-restoration">Effect Restoration</h2>
<blockquote>
<p>This paper highlights several areas where graphical techniques can be harnessed to address the problem of measurement errors in causal inference. In particular, it discusses the control of unmeasured confounders in parametric and nonparametric models and the computational prob- lem of obtaining bias-free effect estimates in such models. We derive new conditions under which causal effects can be restored by observing proxy variables of unmeasured confounders with/without external studies.</p>
<p>– Measurement bias and effect restoration in causal inference</p>
</blockquote>
<h2 id="mediation-analysis">Mediation Analysis</h2>
<blockquote>
<p>Recent advances in causal inference have given rise to a general and easy-to-use for- mula for assessing the extent to which the effect of one variable on another is mediated by a third. This Mediation Formula is applicable to nonlinear models with both dis- crete and continuous variables, and permits the evaluation of path-specific effects with minimal assumptions regarding the data-generating process. We demonstrate the use of the Mediation Formula in simple examples and illustrate why parametric methods of analysis yield distorted results, even when parameters are known precisely. We stress the importance of distinguishing between the necessary and sufficient interpretations of “mediated-effect” and show how to estimate the two components in nonlinear systems with continuous and categorical variables.</p>
<p>– The Causal Mediation Formula</p>
</blockquote>
<h2 id="explanation-actual-causation">Explanation / Actual Causation</h2>
<blockquote>
<p>This chapter offers a formal explication of the notion of “actual cause,” an event recognized as responsible for the production of a given outcome in a specific scenario, as in: “Socrates drinking hemlock was the actual cause of Socrates death.” Human intuition is extremely keen in detecting and ascertaining this type of causation and hence is considered the key to constructing explanations (Section 7.2.3) and the ultimate criterion (known as “cause in fact”) for determining legal responsibility.</p>
<p>…</p>
<p>Statements of the type “a car accident was the cause of Joe’s death,” made relative to a specific scenario, are classified as “singular,” “single-event,” or “token-level” causal statements. Statements of the type “car accidents cause deaths,” when made relative to a type of events or a class of individuals, are classified as “generic” or “type-level” causal claims (see Section 7.5.4). We will call the cause in a single-event statement an actual cause and the one in a type-level statement a general cause.</p>
<p>– Chapter 10, Causality</p>
</blockquote>
<h2 id="constraints-theory">Constraints / Theory</h2>
<p>(Paraphrasing) Help you identify functional constraints and thus help you test causal models as well as infer them from data.</p>
<h2 id="cyclic-models">Cyclic Models</h2>
<blockquote>
<p>We show that the d-separation criterion constitutes a valid test for conditional independence that are induced by feedback systems involving discrete variables</p>
<p>– Identifying Independencies in Causal Graphs with Feedback</p>
</blockquote>
<h2 id="databases">Databases</h2>
<blockquote>
<p>Provenance is often used to validate data, by verifying its origin and explaining its derivation. When searching for “causes” of tuples in the query results or in general observations, the analysis of lineage becomes an essential tool for providing such justifications. However, lineage can quickly grow very large, limiting its immediate use for providing intuitive explanations to the user. The formal notion of causality is a more refined concept that identifies causes for observations based on user-defined criteria, and that assigns to them gradual degrees of responsibility based on their respective contributions. In this paper, we initiate a discussion on causality in databases, give some simple definitions, and motivate this formalism through a number of example applications.</p>
<p>– Causality in Databases</p>
</blockquote>
<h2 id="theses">Theses</h2>
<p>Automated Macro-scale Causal Hypothesis Formation Based on Micro-scale Observation (2016), K. Chalupka [link]</p>
<h2 id="not-interested">Not Interested</h2>
<h1 id="causal-macrovariables">Causal Macrovariables</h1>
<blockquote>
<p>These abstractions are particularly useful when one can establish causal relations amongst macrovariables that hold independent of the micro-variable instantiations of the macrostates.</p>
</blockquote>
<h2 id="abstract-interface-to-the-possible-instances">Abstract Interface to the Possible Instances</h2>
<blockquote>
<p>Each macrovariable state must have a consistent, well-defined causal effect. This effect can be probabilistic and highly variable, but must not depend on the microvariable instantiation of the macrovariable. For just like the specifics of gas molecule momenta do not change the effects of temperature, as long as their mean is equal.</p>
</blockquote>
<p>Hypothesis: The effect of a macrovariable should depend on the <em>interface</em> provided by the microvariables not on the actual configurations.</p>
<p>Total cholesterol fails because low LDL + high HDL and high LDL + low HDL have different effects even though they present the same interface (sum).</p>
<p>Hypothesis: “equivalence relation” = interface satisfied by the microvariables. If two configurations satisfy the same interface (say mean of their velocities), then we consider them equivalent.</p>
<h2 id="chapter-2">Chapter 2</h2>
<blockquote>
<p>The visual cause is distinguished from other macro-variables in that it contains all the causal information about the target behavior that is available in the image.</p>
</blockquote>
<blockquote>
<p>While we are interested in identifying the visual causes of a target behavior, the functional relation between the image pixels and the visual cause should not itself be interpreted as causal. Pixels do not cause the features of an image, they constitute them, just as the atoms of a table constitute the table (and its features).</p>
</blockquote>
<p><strong>Hypothesis</strong>: A category is just a label you give to a set of causal branches that have the same output (for your purposes).</p>
<blockquote>
<p>The probability distribution over the visual cause is induced by the probability distribution over the pixels in the image and the functional mapping from the image to the visual cause.</p>
</blockquote>
<blockquote>
<p>Definition 1 (Observational Partition, Observational Class). The observational partition Pi o (T, I) of the set of images I w.r.t. behavior T is the partition induced by the equivalence relation ~ such that i ~ j if and only if P (T | I = i) = P (T | I = j). We will denote it as Pi o when the context is clear. A cell of an observational partition is called an observational class.</p>
</blockquote>
<blockquote>
<p>Thus, knowing the observational class of an image allows us to predict the value of T . However, the predictive probability assigned to an image does not tell us the causal effect of the image on T . For example, a barometer is widely taken to be an excellent predictor of the weather. But changing the barometer needle does not cause an improvement of the weather. It is not a (visual or otherwise) cause of the weather. In contrast, seeing a particular barometer reading may well be a visual cause of whether we pack an umbrella.</p>
<p>Our notion of a visual cause depends on the ability to manipulate the image.</p>
</blockquote>
<hr />
<blockquote>
<p>The underlying idea is that images are considered causally equivalent with respect to T if they have the same causal effect on T .</p>
</blockquote>
<blockquote>
<p>The Causal Coarsening Theorem</p>
<p>The main theorem of this book relates the causal and observational partitions for a given I and T . It turns out that under appropriate, intuitive assumptions, the causal partition is a coarsening of the observational partition. That is, the causal partition aligns with the observational partition, but the observational partition may subdivide some of the causal classes.</p>
</blockquote>
<blockquote>
<p>Two points are worth noting here: First, the CCT is interesting inasmuch as the visual causes of a behavior do not contain all the information in the image that predict the behavior. Such information, though not itself a cause of the behavior, can be informative about the state of other non-visual causes of the target behavior. Second, the CCT allows us to take any classification problem in which the data is divided into observational classes, and assume that the causal labels do not change within each observational class.</p>
</blockquote>
<blockquote>
<p>The previous chapter develops a method to discover from micro-variable data the macro-variable cause of a pre-defined macro-variable “target behavior”. In this chapter, we do not assume that the macro-level effect is already specified. Instead, in a generalization of the CFL framework, we simultaneously recover the macro- level cause C and macro-level effect E from micro-variable data.</p>
</blockquote>
<h1 id="causal-consistency-of-sems">Causal Consistency of SEMs</h1>
<p>Key variable: Models of the same system at different levels of detail, their consistency</p>
<p>Different models &lt;- large number of variables with irrelevant or unobserved variables marginalized out; micro-level and macro-level models where the macrovariables are aggregates of the microvariables; dynamic time series model vs stationary behaviour models</p>
<p>A: blood cholesterol but not stress</p>
<p>B: 100 billion neurons vs average neuronal activity in functional brain regions</p>
<p>C: time-evolving chemical reaction - just final ratio of reactants and products</p>
<p>Definition: Consistency - Given the same interventions, both models should agree in their predictions.</p>
<p>Structure of the paper: Section 2 - SEMs; Section 3 - SEMs for causal modeling; Section 4 - exact transformation between two SEMs;</p>
<p>Hypothesis: Exact transformation between two SEMs - when can two models be thought of as causal descriptions of the same system?</p>
<p>Hypothesis: Novel idea of the paper - explicitly make use of a “natural ordering on the set of interventions”.</p>
<p>Hypothesis: Questions answered - When can we model only a subsystem of a more complex system? When does a micro-level system admit a causal description in terms of macro-level features? How do cyclic SEMs arise?</p>
<p>Observation: Historically, TC (total cholesterol level) was thought to be important in determining risk of HD (heart disease). Experiments - diets to raise or lower TC. Some found that higher TC lowered HD, others found the opposite.</p>
<p><strong>Hypothesis</strong>: Seemingly conflicting observations &lt;- perform an “invalid” transformation of the true underlying model.</p>
<p>Hypothesis: (Mine) “Transformation” = abstraction of the system. (Or abstraction of the set of interventions you care about.)</p>
<p>Observation: Actual hypothesis - LDL and HDL both increase TC, but have different effects on HD.</p>
<p>Hypothesis: Can’t “transform” (diet -&gt; LDL and HDL -&gt; HD) into (diet -&gt; TC -&gt; HD).</p>
<hr />
<p>SEM =&gt; distributions over interventions</p>
<p>Observation: “The partial ordering of X corresponds to the ability to compose physical implementations of interventions.”</p>
<p>Hypothesis: He’s describing how we compose categories as per the granularity of our tools.</p>
<h2 id="expected-interventions">Expected Interventions</h2>
<p><strong>Hypothesis</strong>: Categories formed &lt;- variables on which interventions are possible.</p>
<p><strong>Hypothesis</strong>: Whether Mx and My are exact transformations depends on the set of interventions you care about.</p>
<p>Otherwise, you wouldn’t be justify losing the detailed information behind variables.</p>
<p>Hypothesis: Ignore variables &lt;- you never expect to intervene on them.</p>
<p>Hypothesis: Simpler model &lt;- you don’t expect to intervene on certain micro-variables.</p>
<p>Hypothesis: Static model &lt;- you don’t expect to change things over time.</p>
<p>Corollary: That’s why we care about hotspots in software engineering - the places where we expect our code to <em>change</em>. That’s the set of interventions for which we want to design our system.</p>
<p>Question: What about just the observations? How coarse can your macrovariables be if all you do is observe?</p>
<h1 id="transportability-and-causal-consistency">Transportability and Causal Consistency</h1>
<p><strong>Hypothesis</strong>: You can transport an intervention across populations iff the target model is an exact transformation of the original model for that intervention.</p>
<p>So, you may not be able to transport certain interventions.</p>
<p>(Not at all sure about this.)</p>
<p>Observation: The current transportability formula uses a mixture of data from the source and target population.</p>
<p>Hypothesis: Maybe you could use a mixture of data using the exact transformation solution by using the commutativity diagram and working in the original model or target model as desired.</p>
<p>Hypothesis: Maybe it will help you consider multiple heterogeneous conditions at the same time.</p>
<h1 id="transportability-across-studies-a-formal-approach-2011">Transportability across studies: A formal approach (2011)</h1>
<h2 id="motivating-examples">Motivating Examples</h2>
<p>Hypothesis: “Selection diagram” :: differences and commonalities between populations of interest</p>
<p>Hypothesis: Transportability problem :: causal effect in different environment -&gt; Maybe causal effect in target environment</p>
<p>Hypothesis: Question - When can we assume that age-specific effects are invariant across cities (but not the age distribution itself)?</p>
<p>Observation: This seems to be transportable.</p>
<p>Now, toggle variables to see when the effect stops being transportable.</p>
<p>Problems:</p>
<ul>
<li><p>Z = age.</p></li>
<li><p>What if Z weren’t age, but some proxy like language proficiency?</p></li>
<li><p>What if Z were an exposure-dependent variable like hypertension level? (motivating example for surrogate endpoint)</p></li>
</ul>
<p>Question: In the hypertension example, is the change in P(Z) due to differences in P(X) or due to differences in the way Z is affected by X? (RCT would distinguish between the two.)</p>
<p>Hypothesis: Transportable effect = if the <strong>mechanism</strong> is the same but it’s just the observational probabilities that are different.</p>
<p>i.e., P(z|x) vs P(z|do(x)).</p>
<p>Corollary: You can’t use statistics because they don’t distinguish between the two. Only interventions can distinguish between observational probabilities and mechanisms.</p>
<p>Corollary: Transportability is a causal notion, not a statistical notion.</p>
<p>Hypothesis: Transportability &lt;- same mechanisms, but potentially different probabilities.</p>
<p>Hypothesis: Population differences = difference in probabilities, but not mechanisms.</p>
<p>Hypothesis: Key variable - What causes the difference between P(z) and P*(z)?</p>
<p>Observation: Language proficiency - distribution in age could be the same in LA and NY, but language proficiency could be caused differently by age in the two cities (maybe LA teaches Spanish in school). Since age distribution is the same, causal effects are just the same in both cities.</p>
<p>Observation: Language proficiency - if proficiency is caused the same way in both cities (P(z|age) = P*(z|age)), then the difference in P(z) was caused by difference in P(age) and so the causal effect won’t be the z-specific effect.</p>
<p>Observation: We assumed that P(y|do(x), z) = P*(y|do(x), z)! We’re assuming that the mechanism is the same.</p>
<p>Hypothesis: Transportability &lt;- causal mechanisms between X and Y are the same, auxiliary mechanisms may be different, probabilities may be different.</p>
<p>Observation: They’re considering cases where some auxiliary variable Z is different in two populations.</p>
<p>Question: What if Z is the same? Do you assume that the causal effect is transportable?</p>
<p><strong>Hypothesis</strong>: Assumption - the causal graph is the same in both populations.</p>
<p><strong>Corollary</strong>: Difference in some auxiliary variable &lt;- difference in mechanism or difference in probabilities.</p>
<p>Corollary: Same mechanism &lt;- auxiliary variable has same probability in both populations, same causal graph, stability assumption</p>
<p>Hypothesis: Transportability &lt;- same causal graph, causal mechanisms between X and Y are the same, auxiliary mechanisms may be different, probabilities may be different.</p>
<p>Basically, do they differ in f or U?</p>
<p>Observation: X -&gt; Z -&gt; Y; the mechanism from X -&gt; Z could be different in the two populations, but you can adjust for that with P*(Z|X). The mechanism in P(y|do(x),z) is assumed to be the same.</p>
<p><strong>Hypothesis</strong>: Transportability &lt;- same causal graph, effects of mechanisms that are different can be identified or experimentally measured, some probabilities may be different.</p>
<p>Question: Why did they assume that P(y|do(x), z) is the same as P*(y|do(x), z) in the Markov chain example above?</p>
<p><strong>Hypothesis</strong>: Use do-calculus on causal effects &lt;- common causal graph.</p>
<p><strong>Hypothesis</strong>: Assume P(A) = P*(A) &lt;- empirical information.</p>
<p>For example, in X -&gt; Z -&gt; Y, P(y|do(x), z) = P(y|z) in the causal graph; so it holds for both P and P*. Then, if we are given that <code>P(y|z) = P*(y|z)</code>, we can remove the star.</p>
<h2 id="data-fusion">Data Fusion</h2>
<p>Hypothesis: Type of dataset = (population (?), experimental vs observation, sampling, measured)</p>
<p><strong>Hypothesis</strong>: Data fusion problem :: heterogeneous datasets -&gt; Maybe target dataset of some type.</p>
<p>Observation: Causal inference :: dataset of type (population a, observational, sampling s, measured v) -&gt; dataset of type (population a, experimental, sampling s, measured v).</p>
<p>For example, P(y|do(x)) is of the type (population a, experimental, sampling s (whatever it may be), {x,y}).</p>
<p><strong>Hypothesis</strong>: It is a dataset. You could use it to infer P(y,z|do(x)) (depending on the graph).</p>
<p>Observation: Z-identifiability :: dataset of type (population a, experimental, sampling s, measured z) -&gt; dataset of type (population a, experimental, sampling s, measured x)</p>
<p>Observation: Transportability :: dataset of type (population a, experimental regime e, sampling s, measured v) -&gt; dataset of type (population b, experimental regime e, sampling s, measured v).</p>
<p>Observation: Sampling selection bias :: dataset of type (population a, experimental regime e, sampling s, measured v) -&gt; dataset of type (population a, experimental regime e, {}, measured v).</p>
<p><strong>Hypothesis</strong>: Type of dataset = (population (?), experimental vs observation, sampling, measured, <em>causal graph</em>, mechanisms).</p>
<p>Question: What about selector diagrams? How do you handle the fact that you have multiple models (but with the same graph)?</p>
<h2 id="bayesian-inference-vs-data-fusion">Bayesian Inference vs Data Fusion</h2>
<p>Question: What about general inference?</p>
<p>Hypothesis: Probabilistic inference :: dataset of type (population a, observational, sampling s, measured v) -&gt; dataset of type (population a, observational, sampling s, measured v + y).</p>
<p>Observation: Bayesian inference :: prior distribution over hypotheses, evidence -&gt; posterior distribution over hypotheses.</p>
<p><strong>Question</strong>: What is the difference between Bayesian inference and data fusion?</p>
<h2 id="potential-tests">Potential Tests</h2>
<p>Hypothesis: Assumptions &lt;- areas where they jumped from P to P*.</p>
<p><strong>Question</strong>: From where do you get feedback for your transportability formula?</p>
<p><strong>Test</strong>: Can you handle the age and skill examples using your idea of two partially-equivalent models?</p>
<h1 id="surrogate-endpoints">Surrogate Endpoints</h1>
<h2 id="understand">Understand</h2>
<p>Question: What were some proposed definitions so far? Where did they break down?</p>
<h2 id="ideas">Ideas</h2>
<p>Hypothesis: The surrogate end-points don’t have to be mediators. They could be effects of the target variable.</p>
<p>For example, the final answer to a math problem (say, 74.5) is strong evidence that you’ve actually understood the relevant technique.</p>
<h2 id="links">Links</h2>
<p>http://blogs.sciencemag.org/pipeline/archives/2017/10/13/a-painful-unacceptable-lack-of-data</p>

<div class="info">Created: June 22, 2017</div>
<div class="info">Last modified: October 20, 2017</div>
<div class="info">Status: in-progress notes</div>
<div class="info"><b>Tags</b>: notes, eb</div>

<br />
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'spkrationalitytrainingground'; // required: replace example with your forum shortname
    var disqus_identifier = '/eb-notes.html';
    var disqus_title = 'EB Notes';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
