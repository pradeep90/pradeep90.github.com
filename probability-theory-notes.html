<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<link rel="icon" type="image/png" href="./images/favicon-32x32.png" sizes="32x32" />
	<link rel="icon" type="image/png" href="./images/favicon-16x16.png" sizes="16x16" />
        <title>Probability Theory Notes - SPK's Rationality Essays</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />
        <link rel="stylesheet" type="text/css" href="./css/highlight.css" />

	<!-- <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script> -->
	<!-- <script type="text/javascript" src="/js/header-links.js"></script> -->

	<!-- Google Analytics stuff -->
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-51321929-1', 'pradeep90.github.io');
	  ga('send', 'pageview');

	</script>

    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">Rationality Essays</a>
            </div>
            <div id="navigation">
                <a href="./">Home</a>
                <!-- <a href="/sequences.html">Sequences</a> -->
                <a href="./notes.html">Notes</a>
                <!-- <a href="/about.html">About</a> -->
                <a href="./archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
          <h1 id="post-title">Probability Theory Notes</h1>

            <!-- <center><img src="https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/t31.0-8/p600x600/10257116_10202295769100492_2438594605053717342_o.jpg" height="400" width="300" class="sujeet-pic" alt="Sujeet pic" /></center> -->

<h1 id="information-theory">Information Theory</h1>
<p>Amount of information gained when you receive a message of probability p = log2(1/p)</p>
<p>Bits can be added because probabilities can be multiplied. Getting a message of probability 1/2 removes half of the probability mass, whatever it was. If you had 1/3 probability mass, you will now have 1/3 x 1/2 = 1/6 probability mass left. If you had 1 probability mass, you will now have 1 x 1/2 = 1/2 probability mass left. Your space has shrunk by half. It is independent of the initial amount of probability mass. So, we call it one bit for convenience. Similarly, a message of probability 1/16 removes 15/16 of the probability mass, whatever it was initially. So, we say that it gives you 4 bits of information. 4 bits of information means reducing your uncertainty by a factor of 4.</p>
<p>Uncertainty is the same as entropy.</p>
<p>I still don’t get it. What does “information” really mean?</p>
<p>You have a bunch of messages. The sum of their probabilities is 1. You get a message of probability p. What now? What changes?</p>
<p>I think it’s all about the set of hypotheses you have. Take the <code>submarine</code> example. You have 64 hypotheses each saying that the submarine is in one of the 64 cells. Initially, you believe in each hypothesis equally with probability 1/64. Your probability distribution over the hypotheses is what determines your probability distribution over the messages at each point.</p>
<p>Initially, say you ask whether the submarine is in cell 28. You will get a message saying yes or no. You weight the predictions from each of your hypotheses with your confidence in them. So, initially, all hypotheses except one predict that you will get no. Therefore, you expect yes with probability 1/64 and no with probability 63/64. If the answer is yes, then, as per Bayes Theorem, all the other hypotheses get a posterior probability of 0 and the lone hypothesis predicting that the submarine is in cell 28 gets a posterior probability of 1.</p>
<p>Each message you receive is strong evidence (really?). It falsifies all hypotheses that predicted the other possible messages. Wait. I don’t think this is right. What actually happens is that your hypotheses together form a probability distribution over the messages. I suspect that information captures how your initial probability distribution changes to your final distribution. But how?</p>
<p>What does one bit mean? In the case where we have 64 equi-probable hypotheses, we can see that one bit means eliminating half of them. But what happens when we have different levels of confidence in the hypotheses? What “half” does one bit eliminate here? This is my single biggest question about information theory.</p>
<p>You accumulate information. What does that mean in probability theoretic terms, reductionistically? How could you say that 6 bits would be exactly enough to identify the submarine no matter how you did it?</p>
<p>In other words, taboo “information” and “bits” and “entropy”, and explain it all in terms of basic probability theory.</p>
<p>Aside: How much do we forget each day? How much information do we lose? Can we avert that fate by writing a lot? Does writing help us save or maybe even get more information than we would otherwise have? Why focus on writing alone? Locate whatever good hypotheses you can with your information.</p>
<p>TODO Decomposability of entropy; When can you decompose?</p>
<h1 id="choosing-vs-sampling">Choosing vs Sampling</h1>
<p>Here’s an example (from David Mackay’s book Information Theory, Learning, and Inference):</p>
<p>Say that the frequency of O blood type in a population is around 60% and that of AB blood type is 1%. What is the probability of picking two random people from the population such that, when you take their blood samples, you find the blood types O and AB?</p>
<p>I thought it would be P(O) x P(AB) = 60% x 1% = 0.06</p>
<p>But that’s wrong!</p>
<p>Here’s the right way. We look at the problem as sampling with replacement. We pick a person X and then we independently pick a person Y. The possibilities are that they have blood types O and O, O and AB, AB and O, and AB and AB (and, of course, the cases involving other blood types). Thus, the chances of finding blood types O and AB are 2 x P(O) x P(AB) because X could be O and Y could be AB or vice versa.</p>
<p>I went wrong earlier because I thought of it in terms of <em>choosing</em>. I looked at it as choosing two people so that I would end up getting O and AB blood types.</p>
<p>Take a concrete example. Say you have a population of 1000 people. As per the problem statement, 600 of them have blood type O, 100 have blood type AB, and the remaining 300 have other blood types. The total number of pairs you can have, independent of blood type, is 1000 x 999 = 999,000. Now, if you want to <em>choose</em> particularly such that you get one person with blood type O and one person with blood type AB, then there are 600 x 100 = 60,000 possible pairings. But if you ask how many of the total pairs are such that you have one person with blood type O and one person with blood type AB, you find that there are 2 x 600 x 100 = 120,000 such pairs.</p>
<p>You get twice as many pairs in the second case because, in sampling, the order matters. When you choose a pair with one O and one AB, you don’t care about whether you got a O-AB pair or an AB-O pair, all that matters is that you get one of each type. In sampling, for every O-AB pair, you’re likely to see an AB-O pair too, thus doubling your results.</p>
<p>The lesson is: don’t ask how you would choose the desired outcome; look at how likely the desired outcome is from the sampling.</p>

<div class="info">Created: December  4, 2015</div>
<div class="info">Last modified: December  5, 2015</div>
<div class="info">Status: in-progress notes</div>
<div class="info"><b>Tags</b>: notes, probability theory</div>

<!-- <div id="sequence-navigation" style="text-align: right"> -->
<!--   <p>Part of <a href="./sequences.html"><i>No Sequence</i></a> -->

<!--   <p>Previous post: "<a href="">Start of Sequence</a>" -->

<!--   <p>Next post: "<a href="">Head of Sequence</a>" -->
<!-- </div> -->

<br />
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'spkrationalitytrainingground'; // required: replace example with your forum shortname
    var disqus_identifier = '/probability-theory-notes.html';
    var disqus_title = 'Probability Theory Notes';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
