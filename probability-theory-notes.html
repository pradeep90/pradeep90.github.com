<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<link rel="icon" type="image/png" href="./images/favicon-32x32.png" sizes="32x32" />
	<link rel="icon" type="image/png" href="./images/favicon-16x16.png" sizes="16x16" />
        <title>Probability Theory Notes - SPK's Rationality Essays</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />
        <link rel="stylesheet" type="text/css" href="./css/highlight.css" />

	<!-- <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script> -->
	<!-- <script type="text/javascript" src="/js/header-links.js"></script> -->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

	<link href="atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed" />

	<!-- Google Analytics stuff -->
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-51321929-1', 'pradeep90.github.io');
	  ga('send', 'pageview');

	</script>

	<script type="text/javascript" src="https://fast.fonts.net/jsapi/f7f47a40-b25b-44ee-9f9c-cfdfc8bb2741.js"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">SPK's Rationality Essays</a>
            </div>
            <div id="navigation">
                <a href="./">Home</a>
                <a href="./notes.html">Notes</a>
                <!-- <a href="/about.html">About</a> -->
                <a href="./archive.html">Archive</a>
		<a href="./atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed">RSS</a>
            </div>
        </div>

        <div id="content">
          <h1 id="post-title">Probability Theory Notes</h1>

            <!-- <center><img src="https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/t31.0-8/p600x600/10257116_10202295769100492_2438594605053717342_o.jpg" height="400" width="300" class="sujeet-pic" alt="Sujeet pic" /></center> -->

<h1 id="information-theory">Information Theory</h1>
<p>Amount of information gained when you receive a message of probability p = log2(1/p)</p>
<p>Bits can be added because probabilities can be multiplied. Getting a message of probability 1/2 removes half of the probability mass, whatever it was. If you had 1/3 probability mass, you will now have 1/3 x 1/2 = 1/6 probability mass left. If you had 1 probability mass, you will now have 1 x 1/2 = 1/2 probability mass left. Your space has shrunk by half. It is independent of the initial amount of probability mass. So, we call it one bit for convenience. Similarly, a message of probability 1/16 removes 15/16 of the probability mass, whatever it was initially. So, we say that it gives you 4 bits of information. 4 bits of information means reducing your uncertainty by a factor of 4.</p>
<p>Uncertainty is the same as entropy.</p>
<p>I still don’t get it. What does “information” really mean?</p>
<p>You have a bunch of messages. The sum of their probabilities is 1. You get a message of probability p. What now? What changes?</p>
<p>I think it’s all about the set of hypotheses you have. Take the <code>submarine</code> example. You have 64 hypotheses each saying that the submarine is in one of the 64 cells. Initially, you believe in each hypothesis equally with probability 1/64. Your probability distribution over the hypotheses is what determines your probability distribution over the messages at each point.</p>
<p>Initially, say you ask whether the submarine is in cell 28. You will get a message saying yes or no. You weight the predictions from each of your hypotheses with your confidence in them. So, initially, all hypotheses except one predict that you will get no. Therefore, you expect yes with probability 1/64 and no with probability 63/64. If the answer is yes, then, as per Bayes Theorem, all the other hypotheses get a posterior probability of 0 and the lone hypothesis predicting that the submarine is in cell 28 gets a posterior probability of 1.</p>
<p>Each message you receive is strong evidence (really?). It falsifies all hypotheses that predicted the other possible messages. Wait. I don’t think this is right. What actually happens is that your hypotheses together form a probability distribution over the messages. I suspect that information captures how your initial probability distribution changes to your final distribution. But how?</p>
<h1 id="taboo-information">Taboo “information”</h1>
<p>What does one bit mean? In the case where we have 64 equi-probable hypotheses, we can see that one bit means eliminating half of them. But what happens when we have different levels of confidence in the hypotheses? What “half” does one bit eliminate here? This is my single biggest question about information theory.</p>
<p>You accumulate information. What does that mean in probability theoretic terms, reductionistically? How could you say that 6 bits would be exactly enough to identify the submarine no matter how you did it?</p>
<p>In other words, taboo “information” and “bits” and “entropy”, and explain it all in terms of basic probability theory.</p>
<p>Taking the <code>log</code> of probability should not do anything by itself. You can now add things (bits) that you could multiply earlier (probabilities). Big deal. Doing <code>log(1/p)</code> does not add any “information”, if you will pardon my phrase.</p>
<p>Let’s work through two examples without using “bits”.</p>
<blockquote>
<p>Example 4.3. The game ‘sixty-three’. What’s the smallest number of yes/no questions needed to identify an integer x between 0 and 63?</p>
</blockquote>
<p>At worst, we can ask 63 yes/no questions (since if it isn’t the first 63 of the 64 numbers, it must be the 64th number). The constraint is that we must ask the smallest number of binary questions.</p>
<p>Looking at it from a Bayesian point of view, we have 64 hypotheses, each saying that the integer is a particular number, like 28. And since we don’t know anything else, we assume that each of them has equal prior probability 1/64. Now, we want to reach the state where one hypothesis, which points to the correct number, has posterior probability 1 and every other hypothesis has posterior probability 0. This is what it means to “identify” the integer x.</p>
<p>So, the initial probability distribution over hypotheses (call it D1) is flat at 1/64, whereas the final distribution (D2) is zero everywhere except for a single peak. Our strategy should let us expect to get from D1 to D2 in as few pieces of evidence as possible, for any given integer.</p>
<p>What if we ask a question for which “yes” has a probability 1/16 and “no” has a probability 15/16? Like the question “Is x+1 divisible by 16?” - only 15, 31, 47, and 63 will match; the rest won’t. What do we expect will happen?</p>
<p>First, note that we say “yes” has a probability 1/16 here <em>because</em> of our prior probability distribution. We come up with the probability by taking each hypothesis’ prediction and weighting it by the hypothesis’ prior probability. Here, 1/16 comes from <code>4 x 1 x 1/64 + 60 x 0 x 1/64</code>. The 4 hypotheses above predict “yes” as the answer to the question with likelihood 1 but their vote is weighted by their prior probability 1/64; similarly, the other 60 hypotheses predict “yes” with likelihood 0 and their vote too is weighted by their prior probability 1/64.</p>
<p>With probability 1/16, we will get a “yes” message. Then, the hypotheses pointing to 15, 31, 47, and 63 respectively will get a posterior probability = <code>P(H) x P(E|H) / P(E) = 1/64 x 1 / (1/16)</code> = 1/4. The other hypotheses all get a posterior probability = <code>1/64 x 0 / (1/16)</code> = 0.</p>
<p>With probability 15/16, we will get a “no” message. Then, the 4 hypotheses get a posterior probability of <code>1/64 x 0 / (15/16)</code> = 0, and the others all get a posterior probability = <code>1/64 x 1 / (15/16)</code> = 1/60.</p>
<p>We could have used a different question, like “Is this number even?”. Half of the hypotheses predict “yes” and half predict “no”, and since we believe all of them equally, our final weighted prediction is 1/2 and 1/2 for each. What would the posterior probability distributions look like in this case?</p>
<p>With probability 1/2, we will see a “yes” message. Then, the even hypotheses will get a posterior probability = <code>1/64 x 1 / (1/2)</code> = 1/32. The odd hypotheses get a posterior probability = 0. With probability 1/2, we will see a “no” message, and get opposite results.</p>
<p>Now, which question is better? Remember, we want to identify the correct integer as soon as possible, which means getting the final spiked distribution D2. So, which question gets us there faster?</p>
<p>I think that every probability distribution over hypotheses has some sort of “distance” away from a fully certain distribution (where you have full confidence in one hypothesis; or maybe one where you get all the probability mass in one outcome. I have to figure this out.). Maybe this is what the “uncertainty” measure in information theory captures.</p>
<h1 id="types">Types</h1>
<p>Information is defined for an outcome of an ensemble. Entropy is defined for an ensemble.</p>
<p>The thing is we can conceive of all the possible hypotheses given a set of variables. Now, it’s like reality knows the correct hypothesis and we’re getting messages from it to reduce our uncertainty and identify the right one. So, the ensemble is the set of possible hypotheses. At each point, we have a posterior probability distribution over the hypotheses. We want to get to certainty, where we believe one hypothesis fully.</p>
<p>Look at the Wenglish example. There you get partial messages. One type of message is “what is the first letter of the word?”. If it is z, then you have eliminated a large number of hypotheses about the word. (Here, a hypothesis is a rule accepting or rejecting each word, thus a subset of all the words.)</p>
<p>In this case, the first letter of the word was the ensemble. You can find its “entropy”. The answer you got (‘z’) was the outcome. You can find its “information content”.</p>
<h1 id="burning-question">Burning Question</h1>
<p>Question: Why is more “information” better?</p>
<p>What does it actually do? Why was the “is x even” question better than the “is x+1 divisible by 16” question?</p>
<h1 id="lesson">Lesson</h1>
<p>Check for type errors when you taboo.</p>
<hr />
<p><br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /> <br /></p>
<p>Aside: How much do we forget each day? How much information do we lose? Can we avert that fate by writing a lot? Does writing help us save or maybe even get more information than we would otherwise have? Why focus on writing alone? Locate whatever good hypotheses you can with your information.</p>
<p>TODO: Decomposability of entropy; When can you decompose? (Talk about this in terms of probability theory, not “messages”.)</p>
<p>TODO: Is information received = change in entropy of hypotheses? What is the significance of the entropy of a set of hypotheses? Is it the expected number of bits to get to certainty? Say it in probability-theoretic terms. I don’t think it is the change in entropy. Take a prior probability distribution of 1/4 on heads and 3/4 on tails. You get a heads. That was log4 bits of information. However, the entropy change was 0 - (1/4log4 + 3/4log(4/3)) which is not equal to log4.</p>
<h1 id="information-theory-exercise">Information Theory Exercise</h1>
<p>Take two words and find their differing predictions. Like “remorseful” and “contrite” - on what variables do they differ?</p>
<h1 id="choosing-vs-sampling">Choosing vs Sampling</h1>
<p>Here’s an example (from David Mackay’s book Information Theory, Learning, and Inference):</p>
<p>Say that the frequency of O blood type in a population is around 60% and that of AB blood type is 1%. What is the probability of picking two random people from the population such that, when you take their blood samples, you find the blood types O and AB?</p>
<p>I thought it would be P(O) x P(AB) = 60% x 1% = 0.06</p>
<p>But that’s wrong!</p>
<p>Here’s the right way. We look at the problem as sampling with replacement. We pick a person X and then we independently pick a person Y. The possibilities are that they have blood types O and O, O and AB, AB and O, and AB and AB (and, of course, the cases involving other blood types). Thus, the chances of finding blood types O and AB are 2 x P(O) x P(AB) because X could be O and Y could be AB or vice versa.</p>
<p>I went wrong earlier because I thought of it in terms of <em>choosing</em>. I looked at it as choosing two people so that I would end up getting O and AB blood types.</p>
<p>Take a concrete example. Say you have a population of 1000 people. As per the problem statement, 600 of them have blood type O, 100 have blood type AB, and the remaining 300 have other blood types. The total number of pairs you can have, independent of blood type, is 1000 x 999 = 999,000. Now, if you want to <em>choose</em> particularly such that you get one person with blood type O and one person with blood type AB, then there are 600 x 100 = 60,000 possible pairings. But if you ask how many of the total pairs are such that you have one person with blood type O and one person with blood type AB, you find that there are 2 x 600 x 100 = 120,000 such pairs.</p>
<p>You get twice as many pairs in the second case because, in sampling, the order matters. When you choose a pair with one O and one AB, you don’t care about whether you got a O-AB pair or an AB-O pair, all that matters is that you get one of each type. In sampling, for every O-AB pair, you’re likely to see an AB-O pair too, thus doubling your results.</p>
<p>The lesson is: don’t ask how you would choose the desired outcome; look at how likely the desired outcome is from the sampling.</p>
<h1 id="testing-for-independence">Testing for Independence</h1>
<p>Let’s say A has two values a1 and a2 and B has two values b1 and b2.</p>
<p>We say A and B are independent if <code>P(a1|b1) = P(a1|b2)</code> or equivalently <code>P(a1|b1) = P(a1)</code>.</p>
<h1 id="quote">Quote</h1>
<p>Apparently from John von Neumann to Shannon:</p>
<blockquote>
<p>You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, no one really knows what entropy really is, so in a debate you will always have the advantage.</p>
</blockquote>
<p>(temporary link: http://www.science4all.org/article/shannons-information-theory/)</p>
<h1 id="resources">Resources</h1>
<p>Check out Everyday Life Information Seeking.</p>

<div class="info">Created: December  4, 2015</div>
<div class="info">Last modified: September 28, 2019</div>
<div class="info">Status: in-progress notes</div>
<div class="info"><b>Tags</b>: notes, probability theory</div>

<br />
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'spkrationalitytrainingground'; // required: replace example with your forum shortname
    var disqus_identifier = '/probability-theory-notes.html';
    var disqus_title = 'Probability Theory Notes';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
	var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<script type="text/javascript" src="https://fast.fonts.net/jsapi/f7f47a40-b25b-44ee-9f9c-cfdfc8bb2741.js"></script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
